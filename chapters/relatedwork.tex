\chapter{Related Work}
\label{chapter:relatedwork}

In this chapter we will look at approaches for reducing cold starts such as improvements to the traditional container model but also some including WebAssembly.

\section{Incremental approaches}

These approaches attempt to work around the limitations of OS-level virtualization with additional techniques, typically implementing some form of cache or pre-fetching. Thus, they can be seen as incremental improvements.

\subsection{Pre-Warming}

\citeauthor{Lin2019} implement a pooling solution for Knative, an open source platform built on Kubernetes \cite{Lin2019}. The pool pre-warms pods and maintains these in a pool. When a request comes in, instead of cold starting, a pod from the pool is migrated and used for execution. According to them, the migration takes around 2 seconds. The response to first request for a simple HTTP server example is improved by 2.3 \times, while a more complex image classifier is sped up by 5.2 \times. In absolute terms, however, the response times are still in the range of multiple seconds. Notably, this approach also includes pre-warming not just the container and language runtime, if applicable, but also the function itself. That in particular explains the large gain in the image classifier example, which needs to load its machine learning model from disk.

\citeauthor{Thoemmes2017} describes the prewarming approach used in OpenWhisk. The platform operator needs to configure the prewarming system by specifying which containers are pre-created. If the anticipated load is primarily JavaScript functions, then the configured number of \inl{node.js} containers is started. In contrast to the previous approach, only the container setup itself and that of the language runtime is taken out of the hot path. Any function initialization still needs to happen during \inl{init}. Hence, the only difference between pre-warmed and warm containers is that the initialization still needs to happen before execution. The upside is that the container is generic over any potential JavaScript function, and not bound to any specific action.
While JavaScript is interpreted or just-in-time compiled, and therefore does not add much to the initialization, for some languages, it can be much longer. As we mentioned before, the reason we used the \inl{dockerskeleton} rather than the Rust runtime is that the Rust source code needs to be compiled during the initialization. While this allows the portable source code to be compiled for any underlying hardware and then executed at native speeds, it comes with the significant compilation cost. Our approach improves on that, since WebAssembly itself is a portable, yet more efficient and compact code representation and could be executed immediately, but also compiled again for more performance.

While these attempts manage to reduce the cold starts, they come at a high cost of resources. Pre-warming a sufficient amount of containers to avoid cold starts even during bursts of requests requires seizing a significant amount of memory. This is especially problematic at the edge.

\subsection{Pre-Creation}

One approach that alleviates cold starts while consuming only negligible amounts of resources, is proposed by \citeauthor{Mohan2019} \cite{Mohan2019}. Their approach avoids the specialization of the container for either one particular function, or the language runtime. Rather, it can be used for any container. First they analyze the core issue for the high cold start times of containers and their rise under increasing concurrency. The startup of a Docker container requires setting up its network namespace in the Linux kernel, which is responsible for more than 90\% of the startup time. The kernel uses a single global lock for this task, which is responsible for the decline in performance under concurrency. Given these findings, they propose an approach that pre-creates these namespaces using so-called pause containers. Their initialization is effectively paused after the network namespace has been created and can then be attached to a Docker container, such that both share the namespace. On top of this, they introduce a pool manager, which holds a number of pause containers from where they can be attached as needed, but also put back after the attached container terminates. Due to only requiring memory on the order of kilobytes, pre-creating a large number of pause containers is reasonable and can thus also accommodate bursty workloads. The evaluation shows a reduction in cold start time of up to 80\%. Given these numbers, this approach is a viable alternative to a more radical approach like ours, at least for the cloud and likely also on edge devices, although this has not been evaluated.

% »Unfortunately, keeping containerspausedentails ahigh memory cost.«
% \url{https://www.usenix.org/system/files/conference/hotcloud16/hotcloud16_hendrickson.pdf}

% Application-level isolation with docker, but function-level isolation with processes
% reduce invocation times of call chains, but not necessarily cold starts themselves
% https://www.usenix.org/system/files/conference/atc18/atc18-akkus.pdf

\subsection{Prediction}

\citeauthor{Shahrad2020} characterized the serverless workload at Microsoft Azure and use their findings to propose an improved keep-alive policy \cite{Shahrad2020}. They first point out that most systems such as OpenWhisk, but also the large cloud providers use a fixed policy, where every function's execution environment is kept alive for the same amount of time. Given their finding that there is an 8 order of magnitude difference in the frequency with which functions are called, this is can be a very wasteful policy for many of the rarely invoked functions.

They characterize policies by a set of rules. One is the pre-warming window, which is the time the policy waits after an execution before pre-warming the container for the next anticipated invocation. The other is the keep-alive window, which is the time it keeps containers warm after either pre-warming or an actual execution.
Their proposed policy works by learning and adjusting a function's invocation frequency. It uses a histogram to track the idle times of functions, from which the 5th percentile is used as the pre-warming window, and the 99th percentile used as the keep-alive time.
Using real invocation traces as input, they simulate workloads against an implementation of their policy in OpenWhisk. The standard fixed keep-alive policy of 10 minutes in OpenWhisk shows the 75th percentile of applications cold starting 50\% of the time. When it uses the same amount of memory as the histogram-based policy it causes 2.5 times more cold starts. Overall, the histogram-based policy reduces the number of cold starts while using a minimal amount of memory.

This approach is independent of the underlying container runtime. Although Wasm executors can reduce the cold start times significantly, in OpenWhisk they would still be using a fixed keep-alive policy. Evidently, there are much better approaches that reduce resource waste. While pre-warming is less important when cold starts are cheap, having a keep-alive time based on the function's invocation pattern would still reduce memory consumption sense than a fixed policy. Future work might examine this policy for Wasm executors and find a suitable percentile to choose the pre-warming window from, given that cold starts are cheaper.

\section{WebAssembly in Serverless}

\subsection{Wasm with Node.js}

We built our work on previous research that built a WebAssembly-based serverless runtime. \citeauthor{Hall2019} use the V8\footnote{\url{https://v8.dev/}} engine's WebAssembly support embedded in \inl{node.js} as their basis.
Since V8 implements the WebAssembly standard like any other runtime, it has the same guarantees in terms of security. In particular, V8 \emph{contexts} provide isolation between different executing WebAssembly modules. Thus, for each request their implementation creates a new context where the WebAssembly module is loaded, executed via the V8 API, the result returned and the context destroyed.

For single clients sending multiple requests, they find their WebAssembly platform to have a small initial cold start penalty, but perfoms more predictably. The test against native binaries in OpenWhisk shows them to be faster over time but suffer a high cold start latency. These are similar results as our CPU-bound workload, where Docker's slow initial startup would be amortized over many invocations. At its best, their numbers suggest the WebAssembly module to only reach 56\% of the throughput of Docker, while our precompiled WebAssembly reached 88\% of throughput in \inl{wasmer}. This confirms that WebAssembly execution in \inl{node.js} is not the best option for pure speed due to its JIT compilation strategy.

They note that the model of single clients making multiple requests is not fully representative of a real workload. Rather, multiple users would make requests, each of which needs to be isolated and consequently result in a cold start. Their findings for this access pattern are in line with our mixed and realistic workloads, namely that WebAssembly-based executors exhibit more consistent performance. Whether or not we have improved upon the cold start times of their approach is unclear, since no specific cold start measurements is given, and only the latency as a whole is examined. 
Since they use \inl{node.js}, their solution would most likely be able to achieve a much higher throughput for an I/O-bound workload, since the runtime uses an event loop to provide efficient non-blocking I/O. As previously mentioned, once WebAssembly runtimes and the corresponding Rust libraries have added support for asynchronous I/O, they should be at least as fast.

\subsection{Cloudflare Workers}

Cloudflare is a cloud service provider, whose \emph{Workers} offering enables end-users to run code on its Edge Network. The Workers use Google's V8 JavaScript and WebAssembly engine to execute that code.
Instead of running each serverless function in a separate docker container or even \inl{node.js} instance, the Workers use V8 \emph{isolates} for lightweight sandboxing. Isolates start within the already running V8 engine, so the start is almost instantaneous, alleviating cold-starts down to 0 ms.
Workers allow execution of JavaScript directly in an isolate as well as Rust, C and other languages via Wasm support \cite{Cloudflare2021}.
Similar to our approach, a lighter-weight sandboxing is introduced to cut down on the cold-start latency. This execution model eliminates the costly startup of a \inl{node.js} process, but it would still need to parse and compile the JavaScript or WebAssembly, before execution can begin.

% Expand with Wasm in our OW and precompiled Wasm in our OW.

\subsection{Fastly's Lucet}

Cloud provider fastly offers its experimental fastlylabs\footnote{\url{https://www.fastlylabs.com/}}, where its previously mentioned \inl{lucet} WebAssembly runtime can be used inside \emph{Terrarium}, to run Wasm code at the edge \cite{fastly2019}. The \inl{lucet} compiler first translates WebAssembly to native code, after which it can be executed in the complementary runtime. Due to this design -- which spawned the idea for our work -- \inl{lucet} can execute Wasm efficiently on every request, with module instantiation times of 50 microseconds and only kilobytes of required memory, according to them.

Even though we did not leverage \inl{lucet} due to security concerns as well as platform and ISA support, we did use the idea of precompiling to native code ahead-of-time. This payed off, given that we measured module instantiation times of 340 microseconds to setup a \inl{wasmtime} instance from a module and would not have achieved module deserialization times of around 10 milliseconds.
Fastly has joined the Bytecode Alliance\footnote{\url{https://bytecodealliance.org/}}, under whose umbrella \inl{wasmtime} is developed, and \inl{lucet} is being integrated into \inl{wasmtime} to provide Ahead-of-Time compilation \cite{Hickey2020}.

\section{WebAssembly in Other Contexts}

smart contracts
wasm @ edge