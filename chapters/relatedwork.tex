\chapter{Related Work}
\label{chapter:relatedwork}

In this chapter we look at approaches for reducing cold starts such as improvements to the traditional container runtime but also including WebAssembly.

\section{Incremental approaches}

These approaches attempt to work around the limitations of OS-level virtualization with additional techniques, typically implementing some form of cache or pre-fetching. Thus, they can be seen as incremental improvements.

\subsection{Pre-Warming}

\citeauthor{Lin2019} implement a pooling solution for Knative, an open source platform built on Kubernetes \cite{Lin2019}. Pods are pre-warmed and added to a management pool. When a request comes in, instead of cold starting, a pod from the pool is used for execution. According to them, the migration takes around 2 seconds. The response to first request for a simple HTTP server example is improved by 2.3 times, while a more complex image classifier is sped up by 5.2 times. In absolute terms, however, the response times are still in the range of multiple seconds. Notably, this approach also includes pre-warming not just the container and language runtime, if applicable, but also the function itself. That in particular explains the large gain in the image classifier example, which needs to load its machine learning model from disk in the first invocation.

\citeauthor{Thoemmes2017} describes the pre-warming approach used in OpenWhisk. The platform operator needs to configure the pre-warming system by specifying which containers are pre-created. For instance, if the anticipated load is primarily JavaScript functions, then the operator configures some number of \inl{node.js} containers to be started. In contrast to the previous approach, only the container setup itself and that of the language runtime is taken out of the hot path. Any function initialization still needs to happen during \inl{init}. Hence, the only difference between pre-warmed and warm containers is that the initialization still needs to happen before execution. The upside is that the container is generic over any potential JavaScript function and not bound to any specific action.
While JavaScript is interpreted or just-in-time compiled, and therefore does not add much to the initialization, for some languages it can be much longer. As we mentioned before, the reason we used the \inl{dockerskeleton} rather than the official Rust runtime is that the source code needs to be compiled during the initialization. While this allows the portable source code to be compiled for any underlying hardware and then executed at native speeds, it comes with a significant compilation cost. Our approach improves on that, since WebAssembly itself is a portable, yet more efficient and compact code representation and could be executed immediately, but also compiled again for more performance.

While these attempts manage to reduce the cold starts, they come at a high cost of resources. Pre-warming a sufficient amount of containers to avoid cold starts even during bursts of requests requires seizing a significant amount of memory. This is especially problematic at the edge.

\subsection{Pre-Creation}

One approach that alleviates cold starts while consuming only negligible amounts of resources, is proposed by \citeauthor{Mohan2019} \cite{Mohan2019}. Their approach avoids the specialization of the container for either one particular function or the language runtime. Rather, it can be used for any container. First they analyze the core issue for the high cold start times of containers and their rise under increasing concurrency. The startup of a Docker container requires setting up its network namespace in the Linux kernel, which is responsible for more than 90\% of the startup time. The kernel uses a single global lock for this task, which is the reason for the decline in performance under concurrency. Given these findings, they propose an approach that pre-creates these namespaces using so-called pause containers. Their initialization is effectively paused after the network namespace has been created. Subsequently, they can be attached to a Docker container, such that both share the namespace and the Docker container itself avoids the namespace creation. On top of this, they introduce a pool manager, which holds a number of pause containers from where they can be attached as needed, but also put back after the attached container terminates. Due to only requiring memory on the order of kilobytes, pre-creating a large number of pause containers is reasonable and can thus also accommodate bursty workloads. The evaluation shows a reduction in cold start time of up to 80\%. Given these numbers, this approach is a viable alternative to a more radical approach like ours, at least for the cloud and likely also on edge devices, although the latter has not been evaluated.

% »Unfortunately, keeping containerspausedentails ahigh memory cost.«
% \url{https://www.usenix.org/system/files/conference/hotcloud16/hotcloud16_hendrickson.pdf}

% Application-level isolation with docker, but function-level isolation with processes
% reduce invocation times of call chains, but not necessarily cold starts themselves
% https://www.usenix.org/system/files/conference/atc18/atc18-akkus.pdf

\subsection{Prediction}

\citeauthor{Shahrad2020} characterized the serverless workload at Microsoft Azure and use their findings to propose an improved keep-alive policy \cite{Shahrad2020}. They first point out that most systems such as OpenWhisk, but also the large cloud providers, use a fixed policy where every function's execution environment is kept alive for the same amount of time. Given their finding that there is an 8 order of magnitude difference in the frequency with which functions are called, this is can be a very wasteful policy for many of the rarely invoked functions.

They characterize policies by a set of rules. One is the pre-warming window, which is the time the policy waits after an execution before pre-warming the container for the next anticipated invocation. The other is the keep-alive window, which is the time it keeps containers warm after either pre-warming or an actual execution.
Their proposed policy works by learning a function's invocation frequency and adjusting the window parameters. It uses a histogram to track the idle times of functions, from which the 5th percentile is used as the pre-warming window, and the 99th percentile used as the keep-alive time.
Using real invocation traces as input, they simulate workloads against an implementation of their policy in OpenWhisk. 
The standard fixed keep-alive policy in OpenWhisk generates 2.5 times more cold starts than the histogram-based policy while using more memory. Overall, the histogram-based policy reduces the number of cold starts while minimizing the used memory.

This approach is independent of the underlying container runtime. Although Wasm executors can reduce the cold start times significantly, for efficient successive invocations some amount of keep-alive is needed. Since the keep-alive policy in OpenWhisk is a fixed one, this approach would evidently be a better choice to reduce cold starts and resource waste. While pre-warming is less important when cold starts are cheap, having a keep-alive time based on the function's invocation pattern still reduces memory consumption compared to a fixed policy. Future work might examine this policy for Wasm executors and find a suitable percentile to choose the pre-warming window from, given that cold starts are cheaper.

\section{WebAssembly in Serverless}

\subsection{Wasm with Node.js}

We built our work on previous research that built a WebAssembly-based serverless runtime. \citeauthor{Hall2019} use the V8\footnote{\url{https://v8.dev/}} engine's WebAssembly support within \inl{node.js} as their basis.
Since V8 implements the WebAssembly standard like any other runtime, it has the same guarantees in terms of security. In particular, V8 \emph{contexts} provide isolation between different executing WebAssembly modules. Thus, for each request their implementation creates a new context where the WebAssembly module is loaded, executed via the V8 API, the result returned and the context destroyed.

For single clients sending multiple requests, they find their WebAssembly platform to have a small initial cold start penalty due to context creation, but performs more predictably than Docker afterwards. Their test against native binaries in Docker containers shows them to suffer from a high cold start latency but are faster over time. The examples they use are image recognition applications and thus CPU-bound. In that regard, these are similar results as our CPU-bound workload, where Docker's slow initial startup was amortized over many invocations.
In the best case, their numbers indicate the WebAssembly module to reach 56\% of the throughput of Docker, while our precompiled WebAssembly reached 88\% of throughput in \inl{wasmer}. Recall that \inl{wasmtime}, our JIT-configured runtime reached 57\% of throughput in our test. This confirms that WebAssembly execution in \inl{node.js} is not the best option for pure speed due to its JIT compilation strategy.

They note that the model of single clients making multiple requests is not fully representative of a real workload. Rather, multiple users would make requests, each of which needs to be isolated and consequently result in a cold start. Their findings for this access pattern are in line with our mixed and realistic workloads, namely that WebAssembly-based executors exhibit more consistent performance. Whether or not we have improved upon the cold start times of their approach is unclear, since no specific cold start measurements are given, and only the latency as a whole is examined. 
Since they use \inl{node.js}, their solution would most likely be able to achieve a much higher throughput for an I/O-bound workload, since the runtime uses an event loop to provide efficient non-blocking I/O. As previously mentioned, once WebAssembly runtimes and the corresponding Rust libraries have added support for asynchronous I/O, they should be at least as fast.

\subsection{Cloudflare Workers}

Cloudflare is a cloud service provider, whose \emph{Workers} offering enables end-users to run code on its Edge Network. The Workers also use Google's V8 JavaScript and WebAssembly engine to execute that code.
Instead of running each serverless function in a separate Docker container or even \inl{node.js} instance, the Workers use V8 \emph{isolates} for lightweight sandboxing. Isolates start within the already running V8 engine, so the start is almost instantaneous.
Workers allow execution of JavaScript directly in an isolate as well as Rust, C and other languages via Wasm support \cite{Cloudflare2021}.
Similar to the preceding approach, a lighter-weight sandboxing is introduced to cut down on the cold-start latency, only this time isolates instead of contexts. This execution model eliminates the costly startup of a \inl{node.js} process, but it would still need to parse and compile the JavaScript or WebAssembly before execution, which is what our approach eliminates.

% Expand with Wasm in our OW and precompiled Wasm in our OW.

\subsection{Fastly's Lucet}

Cloud provider fastly offers its experimental fastlylabs\footnote{\url{https://www.fastlylabs.com/}}, where its previously mentioned \inl{lucet} WebAssembly runtime can be used inside \emph{Terrarium}, to run Wasm code at the edge \cite{fastly2019}. The \inl{lucet} compiler first translates WebAssembly to native code, after which it can be executed in the complementary runtime. Due to this design -- which spawned the idea for our work -- \inl{lucet} can execute Wasm efficiently on every request, with module instantiation times of 50 microseconds and only kilobytes of memory, according to them.

Even though we did not leverage \inl{lucet} due to security concerns as well as platform and ISA support, we did use the idea of precompiling to native code ahead-of-time. This payed off, given that we measured module instantiation times of 340 microseconds to setup a \inl{wasmtime} instance from a module. Without this, we also would not have achieved cold starts of around 10 milliseconds in that runtime.
Fastly has joined the Bytecode Alliance\footnote{\url{https://bytecodealliance.org/}}, under whose umbrella \inl{wasmtime} is developed, and \inl{lucet} is being integrated into \inl{wasmtime} to provide Ahead-of-Time compilation \cite{Hickey2020}. This might bring the AoT performance into the realm of \inl{wasmer}'s.

\subsection{Faasm}

\citeauthor{Shillaker2020} introduce \inl{faasm}, a serverless runtime using WebAssembly \cite{Shillaker2020}. One or more functions is executed in a \inl{faaslet}, which uses WebAssembly's software fault-isolation to restrict memory access to its own address space. Every \inl{faaslet} is run in a new thread which allows limiting CPU cycles with \inl{cgroups} and network with Linux namespaces.
We did not provide a CPU limitation mechanism since we believe our threading model to be only an intermediate solution until WASI functions can be executed asynchronously, but it could use the same \inl{cgroup} mechanism in principle. When executing asynchronously, however, \inl{cgroups} cannot be used to limit an individual function's CPU time, which is therefore an open problem. Memory on the other hand can be limited through appropriate APIs of the WebAssembly runtimes.

Running multiple functions together allows for memory sharing and statefulness -- an explicit goal of the runtime --  also across hosts via a state management API. Even though in traditional serverless platforms functions can be chained together, this process requires serialization which can be omitted when sharing memory directly. This is implemented on top of the Wasm memory model with \emph{shared regions}, where the linear memory of a WebAssembly instance is extended by additional pages which are mapped onto that region.
Statefulness was not a goal of our work, given that this limitation of serverless platforms has not been a primary one for developers, according to our literature review. For applications that make heavy use of state sharing, this may prove beneficial, although the increase in implementation complexity needs to be considered.

Fast cold starts are achieved by initializing a \inl{faaslet} ahead-of-time as a snapshot of a function's stack, heap function table, stack pointer and data -- in WebAssembly terminology. This is generated at function-creation time and stored in an object store. When cold starting a function, the snapshot is retrieved and restored to produce a \inl{faaslet} instance. The restoration procedure guarantees no data from previous invocations is shared with future invocations. This is similar to how we use WebAssembly \inl{Module}s as our templates and produce \inl{Instance}s on every invocation for isolation.
However, due to their pre-creation and restore procedure, the cold start times are at around 1 millisecond.
This technique could be adopted in our work as well to provide even faster cold starts.

\section{WebAssembly in Smart Contracts}

Smart contracts are programs deployed to distributed ledgers from where they can be run by users on-demand. As such, smart contracts are akin to serverless functions, since developers do not need to manage the machines they run on. Users of the contract often pay a per transaction fee similar to the pay-as-you-go model of serverless and the node running the untrusted contract needs to isolate its execution to enable secure multi-tenancy.
The transaction fee is usually based on the amount of CPU the smart contract consumes, which incentivizes developers to produce lightweight code. That in turn means it is very likely that the average smart contract executes much faster than the average serverless function, making OS-level virtualization even less suitable for isolation, since it would be responsible for an even larger proportion of the latency. The node operator running the smart contract has an interest in achieving high throughput to make the most of the available resources, so performance cannot be neglected. Additionally, smart contract nodes likely need to handle bursty request patterns and thus need to be elastic, too.
These requirements are similar to what a serverless container runtime needs to offer.

Fittingly, WebAssembly has been recognized as a suitable target format for smart contracts. The Ethereum-flavored WebAssembly restricts the instruction set to a suitable subset for smart contracts and one of the cited reasons for the switch from the previous custom virtual machine is performance \cite{Ewasm2021}.
The conforming \inl{hera}\footnote{\url{https://github.com/ewasm/hera}} implementation can use WebAssembly runtimes that use interpretation, JIT or AoT compilation strategies. One of the options includes \inl{WAVM}\footnote{\url{https://github.com/WAVM/WAVM}}, a runtime written in C++ with a focus on performance and claimed near-native execution speed.
Other ledgers like near\footnote{\url{https://github.com/near/nearcore/tree/master/runtime/near-vm-runner/}} use \inl{wasmer} with caching, IOTA\footnote{\url{https://github.com/iotaledger/wasp}} uses \inl{wasmtime} and EOSIO\footnote{\url{https://github.com/EOSIO/eos-vm}} implemented their own WebAssembly VM, citing a lack of performance in existing runtimes. Many more ledgers have adopted what seems to have become the de-facto standard target format for smart contracts.

Overall, these approaches validate the fundamental choice of WebAssembly as a performant option in scenarios where lightweight isolation is required. The smart contract space will be interesting to watch and has already given rise to some WebAssembly synergies. For example, both near and fastly are listed as platinum sponsors on AssemblyScript's GitHub page\footnote{\url{https://github.com/AssemblyScript/assemblyscript/}}, which might make AssemblyScript a more viable and mature alternative to write serverless functions in WebAssembly.
