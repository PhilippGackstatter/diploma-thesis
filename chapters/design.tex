\chapter{Design}
\label{chapter:design}

In this chapter, we lay out the requirements for our implementation of Wasm containers in Apache OpenWhisk. Then we describe and discuss our implementation and the trade-offs involved.

\section{Core approach}

As we've seen in the previous chapter, there are a number of issues with serverless in general, but also serverless on the edge. And they are quite similar. In this section we will describe our idea for rectifying these issues to make serverless (edge) computing more viable.

A substantial cold start latency has emerged as the paramount issue of the serverless platform. It prevents serverless platforms from implementing true scale-to-zero, since functions are kept warm after invocations, wasting energy and resources. While we may not be able to get rid of the keep-alive completely, a significant reduction in cold start latency would allow a similarly large reduction in keep-alive time. Since energy and resources are even more precious on typical edge devices, this is also very important for their quality of service.
Serverless is employed by developers for bursty, unpredictable workloads, a pattern that is also common at the edge. In this scenario we see cold starts occurring concurrently, one in which container starts slow each other down.
Operators are forced to choose between saving cost and resources or enabling a high quality of service, in particular shorter response times. While this trade-off is inevitable to some degree, the cold start exacerbates it. If we can alleviate that latency, we have a win-win for the serverless user and the operator.

Serverless enables the execution of \quot{polgylot tenant-provided code} \cite{Nastic2018}. A new runtime for serverless must keep up these principles of being programming language agnostic as well as being securely sandboxed, to allow for multi-tenancy. As described in the previous chapter, WebAssembly provides sandboxing and support for many languages, with support for more, likely to come. At the same time, it is a lighter-weight solution than virtualization with Docker containers. Thus, it seems like a good candidate to replace Docker containers in serverless platforms while alleviating the cold start latency. This is also why WebAssembly may fare better on low-power edge devices.

\citeauthor{Mendki2020} compared WebAssembly and Docker container startup times, and found WebAssembly being 91\% faster than containers and used 75\% less memory \cite{Mendki2020}. \citeauthor{Hall2019} find their WebAssembly-based serverless platform fared 60\% better on average, than the Docker container solution, for their concurrent \quot{Multiple Client, Multiple Access} workload \cite{Hall2019}. These promising results are the basis for our work and validate the core idea of our approach.

\section{Requirements}

\subsection{Apache OpenWhisk}

In order to understand the requirements for our Wasm-flavored OpenWhisk, we need to understand the design of OpenWhisk itself first.

\begin{figure}
    \includegraphics{figures/OpenWhiskActionInvocationFlow.pdf}
    \caption{Action invocation flow in Apache OpenWhisk based on the source code and documentation \cite{OpenWhiskSystemDesign}. Potentially many Invokers can exist in this setup.}
    \label{fig:openwhisk-action-invocation-flow}
\end{figure}

Figure \ref{fig:openwhisk-action-invocation-flow} sketches the design of OpenWhisk. Users interact with it through its API Gateway, which consists of \inl{nginx}, mostly for SSL termination, and the \inl{Controller}, which implements the main logic. It handles the creation of actions, OpenWhisk's name for serverless functions, authentication and authorization as well as invoking the action.
In the latter case, the controller uses its load balancer to determine which Invoker should handle the request, and publishes the request to an Apache Kafka queue, which the invoker is subscribed to. The controller and its load balancing take a central role in OpenWhisk, but in our figure it is represented by the API Gateway and the Load Balancer only, simply because we will not be modifying this part.
Our focus is on the Invoker. It is the part of OpenWhisk responsible for \emph{invoking} the action. A number of steps are involved in that process \cite{OpenWhiskSystemDesign}.

\begin{enumerate}
    \item It retrieves the action's code and execution permissions from an Apache CouchDB database, where the Controller stored it during action creation. 
    \item It instructs the local Docker daemon to start a new container, based on the runtime that the action needs to execute. This would be a \inl{openwhisk/action-nodejs-v10} image for JavaScript actions but could also be a generic black-box image (\inl{openwhisk/dockerskeleton}), that can execute any binary, such as one written in Rust or C.
    \item Finally, it injects the action's code into the container, invokes it with the given parameters and returns the result \cite{OpenWhiskSystemDesign}.
\end{enumerate}

Because OpenWhisk supports many runtimes and even the mentioned black-box image, it needs a common protocol to communicate with all of them. The just-described step 3 is the gist of that protocol. Each runtime needs to implement three endpoints. The \inl{/start} endpoint creates a new container and returns its address. The following requests to the \inl{/init} endpoint are then sent to that address, where the container receives the code and takes whatever steps necessary to make the action ready for execution. Thus, over a containers life cycle, this endpoint is called exactly once. Once initialized, the \inl{/run} endpoint can be called multiple times to execute the action \cite{OpenWhiskSystemDesign}.

That implies that the container isn't destroyed immediately after it has been invoked once. Indeed, OpenWhisk uses various optimizations of the described flow, in order to improve the system's performance. Among those are pre-warming containers or keeping the container running for some time after it has been invoked, before it is removed.
We will explore those optimizations in more depth later, to compare them to the optimizations in our Wasm runtime to better see where they coincide and differ \cite{OpenWhiskSystemDesign}.

\subsection{OpenWhisk on Kubernetes}

OpenWhisk can be deployed on a Kubernetes cluster, a container orchestration tool for automating deployment and scaling of containers \cite{Kub2021}. OpenWhisk has a \inl{ContainerFactoryProvider} service provider interface (SPI), which represents the underlying container management platform. It provides two implementations of that SPI \cite{OWKub2020}.

\begin{enumerate}
    \item The \inl{DockerContainerFactory} implements the architecture shown above, except that most components of OpenWhisk run in separate Kubernetes pods. In non-Kubernetes deployments, they usually run in separate Docker containers. The crucial part of this deployment is, that the Invoker runs in a single pod (potentially on multiple worker nodes), and also spawns all of the actions in docker containers inside that pod. Thus container management latency is rather small, but does not take advantage of Kubernetes' scaling mechanisms.
    \item The \inl{KubernetesContainerFactory} interfaces with the Kubernetes API to create and schedule actions in containers -- in turn contained in pods -- to run on different Kubernetes worker nodes. This trades higher container management latency for access to Kubernetes' scaling mechanisms.
\end{enumerate}

\subsection{Executing WebAssembly with OpenWhisk}

To execute Wasm modules in OpenWhisk, we have two similar options with similar trade-offs.

\begin{enumerate}
    \item Using one of a number of WebAssembly runtimes that are currently being developed and are in various stages of maturity. We would need to embed one of these runtimes in a program of our own that provides a similar interface as Docker. This WebAssembly container runtime would then be able to replace the Docker daemon in the architecture presented above, and run Wasm modules instead of Docker images. Since this program would sit in-between OpenWhisk and the Wasm runtime, we would have precise control over the management of the modules, i.e. whether the Wasm module is just-in-time compiled, interpreted or compiled to native code ahead-of-time; the caching layer that keeps initialized modules ready for fast execution; or the configuration of the system interface with which Wasm modules can interact with the underlying operating system.
    \item The second option is to use Kubernetes as the management system for our Wasm modules. Kubernetes is generic over the underlying container runtimes, and as such it runs on \inl{containerd}, \inl{CRI-O} and Docker \cite{Kub2021}. However, it is also possible to let Kubernetes pods run on other runtimes by implementing the \inl{kubelet} API. The \inl{krustlet} project provides a \inl{kubelet} implementation that can execute Wasm workloads \cite{Krustlet2021}. With that, we can now instruct Kubernetes to run Wasm modules for us, instead of the typical Docker container.
\end{enumerate}

We will explore those two options in more detail and discuss which may be a better fit for our requirements.

\subsection{Krustlet: The WebAssembly Kubelet}

As a \inl{kubelet}, \inl{krustlet} runs on Kubernetes worker nodes and accepts workloads of the \inl{wasm32-wasi} architecture. Just like a regular kubelet, krustlet will then pull the Wasm module from an OCI registry and run it. We could thus schedule our Wasm workloads from an Invoker, similar to how the \inl{KubernetesContainerFactory} implementation schedules actions in Docker containers. However, this execution model is also different from the OpenWhisk model in some ways.

\begin{itemize}
    \item In OpenWhisk there is a split between runtime environment, provided through the Docker image, and the code that is stored in a database, retrieved from there and injected into the container. Krustlet expects the Wasm module to be the equivalent of the container, but with the code baked in. Thus no injection is necessary. This makes it slightly harder to integrate with the current OpenWhisk model, since the Wasm module doesn't need to be stored in the database, but rather in the OCI registry. That registry would need to be part of the Kubernetes cluster, since the images are not generic runtime environments, but the Wasm modules uploaded to OpenWhisk by the users. We can assume, that the latency introduced by retrieving the image from the registry is offset by not having to retrieve the code from the database, so both solutions should perform the same in that regard.
    \item Since there is no split anymore, every Wasm module needs to implement the OpenWhisk runtime protocol itself, instead of being able to rely on the runtime. In practice, this means implementing the \inl{/init} and \inl{/run} endpoints, where the former would no longer do anything useful, since there is no code to inject. Because network support in WASI is still under active development, the alternative is to implement a \inl{wasmcloud-actor}, which essentially provides capabilities such as networking to Wasm modules \cite{WC2021}. The function can then be run by calling the \inl{/run} endpoint. Just like containers in OpenWhisk keep running for some time after having been called once, the module would also continue to listen for more requests to avoid the cold-start.
\end{itemize}

While this approach has its merits, we believe that there would be higher friction for implementing it in OpenWhisk, than writing our own layer to Wasm runtimes. It provides more control over optimizing the cold-start latency as well as the execution performance, which are our main priorities.

\subsection{An OpenWhisk-WebAssembly Layer}

We will implement a layer between OpenWhisk and different WebAssembly runtimes, in order to enable the execution of Wasm modules in OpenWhisk. The layer will be generic over the underlying WebAssembly runtime, such that we can use different ones and compare them. We aim to replace the Docker daemon in the architecture shown above, leveraging the existing OpenWhisk interface for container management.

\begin{figure}
    \includegraphics{figures/WasmOpenWhiskActionInvocationFlow.pdf}
    \caption{Invocation flow of a Wasm action in our modified Apache OpenWhisk.}
    \label{fig:wasm-openwhisk-action-invocation-flow}
\end{figure}

In figure \ref{fig:wasm-openwhisk-action-invocation-flow} we have sketched the high-level modifications that we need to make to OpenWhisk. In this invocation flow, we need to hook into OpenWhisk's invoker to enable its communication with the Wasm runtime we will implement, instead of the Docker daemon. Fortunately, the Invoker is already well-separated from the concrete containerization technology through a Service Provider Interface (SPI). OpenWhisk already supports Docker, but also Kubernetes as the container management system, such that the SPI was a logical addition. We implement a \inl{WasmContainer} that extends OpenWhisk's \inl{Container} abstraction, which already handles container communication, such as calling the \inl{/init} and \inl{/run} endpoints.

% Figure that shows a zoomed-in version of the invoker and its SPIs (as well as the Wasm runtime?) - Help better understand OW's design and what we added

In OpenWhisk's Docker container implementation, OpenWhisk only uses the Docker daemon to create a container, but then communicates with the container directly. In our implementation, every request is proxied through the \inl{WasmRuntime}. The advantage is, that not every Wasm module needs to implement the OpenWhisk protocol, which requires an HTTP server; increasing both the module's size and its runtime overhead. That presupposes, that the \inl{WasmRuntime} can handle a potentially large amount of requests.

% Need to decide whether OW handles containers life cycle or if runtime takes that responsibility

The runtime also needs to be fast in general, in order to aid in alleviating the cold start problem. That includes efficient handling of receiving the module's code from OpenWhisk, instantiating it and setting up the underlying Wasm executor. A general requirement for a useful Wasm runtime would also be its ability to run on different instruction set architectures.

In summary, the requirements for our runtime are

\begin{enumerate}
    \item It needs to be fast to alleviate the cold-start problem.
    \item It needs to run on devices with potentially different instruction set architectures.
    \item It needs to be able to efficiently handle a large amount of I/O concurrently, due to all requests being proxied through
\end{enumerate}

\section{Implementation}

To implement these requirements in Apache OpenWhisk we will take the following steps. The current Docker-based invoker will be replaced, to forward all the incoming requests to our new Wasm runtime.

The Wasm runtime will be written from scratch in the Rust programming language. There are a number of reasons for this choice.

\begin{itemize}
  \item It is a compiled language with performance close to that of C.
  \item It uses LLVM in the backend so that it can be compiled for almost any architecture.
  \item It allows for \quot{fearless concurrency}, since the compiler can detect data races at compile time.
  \item It has good support for asynchronous I/O, so it should be able to efficiently handle a large amount of concurrent requests.
  \item 3 out of the 4 Wasm executors investigated here are written in Rust, so embedding them is efficient and well documented.
\end{itemize}

The runtime consists of a web server that can take requests from the invoker asynchronously and concurrently. Because OpenWhisk doesn't communicate with the Wasm modules directly, but through the runtime, the \inl{/start} endpoint simply returns the runtimes own network address. This endpoint also returns an identifier for the container, so that OpenWhisk can handle its life cycle, i.e. the above-mentioned mechanism of keeping it warm for some time, for a faster subsequent invocation. There is a chance that this mechanism may not be necessary for Wasm modules, as they are much more lightweight in their instantiation. We will come back to this question later in our optimization discussion.

The entry point into our runtime will be through HTTP endpoints. In order for our runtime to handle requests concurrently, we use Rust's \inl{async} features. We will not make use of any advanced HTTP features, so the choice of the web framework doesn't matter too much. We use \inl{tide} \cite{Turon2021}, an asynchronous by-default web framework ideal for prototyping. It builds on top of \inl{async-std}, the asynchronous version of the Rust standard library.
Rust's async model works with \inl{Futures} -- also known as Promises in JavaScript -- which represent values that have not yet been computed. Contrary to \inl{Promises} however, Rust's \inl{Futures} are lazy: They do nothing unless actively polled. This task falls to \inl{async} runtimes, of which \inl{async-std} provides one. In \inl{async-std} the unit of execution is a \inl{Task}, which is similar to a \inl{Thread}, except it is driven to completion by the user space runtime instead of the OS. Many of those tasks are executed on the same thread, and by default, a \inl{Thread} per logical CPU core is spawned. Thus we have $M \cdot N$ tasks in total, where $M$ is the number of tasks per thread and $N$ the number of logical CPU cores.
From that, a second distinction is implied. A \inl{Task} should not do a blocking or long-running operation, like reading from a file via the standard library's \inl{std::fs::read_to_string}. Instead, it should use the \inl{async} version of that method, from \inl{async-std}, which does not block the thread. Otherwise, all other $M - 1$ tasks, which are executed on the same thread, will also be blocked, even though they may be able to make progress. Note that, we could still process $N$ requests concurrently, even if tasks block, but that doesn't scale well enough. This is why async functions are also called cooperative routines (coroutines). When they need to wait for the OS to finish a task, they \emph{yield} to another routine, which can then continue actual work.

That becomes relevant for us, when we execute WebAssembly modules. Taking the module, setting up the Wasm runtime and executing it, is a long-running computation -- and one that could block as well. This would in turn block the thread we are currently running on, which means, only $N$ Wasm modules can be executed concurrently. This is not an issue for computationally-intensive workloads, where no more work can be done concurrently, than the CPU has logical cores. However, for I/O-bound workloads, such as requests over the network, this can quickly become a bottleneck. Since one of the appealing offerings of serverless frameworks is their elasticity, we clearly need a way to handle many more requests, even if they block.
To let other coroutines on the same thread make progress, while one executes the module, we can push the entire execution onto a thread pool. The coroutine then has the simple job to asynchronously wait for that execution to finish and collect the result. That allows the other coroutines to process more requests in the meantime.

To test the theory, whether the thread pool model is able to handle more requests, we run two short experiments.

\begin{enumerate}
    \item Compute-bound: Calculating a large prime number.
    % \item Filesystem-I/O-bound: Reading a 30 MB file from disk.
    \item Network-I/O-bound: Making a long-running (1 second) HTTP request.
\end{enumerate}

Our test machine has 8 logical CPU cores, resulting in 8 threads being started for the async-std runtime. We thus send 16 concurrent requests to be able to see the potential blocking in the result. The first experiment showed no difference in the total execution time between the default and thread pool model. As we would expect, in the default model the compute-bound task finishes the first 8 requests, before starting the second set. The execution in a thread pool executes all 16 requests concurrently, but each took roughly twice as long, resulting in no significant difference between the total execution time. The default model operates more along the first-come, first-serve principle, where an early request will also get a response more quickly. In this simplified example, where all workloads are compute-bound, this may be preferable.

\begin{figure}
    \begin{center}
        \input{figures/async_std_default_thread_vs_thread_pool.pgf}
    \end{center}
    \caption{Executing blocking Wasm workloads under the \inl{async-std} default model with 8 threads, compared with running each in a thread from a pool.}
    \label{fig:default_vs_thread_pool}
\end{figure}

The result of the second experiment is shown in figure \ref{fig:default_vs_thread_pool}. In the default model, the first 8 requests block the other 8, while they wait for the HTTP request to finish, resulting in roughly 2 seconds of total execution time. Unsurprisingly, the thread pool model finishes in half the total execution time, since it can handle all 16 requests simultaneously, due to using a new thread from the pool for each blocking operation.

Both models seem to have a workload better suited to them. However, in a real-world, mixed-workload scenario, a long-running, compute-bound function may prevent another I/O-bound one from initializing its computationally cheap request. A mixed scenario may therefore also be better served by the thread pool model.

\begin{figure}
    \begin{center}
        \input{figures/async_std_default_thread_vs_thread_pool_mixed.pgf}
    \end{center}
    \caption{The average request duration and the average amount of requests finished per second under the default and thread pool execution model, sampled over 50 runs.}
    \label{fig:default_vs_thread_pool_mixed}
\end{figure}

In figure \ref{fig:default_vs_thread_pool_mixed} a mixed scenario is run, where 16 compute-bound and 16 I/O-bound requests are executed simultaneously. In the default model, the compute-bound requests finish in about one third of the time they take under the thread pool model. This is again due to more requests being processed concurrently in the latter model. This is also why the standard deviation is slightly higher in that model. For I/O-bound workloads, there is a slight advantage for the default model. However, in the context of the plot on the right side, the thread pool model is able to handle more requests per second in general, but specifically excels at the I/O-bound ones. There is an inherent trade-off between serving a request as soon as possible, and the time it takes to finish serving it. It depends on the expected workload, which model should be used.

% TODO: Find out whether typical serverless workload is compute-bound or I/O-bound

\subsection{Optimizations}

\begin{itemize}
    \item Caching of modules. What can be created per-thread different for each runtime (wasmtime, wasmer, ...)
    \item How important is the scale-to-zero principle vs. performance?
\end{itemize}