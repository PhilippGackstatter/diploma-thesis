\chapter{Design}
\label{chapter:design}

In this chapter, we lay out the requirements that an implementation of a WebAssembly container runtime should satisfy in general. We introduce several WebAssembly runtimes and pick three of them for our implementation.
By looking at OpenWhisk specifically, we expand on the requirements and the parts that need to be modified in order to enable our new container runtime. Other approaches that could be used to run WebAssembly workloads are also detailed. Finally, we describe and discuss our concrete implementation in Apache OpenWhisk, how to achieve fast cold starts and high performance, while also examining the trade-offs involved.

\section{Core Approach}

As we have seen in the previous chapter, there are a number of issues with serverless in general, but also serverless on the edge. In this section we summarize the core problem and describe our idea for rectifying these issues to make serverless (edge) computing more viable.

A substantial cold start latency has emerged as the paramount issue of the serverless platform. It prevents serverless platforms from implementing true scale-to-zero, since functions are kept warm after invocations, wasting energy and resources. While we may not be able to get rid of the keep-alive completely, a significant reduction in cold start latency would allow a similarly large reduction in keep-alive time. Since energy and resources are even more precious on typical edge devices, this is also very important for their quality of service.
Average cold start times are on the same order of magnitude as average function execution times, thereby significantly impacting the quality of service a serverless platform can offer. Moreover, serverless is employed by developers for bursty, unpredictable workloads, a pattern that is also common at the edge. In this scenario, we see concurrent requests, which translate to concurrent cold starts. The more concurrent cold starts, the longer each one takes \cite{Cui2018}.
Operators are forced to choose between saving cost and resources or enabling a high quality of service -- in particular shorter response times. While this trade-off is inevitable to some degree, the cold start exacerbates it. A new container runtime must alleviate that latency, to provide mutual benefits for the serverless user and the operator.

Serverless enables the execution of »polyglot tenant-provided code« \cite{Nastic2018}. A new runtime must keep up these principles of being programming language agnostic to continue enabling all developers to use the serverless platform.
To allow for multi-tenancy on the same physical host, containers need to be sandboxed securely. As described in the previous chapter, WebAssembly provides sandboxing and support for many languages with support for more likely to come. At the same time, it is a lighter-weight solution than virtualization with Docker containers. In contrast to Docker, its isolation does not depend on operating system features, which may incur a performance cost, on the downside. Thus, it seems like a good candidate to replace Docker containers in serverless platforms while alleviating the cold start latency. This is also why WebAssembly may fare better on low-power edge devices.

\citeauthor{Mendki2020} compared WebAssembly and Docker container startup times and found WebAssembly being 91\% faster than containers and used 75\% less memory \cite{Mendki2020}. \citeauthor{Hall2019} found their WebAssembly-based serverless platform fared 60\% better, on average, than the Docker container solution, for their concurrent »Multiple Client, Multiple Access« workload \cite{Hall2019}. These promising results are the basis for our work and validate the core idea of the approach.

\section{Requirements}

Some of the requirements for a WebAssembly container runtime are a direct consequence of the issues we have assessed. We expand on others in the following sections. The summary of concerns and requirements are:

\begin{enumerate}
    \item Be easily integratable with an existing serverless framework.
    \item Be programming-language agnostic.
    \item Provide a cheap sandboxing mechanism to ensure multi-tenant capability and no adverse effects on the cold start latency.
    \item Be as close to native speed as possible to be a viable alternative to existing container runtimes.
    \item Run on devices with potentially different instruction set architectures (at least \inl{x86\_64} and \inl{arm}).
    \item Be able to efficiently handle a large amount of I/O concurrently, due to all requests being proxied through.
    \item Be thread-safe.
\end{enumerate}

Finding a suitable serverless framework that we can modify and integrate with, will be the topic of the next section. In Chapter \ref{chapter:background} and the preceding section, we have seen how WebAssembly provides some of these requirements, such as sandboxing, good execution speed and language-agnosticism. However, for our implementation we need concrete WebAssembly runtimes that provide these properties, so we will explore the options for such runtimes and work out the differences. The choice for which language and libraries to implement our executor with will be driven by how the Wasm runtimes we choose can be integrated. Other concerns include having access to highly performant, concurrent I/O and support for correctness guarantees, including thread-safety.

\subsection{Apache OpenWhisk}

We use Apache OpenWhisk as the framework to implement WebAssembly support for. OpenWhisk is a production-ready framework, used as the basis for IBM's Cloud Functions. Thus, implementing Wasm support in OpenWhisk would show the viability of the approach in the real world. The framework is also frequently used by serverless researchers to implement new ideas to reduce cold start times \cite{Mohan2019} or Wasm support via \inl{node.js} \cite{Hall2019}, making it a well-respected and frequently used framework. OpenWhisk also has \emph{lean} components, which are lighter-weight replacements for its heavier ones, such as Kafka. These enable OpenWhisk to be deployed on edge devices where the available memory is limited, such as Raspberry Pis, on which we plan to evaluate our implementation. Furthermore, because OpenWhisk is already designed to be easily extensible with new language runtimes, the protocol is straightforward to implement.

\section{WebAssembly Runtimes}
\label{section:wasm-runtimes}

WebAssembly is a specification\footnote{\url{https://webassembly.org/specs/}} and implementations adhering to it are WebAssembly runtimes. Since Wasm was originally designed for the web, the virtual machines of browsers, such as Google's V8 or Mozilla's SpiderMonkey, can also execute WebAssembly in addition to JavaScript. With the rise of Wasm outside the Web, a large number of standalone runtimes have emerged and many are actively being developed. This section introduces a number of runtimes with different compilation strategies, and justify our choice of runtimes to implement in our executor.


\subsection{Wasmtime}

Wasmtime\footnote{\url{https://github.com/bytecodealliance/wasmtime}} is a runtime that has support for just-in-time compilation, available for the \inl{x86\_64} and \inl{aarch64} instruction set architectures (ISA). It is written in Rust and thus easily embeddable from there. Support for the latest WASI standards and future proposals for Wasm are implemented.
It is developed as part of the Bytecode Alliance\footnote{\url{https://bytecodealliance.org/}}, with founding members like Mozilla, fastly, Intel and RedHat. The \inl{cranelift} code generator is developed as part of the alliance and \inl{wasmtime} uses it for just-in-time compilation. The code generator's documentation points out that the backend for \inl{x86\_64} is »the most complete and stable; other architectures are in various stages of development \cite{Alliance2021}.« 
Thus, it will most likely run on an \inl{aarch64} Raspberry Pi, but performance may not be on-par with an \inl{LLVM}-based runtime.

\subsection{Lucet}

Lucet\footnote{\url{https://github.com/bytecodealliance/lucet}} is the compiler and runtime used for cloud provider fastly's Terrarium, a platform for running WebAssembly on edge devices \cite{fastly2019}. It consists of a compiler and a runtime to execute WebAssembly. The compiler, \inl{lucetc}, compiles Wasm modules to native object or shared object files. The complementary \inl{lucet-runtime} can then load the native code and call the contained Wasm functions. Because of this compilation model, execution can reach near-native speeds. However, the execution is not sandboxed by default, requiring mechanisms like \inl{seccomp-bpf}\footnote{\url{https://wiki.mozilla.org/Security/Sandbox/Seccomp}}, and the security model is actively being developed. It is currently only available on the \inl{x86\_64} ISA. Since we require support for smaller edge devices, often built on the \inl{arm} ISA, we will not be using \inl{lucet}. Furthermore, the lack of mature sandboxing makes other runtimes a better choice.

\subsection{Wasmer}

Wasmer\footnote{\url{https://github.com/wasmerio/wasmer}} is a WebAssembly runtime with support for the three major platforms -- Linux, Windows and macOS -- and \inl{x86\_64} and \inl{aarch64} ISAs. Notably, \inl{wasmer} has already reached version \inl{1.0}, indicating a certain level of maturity. The runtime offers three different compilers, \inl{singlepass}, \inl{cranelift} and \inl{LLVM}, whose compilation times get slower and execution times get faster, in that order, respectively. The runtime has a JIT and a native engine, which differ in the way the module is loaded. In particular, the native engine produces a shared object, which can be loaded with a fast \inl{dlopen} call. The support for high-quality AoT compilation with \inl{LLVM} and fast startup times makes \inl{wasmer} a promising choice for our purpose.

\subsection{WASM3}

Due to Wasm's simplicity in the binary format, there is wide-spread interest for running it on embedded devices. An example of that is \inl{wasm3}\footnote{\url{https://github.com/wasm3/wasm3}}, a Wasm interpreter that claims to be the fastest. They argue in favor of interpretation over JIT compilation, since startup latency is more important in many situations. Furthermore, portability and security are easier to achieve with this approach and development with the virtual machine is simpler. Still, a performance penalty of 4 to 5$\times$ compared to JIT or 12$\times$ compared to native execution, leads us to believe that no amount of startup latency gain can make up for the performance loss. Even though our primary goal is that of reducing the cold start latency, the execution that follows cannot massively lag behind Docker containers, if adoption is to succeed. The relationship between startup and execution performance is too far out of balance in this case.

\subsection{WebAssembly Micro Runtime}

However, there is still value in runtimes written specifically for embedded devices. The WebAssembly Micro Runtime\footnote{\url{https://github.com/bytecodealliance/wasm-micro-runtime/}} (\inl{wamr}) has support for all three of the mentioned compilation strategies. It has low memory usage and binary size. For example, the claimed binary size of the AoT runtime is just 50 kB. Further, the AoT runtime claims near-native speed. For execution on edge devices, where memory is scarce, this runtime may allow more modules to be kept in-memory compared to heavier-weight runtimes.

A summary of the runtimes we looked at can be found in Table \ref{table:wasm-runtime-overview}. The diversity of compilation strategies sticks out -- clearly no single mode is the best for all scenarios. Except for \inl{lucet}, all runtimes have good platform support. However, particularly for edge devices, the lack of support for the 32-bit \inl{arm} architecture can be challenging in the Rust-written runtimes (\inl{wasmtime}, \inl{wasmer}, \inl{lucet}). The support in the other ones, written in C, is there.

\newcommand{\linux}{\input{icons/linux.pdf_tex}}
\newcommand{\macos}{\input{icons/macos.pdf_tex}}
\newcommand{\windows}{\input{icons/windows.pdf_tex}}
\newcommand{\freebsd}{\input{icons/freebsd.pdf_tex}}
\newcommand{\android}{\input{icons/android.pdf_tex}}
\newcommand{\ios}{\input{icons/ios.pdf_tex}}

\begin{table}[h!]
    \centering
    \begin{tabular}{c | c | c | c}
        Runtime        & ISA support & Compilation  & Platforms\\
        \hline
        \inl{wasmtime} & \inl{x86\_64}, \inl{aarch64} & AoT, JIT & \linux \hspace{1mm} \macos \hspace{1mm} \windows\\
        \hline
        \inl{lucet}    & \inl{x86\_64} & AoT & \linux \hspace{1mm} \macos\\
        \hline
        \inl{wasmer}   & \inl{x86}, \inl{x86\_64}, \inl{aarch64} & AoT, JIT & \linux \hspace{1mm} \macos \hspace{1mm} \windows\\
        \hline
        \multirow{2}{*}{\inl{wasm3}} & \inl{x86}, \inl{x86\_64}, \inl{arm}, & \multirow{2}{*}{Interpreted} & \linux \hspace{1mm} \macos \hspace{1mm} \windows\\
        & \inl{RISC-V}, ... & & \freebsd \hspace{1mm} \android \hspace{1mm} \ios\\
        \hline
        \multirow{2}{*}{\inl{wamr}} & \inl{x86}, \inl{x86\_64}, \inl{arm}, & \multirow{2}{*}{AoT, Interpreted} & \linux \hspace{1mm} \macos\\
        & \inl{aarch64}, ... & & \windows \hspace{1mm} \android \hspace{1mm} ... \\
    \end{tabular}
    \caption{WebAssembly runtime feature overview.}
    \label{table:wasm-runtime-overview}
\end{table}

\section{WebAssembly in OpenWhisk}

In this section, we first examine the possibilities for bringing WebAssembly support to OpenWhisk before exploring the available options and discussing them in the context of our requirements.

\subsection{OpenWhisk's Anatomy}

In order to understand the requirements for our Wasm-flavored OpenWhisk, we first need to understand the design of OpenWhisk itself.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/OpenWhiskActionInvocationFlow.pdf}
    \caption{The invocation flow of an action in Apache OpenWhisk, based on the source code and documentation \cite{OpenWhiskSystemDesign}. Potentially many Invokers can exist on different hosts in this setup.}
    \label{fig:openwhisk-action-invocation-flow}
\end{figure}

Figure \ref{fig:openwhisk-action-invocation-flow} sketches the design of OpenWhisk. Users interact with it through its API Gateway, which consists of \inl{nginx}, mostly for SSL termination, and the \inl{Controller}, which implements the main logic. It handles the creation of actions - OpenWhisk's name for serverless functions - authentication and authorization as well as invoking the action.
In the latter case, the controller uses its load balancer to determine one of the potentially many \inl{Invoker}s, that should handle the request. The \inl{Invoker}s are subscribed to an Apache Kafka queue, to which the load balancer publishes the request. The controller and its load balancing apparatus take a central role in OpenWhisk, but in our figure it is represented by the API Gateway and load balancer only, simply because we will not be modifying this part.
Our focus is on the \inl{Invoker}. It is the part of OpenWhisk responsible for \emph{invoking} the action. A number of steps are involved in that process \cite{OpenWhiskSystemDesign}.

\begin{enumerate}
    \item It retrieves the action's code and execution permissions from an Apache CouchDB database, where the Controller stored it during action creation.
    \item It instructs the Docker daemon to start a new container, based on the runtime that the action needs to execute. This could be a \inl{openwhisk/action-nodejs-v10} image for JavaScript actions, but could also be a generic \inl{openwhisk/dockerskeleton} black-box image that can execute any binary, such as one written in Rust or C.
    \item Finally, it injects the action's code into the container, invokes it with the given parameters and returns the result \cite{OpenWhiskSystemDesign}.
\end{enumerate}

Because OpenWhisk supports many runtimes and even the mentioned black-box image, it needs a common protocol to communicate with all of them. The just-described step 3 is the gist of that protocol. Each runtime needs to implement a number of functions. The \inl{start} function creates a new container and returns its address. The following requests to the \inl{/init} endpoint are then sent to this address, where the container receives the code and takes the necessary steps to make the action ready for execution. Thus, over a container's lifecycle, this endpoint is called exactly once. Once initialized, the \inl{/run} endpoint can be called multiple times to execute the action \cite{OpenWhiskSystemDesign}.

This implies that the container is not destroyed immediately after it has been invoked. Indeed, OpenWhisk, just like other known serverless platforms, uses various optimizations of the described flow, in order to improve the system's performance. Among those are pre-warming containers or keeping the container running for some time after it has been invoked. Both have the same effect: When the user wants to invoke the action, no initialization is necessary and \inl{/run} can be called immediately. These optimizations exist precisely because the cold start has been recognized as an expensive operation. In the absence of invocations and some threshold time elapsed, the container is removed entirely. In that case, the container runtime's \inl{destroy} function is called, which removes the container, thereby freeing up the memory it uses \cite{OpenWhiskSystemDesign}. These optimizations are part of the OpenWhisk logic and independent of the underlying container runtime. Our container runtime will profit from them too, but we have to take them into account in our design.

\subsection{OpenWhisk on Kubernetes}

OpenWhisk can be deployed on a Kubernetes cluster, a container orchestration tool for automating deployment and scaling of containers \cite{Kub2021}. OpenWhisk has a service provider interface (SPI) called \inl{ContainerFactoryProvider}, which represents the underlying container management platform. It provides two implementations of that SPI \cite{OWKub2020}.

\begin{enumerate}
    \item The \inl{DockerContainerFactory} implements the architecture shown above, except that most components of OpenWhisk run in separate Kubernetes pods. In non-Kubernetes deployments, they usually run in separate Docker containers. The crucial part of this deployment is that the \inl{Invoker} runs in a single pod and also spawns all of the Docker containers inside that pod. Multiple \inl{Invoker}s can run on different worker nodes. Because OpenWhisk talks directly to the local Docker daemon, container management latency is rather small, but this SPI implementation does not take advantage of Kubernetes' scaling mechanisms.
    \item The \inl{KubernetesContainerFactory} is the counterpart. It interfaces with the Kubernetes API to create and schedule actions in containers -- in turn contained in pods -- to run on different Kubernetes worker nodes. This trades higher container management latency for access to Kubernetes' scaling and scheduling mechanisms.
\end{enumerate}

\subsection{Executing WebAssembly with OpenWhisk}

To execute Wasm modules in OpenWhisk, we have two similar options with similar trade-offs.

\begin{enumerate}
    \item The first is using one of a number of WebAssembly runtimes that are currently being developed and are in various stages of maturity. We would need to embed one of these runtimes in a program of our own, which provides a similar interface as Docker. This WebAssembly container runtime would be able to replace the Docker daemon in the architecture presented above, and run Wasm modules instead of Docker images. Since this program would sit in-between OpenWhisk and the Wasm runtime, we would have precise control over the management of the modules, i.e. whether the Wasm module is just-in-time compiled, interpreted or compiled to native code ahead-of-time; the caching layer that keeps initialized modules ready for fast execution; or the configuration of the system interface with which Wasm modules can interact with the underlying operating system.
    \item The second option is to use Kubernetes as the management system for our Wasm modules. Kubernetes is generic over the underlying container runtimes, and as such it runs on \inl{containerd}, \inl{CRI-O} and Docker \cite{Kub2021}. However, it is also possible to let Kubernetes pods run on other runtimes by implementing the \inl{kubelet} API. The \inl{krustlet} project provides a \inl{kubelet} implementation that can execute Wasm workloads \cite{Krustlet2021}. With that as the container runtime setup, we can instruct Kubernetes to run Wasm modules for us, instead of the usual container image.
\end{enumerate}

We will explore those two options in more detail and discuss which may be a better fit for our requirements.

\subsection{Krustlet: The WebAssembly Kubelet}

As a \inl{kubelet}, \inl{krustlet} runs on Kubernetes worker nodes and accepts workloads of the \inl{wasm32-wasi} architecture. Just like a regular \inl{kubelet}, \inl{krustlet} will then pull the Wasm module from an Open Container Initiative (OCI) compatible registry, like a Docker image can be pulled from Docker Hub. This process allows us to schedule our Wasm workloads from an \inl{Invoker}, similar to how the \inl{KubernetesContainerFactory} implementation schedules actions in Docker containers. However, this execution model is also different from the OpenWhisk model in some ways.

\begin{itemize}
    \item In OpenWhisk action runtimes, there is a separation between the runtime environment, provided through the Docker image, and the action code, that is stored in a database, retrieved from there and injected into the container. \inl{krustlet} expects the Wasm module to be the equivalent of the container, but with the code baked in. Thus, no injection is necessary and there is no separation. This is an incompatibility with the current OpenWhisk model, since the Wasm module does not need to be stored in the database, but rather in the OCI registry. To implement this in OpenWhisk, we would have to modify where the module is stored, as well as remove the retrieval from the database and the subsequent injection, which are then useless operations. The OCI registry would become part of the OpenWhisk deployment, for latency reasons alone. We would not expect the cold start to be significantly affected by these changes. Not having to retrieve the code from the database but pulling the image from the registry instead should roughly be time equivalent operations.
    \item Since there is no separation anymore, every Wasm module needs to implement the OpenWhisk runtime protocol itself, instead of being able to rely on the runtime implementing it once. In practice, this means implementing the \inl{/init} and \inl{/run} endpoints, where the former would no longer do anything useful, since there is no code to inject. It follows that every Wasm module needs to include an HTTP server; increasing the size of its binary and runtime cost above what would be needed only for its unique functionality. Of course, the current OpenWhisk version implements it in the same way: Each container must come with an implementation of the OpenWhisk protocol. Thus, we can assume that the extra cost is not substantial. An advantage of this approach is that the system is more resilient, since one container failure does not affect others.
    % Because network support in WASI is still under active development, the current alternative is to implement a \inl{wasmcloud-actor}, which essentially provides capabilities such as networking to Wasm modules \cite{WC2021}. The function can then be run by calling the \inl{/run} endpoint. Just like containers in OpenWhisk keep running for some time after having been called once, the module would also continue to listen for more requests to avoid the cold-start, for some time.
    \item Evidently, Docker container operations are already barely fast enough to facilitate serverless platforms. As the OpenWhisk developers pointed out in the documentation for their \inl{KubernetesContainerFactory}, Kubernetes pod management operations come with an even higher latency. More generally, Kubernetes is a tool that has not primarily been designed for serverless platforms. While some platforms like OpenFaaS or Knative\footnote{\url{https://knative.dev/}}  do depend on it, they also suffer from significant cold starts. It is therefore reasonable to reexamine this choice.
    Because of these aspects, we have concerns about \emph{not} alleviating the cold start latency with this approach. What we may win by starting Wasm rather than Docker containers, we may lose through higher management latency.
    \item Because network support in WASI is still under active development, the current alternative is to implement a \inl{wasmcloud-actor}, which essentially provides capabilities such as networking to Wasm modules \cite{WC2021}. This would be necessary due to the aforementioned HTTP server requirement. The \inl{wasmcloud} uses either \inl{wasm3} or \inl{wasmtime} as the underlying WebAssembly runtime. In the preceding section, we have excluded \inl{wasm3} for its lack of performance. \inl{wasmtime} uses a JIT compiler, which compiles the module when it first receives the code. Recall that our fourth requirement is near-native execution speed. Compared to embedding a runtime, the \inl{wasmcloud} approach does not give us the same level of control over the compilation strategy and execution. However, having control over these aspects is a priority, since these can substantially affect startup and execution time. A JIT compiler, in its typical usage, may impose a substantial startup tax.
\end{itemize}

While this approach has its merits, we believe that there would be higher friction for implementing it in OpenWhisk than writing our own purpose-built Wasm executor.

\subsection{An OpenWhisk-WebAssembly Executor}

% It provides more control over optimizing the cold-start latency as well as the execution performance, which are part of the requirements we laid out.

The first approach allows us to integrate Wasm into OpenWhisk with fewer changes to it. Furthermore, the more precise control over container management as well as the compilation strategy and execution are the main arguments in favor of this approach. These were some of the main requirements we laid out. We therefore implement our own layer between OpenWhisk and different WebAssembly runtimes which enable the execution of Wasm modules. It will implement the OpenWhisk protocol and manage WebAssembly containers. We refer to this as the Wasm executor, to distinguish it from Wasm runtimes -- those responsible for the actual execution of Wasm modules. We aim to replace the Docker daemon in the architecture shown in Figure \ref{fig:openwhisk-action-invocation-flow}, leveraging the existing OpenWhisk interface for container management.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/WasmOpenWhiskActionInvocationFlow.pdf}
    \caption{Invocation flow of a Wasm action in our modified Apache OpenWhisk. The \inl{Invoker} \emph{injects} the code into the executor which \emph{creates} a Wasm module ready for execution. It then \emph{instructs} the executor to \emph{invoke} the module with the parameters it passes. The result is passed back to the Invoker. Again, a deployer can define whether one or more Invokers exist.}
    \label{fig:wasm-openwhisk-action-invocation-flow}
\end{figure}

In Figure \ref{fig:wasm-openwhisk-action-invocation-flow}, we have sketched the high-level OpenWhisk modifications necessary for the integration. In this invocation flow, we need to modify OpenWhisk's \inl{Invoker} such that it communicates with the Wasm executor instead of the Docker daemon. Fortunately, the \inl{Invoker} is already well-separated from the concrete containerization technology through a Service Provider Interface (SPI). As previously mentioned, OpenWhisk supports Docker as well as Kubernetes through the \inl{ContainerFactory} trait. We add a \inl{WasmContainerFactory} that implements this trait. Via a property, we can instruct OpenWhisk to use this factory. It uses a \inl{WasmClient}, that can start and destroy a container by invoking the endpoints of the same name on the Wasm executor. The start function returns its result as a \inl{WasmContainer} that extends OpenWhisk's \inl{Container} abstraction. This trait handles container communication, such as calling the \inl{/init} and \inl{/run} endpoints. It also implements \inl{suspend} and \inl{resume} functions, which we do not implement. Docker containers potentially use CPU cycles and memory while they are up, so OpenWhisk pauses them between invocations, which suspends all of its processes and prevents them from using the CPU. Containers in our executor, on the other hand, consume only memory but no CPU in-between requests, so pausing is not a meaningful operation. The \inl{Container} also allows collecting the logs that an action produced. This is not strictly necessary functionality for correct operation, so we do not implement it. Since we want to implement three runtimes in our executor, keeping the requirements for each of them minimal is important. However, implementing a log mechanism with WASI is entirely possible.

In OpenWhisk's Docker container implementation, OpenWhisk only uses the Docker daemon to create a container, but then communicates with the container directly. In our implementation, every request is proxied through the Wasm executor to the container itself. This design presupposes that the Wasm executor can handle a large amount of concurrent requests, which we noted in our requirements. The advantage is, that not every Wasm module needs to implement the OpenWhisk protocol, as discussed under the \inl{krustlet} topic before. The disadvantage is the single point of failure. However, that is mitigated by the bigger picture of OpenWhisk. In a truly resilient deployment, we would have multiple \inl{Invoker}s. A failing Wasm executor would let the entire \inl{Invoker} unit fail, but other \inl{Invoker}s could take over requests. Furthermore, the executor itself could be scheduled as, e.g. a Kubernetes pod with a restart policy of \inl{Always}. Finally, because WebAssembly runtimes isolate faults of their containers, it is impossible for a faulty Wasm module to affect the entire executor -- barring bugs in the runtimes. The absence of bugs in the executor and the runtimes -- or, at least as small likelihood of their existence -- is therefore also a basic requirement. The mentioned handling of concurrent requests further implies that thread-safety is another one. We noted both of these requirements under the umbrella term of program correctness.
The executor also needs to be generally fast, in order to aid in alleviating the cold start problem. That includes efficient handling of receiving the module's code from OpenWhisk, instantiating it and setting up the underlying Wasm executor. In order to enable execution on all kinds of platforms, including edge devices, another requirement is the executor's ability to run on different instruction set architectures.

% Figure that shows a zoomed-in version of the invoker and its SPIs (as well as the Wasm executor?) - Help better understand OW's design and what we added

\section{Executor Implementation}


To implement these requirements in Apache OpenWhisk, we take the following steps. The current Docker-based \inl{Invoker} is replaced to forward all the incoming requests to our new Wasm executor.

The Wasm executor is written from scratch in the Rust\footnote{\url{https://www.rust-lang.org/}} programming language. There are a number of reasons for this choice.

\begin{itemize}
    \item It is a compiled language with performance similar to that of C or C++.
    \item The Rust compiler uses \inl{LLVM} as its backend so Rust programs can be compiled for any architecture we care about, in particular also \inl{aarch64}. Moreover, we can also compile to \inl{wasm32-wasi}, the WebAssembly WASI target. This allows us to write highly performant WebAssembly modules in Rust, such that we can test our executor \emph{and} the Docker actions under the best possible conditions.
    \item It has good support for asynchronous I/O, so Rust web frameworks can efficiently handle a large amount of concurrent requests.
    \item Program correctness is one of Rust's primary goals, which is enabled through a large amount of guarantees that can be checked at compile time. In particular, writing memory-safe programs is much easier than in C or C++, while providing a similar level of performance.
    \begin{itemize}
        \item A subcategory of these checks is particularly worth pointing out: Rust allows for »fearless concurrency«, that is, having concurrent programs checked for thread-safety at compile time. In particular, the compiler can detect subtle bugs like data races.
    \end{itemize}
    \item Two of the three used Wasm runtimes are written in Rust, so embedding them is well documented, straightforward and their APIs tend to be the most idiomatic, powerful and up-to-date. The other Wasm runtime, \inl{wamr}, is written in C, which can be efficiently embedded from Rust through bindings.
\end{itemize}

\begin{listing}[ht]
    \begin{minted}{rust}
#[async_std::main]
async fn main() -> anyhow::Result<()> {
    #[cfg(feature = "wasmtime_rt")]
    let runtime = ow_wasmtime::Wasmtime::default();

    #[cfg(feature = "wasmer_rt")]
    let runtime = ow_wasmer::Wasmer::default();

    #[cfg(feature = "wamr_rt")]
    let runtime = ow_wamr::Wamr::default();

    let mut executor = tide::with_state(runtime);

    executor.at("/:container_id/init").post(core::init);
    executor.at("/:container_id/run").post(core::run);
    executor.at("/:container_id/destroy").post(core::destroy);

    executor.listen("127.0.0.1:9000").await.unwrap();

    Ok(())
}
\end{minted}
    \caption{The main function of our WebAssembly executor. The \inl{\#[cfg(feature = "...")]} macros allow us to define multiple runtimes while deciding at compile-time which of them to enable. Thus, we can build three separate binaries, with each only containing the code necessary for its runtime.}
    \label{listing:wasm-executor-main}
\end{listing}

The main function in Listing \ref{listing:wasm-executor-main} suits itself well to describe the high-level architecture of the executor. The entry point into our executor will be through HTTP endpoints. We do not make use of any advanced HTTP features, so the choice of the web framework does not matter too much. We use \inl{tide} \cite{Turon2021}, an asynchronous by-default web framework ideal for prototyping. We discuss asynchronous Rust in greater detail in Section \ref{section:enabling-concurrency}. Suffice it to say, that the \inl{\#[async\_std::main]} macro starts this program in an asynchronous runtime, which enables us to use \inl{async} functions and \inl{await} them. The async runtime allows multiple concurrent requests to any of the endpoints, without us having to spawn threads explicitly.

We instantiate one of our three implemented Wasm runtimes, only one of which is compiled into the executable, based on the feature flag we pass to the compiler. The runtime is used as \emph{state}, in \inl{tide} terminology, which is to say that it is injected into each request and thus shared among requests. Sharing objects among threads requires them to be thread-safe. This is where Rust's compile-time checks help us ensure this property. The \inl{tide::with\_state} function can take any argument that implements three traits: \inl{Clone}, \inl{Send} and \inl{Sync}. The first simply gives objects implementing it the ability to be duplicated. This is necessary, such that each request can receive a copy of the runtime. However, a deep copy would be too expensive for each request. Our runtime wrappers therefore implement \inl{Clone} as a shallow copy, that is, they create a new reference to the same inner object that all other wrapper objects also point to. For this to work safely, we need the other traits. These are only \emph{marker traits}\footnote{\url{https://doc.rust-lang.org/std/marker/index.html}}, indicating that an object satisfies a property. \inl{Send} means a type can be transferred across thread boundaries, while \inl{Sync} means that references to that type can be shared between threads. These properties are satisfied, if the types that we use in the definition of our runtimes and those used transitively, have these properties. That is all to say that the types of the runtimes we use need to be thread-safe, i.e. implement those marker traits. Here is where the differences between the runtimes start to emerge, but we discuss those in a later section.

To start a container, the \inl{WasmContainer} object in OpenWhisk generates a new universally unique identifier (uuid), without having to communicate with the Wasm executor at all. This is possible because there is no setup for a Wasm container, that needs to happen ahead-of-time, like network namespace creation for Docker containers.
That shows an important aspect of what constitues a WebAssembly container. In-memory, it is ultimately only represented by the action's code. No external operating system resource is used.
It follows, that actual work only happens in the endpoints, which are parameterized by the container id. \inl{/init} is called once by OpenWhisk per container to initialize the container with the module's code. Although different for each runtime, in general, this endpoint will do any work that can be frontloaded. \inl{/run} is invoked potentially many times, so any work that is not frontloaded would multiply there. Of course, \inl{/init} is the potential culprit for the cold start latency, so our focus is on reducing even the amount of work this endpoint needs to handle. We describe the optimizations towards that end, later in this chapter. Once OpenWhisk decides to destroy the container, it does so through \inl{/destroy}.

% Finally, we give \inl{tide} a function pointer for each endpoint and listen on a local address.

\subsection{Fast Cold Starts}
In serverless platforms we can identify three important steps, in order for code to be executed. Uploading the code to the platform, initializing the execution environment, and running it. The latter two steps are already part of the execution path; they occur when an execution request is actively waiting for the result to be returned. Thus, during initialization, the action should already be well-optimized for execution such that as little preparation as possible is needed. WebAssembly is a binary target format, however, not one that is ready for \emph{fast} execution. It can be interpreted right away -- thus a small cold start time -- but execution performance will lag behind massively compared to native code, which is unacceptable. So compilation to native code is essential for good runtime performance, but it also takes time for that to be applied to a Wasm module. Given this trade-off, it would seem that just-in-time compilation should be the best model. In \inl{wasmtime}, our JIT-runtime of choice, compilation of a simple, WASI-enabled module (1,6 MB) still takes 35 milliseconds on our test machine (or 40 milliseconds when optimizing for speed). This time represents 98\% of the entire setup time, including every other part of the runtime. Thus, unsurprisingly, the cold start is defined almost entirely by the compilation phase. For comparison, the cold start time of the \inl{hello-world} Docker image on the same machine is 452 milliseconds. This number is roughly in line with the cold start times of AWS Lambda or Google Cloud Function, that we have cited in Chapter \ref{chapter:background}. While the JIT cold start is an order of magnitude faster than the startup times of Docker containers, it is arguably still on the same order of magnitude as the execution time of some actions. Given that the compilation phase takes up such a big chunk of the overall time, it is the most obvious starting point for further optimizations. And there is room for improvement.

In contrast to browsers, which receive JavaScript or WebAssembly just-in-time, and thus need to compile it in the same manner, a serverless platform takes ownership of a module earlier. When a user uploads a module, the platform has an almost unconstrained amount of time to apply optimizations to the module thereby making it ready for a fast startup \emph{and} execution, solving the previously described dilemma. Thus, the key is to run the expensive module creation ahead of the time of execution. It does not matter in this case, whether the actual compilation strategy of the underlying runtime is JIT or AoT. It is obvious for AoT - the code needs to be compiled ahead-of-time in its entirety, by definition. As we have seen, JIT also has a cold start penalty and we can complete that step ahead-of-time, too. We refer to this as \emph{precompilation}. Once the module is precompiled, we can serialize it and store it in the database. During initialization, all that is left to do is to deserialize precompiled bytes into a \inl{Module} and keep it in memory, which is a very fast operation. At the time of execution, the module is ready to produce an instance and execution can start immediately.

Both \inl{wasmer} and \inl{wamr} support AoT compilation. \inl{wasmer} in particular, allows us to leverage \inl{LLVM} for highly optimized native code to produce a shared object. The runtime loads this via a fast \inl{dlopen} call. \inl{wamr} produces its own AoT format which it can deserialize at runtime and achieve a fast startup as well. For \inl{wasmtime} it is the same in all but the name. We can run the relatively expensive module creation ahead-of-time, and serialize and store the result. Thus, all the runtimes can support some form of precompilation.

\subsection{Warm Starts}

Even though the cold start of our executor is well-optimized with this technique, cold starting for every single invocation would still be too expensive for sequential invocations. After all, the \inl{init} procedure still requires retrieving the module from the database and copying it to the executor. We can save on that too, by simply taking advantage of OpenWhisk's current design. In order to adhere to it, we need to store the module between \inl{init} and \inl{run}. The advantage of that is, of course, that after the initialization, \inl{run} can be called again and again, without having to go through any of the setup. This is how OpenWhisk works with Docker containers already. After the initial cold start and first invocation, OpenWhisk pauses the container. If another subsequent request to the same action comes in, it unpauses the container and the action invocation experiences a so-called warm start. This scenario skips the expensive cold start and is much faster. After a threshold without any invocations -- which defaults to ten minutes -- OpenWhisk calls the \inl{destroy} function on the container, removing it entirely.
Thus, by adhering to OpenWhisk's design, we have support for fast warm starts as well.

The advantage of keeping a module in the executor's memory, is that it is already deserialized and new instances can be instantiated from it immediately. An instance is the runtime's representation of a callable Wasm module. If a module is the equivalent of a Docker image, then the instance is the running Docker container. Note that, unlike Docker-based OpenWhisk, we do not keep instances (containers) in-memory, but only the modules (images). The cost of instantiating an instance is negligible. On our test machine it took, on average, 340 microseconds to setup a \inl{wasmtime} instance and the parts needed for execution. This represents the overhead on every warm invocation. Moreover, two instances can execute concurrently, even if the module itself is not thread-safe, for instance when reentrant execution would be unsafe. Finally, implemented this way, it would also be safe to execute two requests from different users, since instances are isolated from each other. They operate on their own memory space and have their own WASI environment. Thus, caching modules rather than instances is a good trade-off.

\subsection{The WasmRuntime Abstraction}

\begin{listing}[ht]
    \begin{minted}{rust}
pub trait WasmRuntime: Clone {
    fn initialize(
        &self,
        container_id: String,
        capabilities: ActionCapabilities,
        module: Vec<u8>,
    ) -> anyhow::Result<()>;

    fn run(
        &self,
        container_id: &str,
        parameters: serde_json::Value,
    ) -> Result<Result<serde_json::Value, serde_json::Value>, anyhow::Error>;

    fn destroy(&self, container_id: &str);
}
\end{minted}
    \caption{The \inl{WasmRuntime} abstraction that each of our Wasm runtime wrappers implements. Note the previously mentioned \inl{Clone} trait that is required for using a \inl{WasmRuntime} object as \inl{tide} state. This is required here through the \emph{trait bound}, which means the type implementing this trait needs to also implement \inl{Clone}.}
    \label{listing:wasm-executor-trait}
\end{listing}


The Wasm executor is generic over the underlying WebAssembly runtime, such that we can easily implement different ones and facilitate our comparison. To that end, we abstract the common runtime tasks into a trait -- essentially an interface in Rust's terminology. It is shown in Listing \ref{listing:wasm-executor-trait}. The state of our executor can be any object that implements this trait. For each Wasm runtime we have selected, we implement a wrapper, each of which implements the \inl{WasmRuntime} trait. All our wrapper implementations work in similar ways and differ in details. We describe the implementation for each trait method in the wrappers, while pointing out the important differences between runtimes. Afterwards, we discuss alternatives.

\subsubsection{Initialization}
The executor decodes the \inl{base64} string given by OpenWhisk and unzips it, before calling \inl{initialize} with the result. The function initializes the container identified by the given id. All wrappers store the code they are given in a thread-safe HashMap -- provided by the \inl{dashmap}\footnote{\url{https://crates.io/crates/dashmap}} library.

A note on WebAssembly runtime terminology first, based on \inl{wasmtime}'s\footnote{\url{https://docs.rs/wasmtime}} and \inl{wasmer}'s\footnote{\url{https://docs.wasmer.io/integrations/examples/instance}} documentation and the WebAssembly specification\footnote{\url{https://webassembly.github.io/spec/core/exec/runtime.html}}. An engine is used for compiling and managing modules. Recall that a module represents compiled WebAssembly code, similar to a container image, and an instance is the callable instantiation of that code, i.e. the running container. A store is necessary to hold on to memories, tables, functions and other auxiliary data structures.

\begin{description}[style=multiline, leftmargin=2.5cm, font=\bfseries]

    \item[Wasmtime] In \inl{wasmtime}, we instantiate a single, global \inl{Engine} when the executor starts, which is then used to instantiate all \inl{Module}s. The bytes passed through \inl{initialize} are deserialized into a \inl{Module} -- a fast operation -- and then stored in the internal HashMap. This is only possible because a \inl{Module} is thread-safe so the concurrent HashMap it is contained in can also be safely shared across threads.

    \item[Wasmer] In \inl{wasmer} we also store an \inl{Engine} globally, and instantiate a \inl{Store} per \inl{Module}. In contrast to \inl{wasmtime}, the \inl{Store} is thread-safe and could be stored globally as well. However, it holds on to the memory-heavy parts of the \inl{Module} and these would not be freed even if the \inl{Module} was. Thus, until the executor terminates, the \inl{Store} would only grow in size. Like in \inl{wasmtime}, the \inl{Module} is thread-safe and thus enables our pre-creation optimization of them in the first place.

    \item[Wamr] \inl{wamr} is originally written in C, and we use \inl{wamr\_sys}\footnote{\url{https://crates.io/crates/wamr\_sys}} bindings to embed it in Rust. Contrary to the other runtimes, a module in \inl{wamr} is not thread-safe. Thus, only the serialized bytes representing the module are stored in the HashMap. It follows that the deserialization happens in every \inl{run} call instead, where the cost multiplies.
    Additionally, two functions need to be called globally for the runtime to be instantiated and torn down, \inl{wasm\_runtime\_init} and \inl{wasm\_runtime\_destroy}, respectively.
    Our implementation requires each runtime state to implement \inl{Clone}. A common pattern in Rust (also used by \inl{wasmtime} and \inl{wasmer}) is to implement the clone operation to produce a shallow copy. Internally, these objects typically use Rust's atomically reference-counted smart pointer (\inl{Arc}) to store a pointer to the actual object, in order to safely share this object across threads. Then only a cheap copy of the \inl{Arc} is necessary to share the wrapper object across different threads, while having access to the same underlying data. On each request, a clone of the runtime is created and dropped -- its memory freed -- at the end. Because of that, we cannot call \inl{wasm\_runtime\_destroy} in the wrapper's \inl{Drop} implementation, since that would destroy the runtime too early. Instead, we use another internal \inl{Arc} in the wrapper, which is instantiated with an object that calls the initialization function when it is created, and destroys the context when it is being dropped. Because the \inl{Arc} will live as long as the runtime and only calls the contained object's \inl{drop} function when all other references to the runtime object itself have gone away, this implements the desired behavior.
    This exemplifies that \inl{wamr}'s API is harder to use safely and correctly than the idiomatic Rust APIs of the other runtimes, where memory- and thread-safey properties are encoded in the type system and checked by the compiler.

\end{description}

Both \inl{wasmtime} and \inl{wasmer} underline the importance of the validity of the bytes given to the respective \inl{deserialize} methods. Unlike a Wasm module, the compilation artifacts can not be validated in the same way since they already represent a compiled module. So they have to be trusted even when the corresponding Wasm module would not have to be trusted. In our prototype, the users actually supply the compilation artifacts themselves, for development convenience reasons. For an actual serverless framework making use of Wasm, it would have to take ownership of the Wasm module and do the precompilation itself, in order to trust the code.

\subsubsection{Execution}

All used runtimes support the WebAssembly System Interface. Recall that WASI follows a capability-based security model, which allows for fine-grained control over what the module has access to. Hence, building a so-called WASI context -- named differently in each runtime -- means setting up those capabilities. That might include writing parameters to \inl{stdin}, receiving logs from \inl{stdout} or setting environment variables and \inl{argv} arguments. By default, the module also has access to WASI APIs like \inl{random\_get} for high-quality randomness or \inl{clock\_time\_get} to attain values from various clocks. WASI also controls -- on a per-file basis -- what directories the module has access to. Specifically, the module needs a preopened file descriptor in order to access files. Our wrappers therefore have to open the files and pass them to the module. However, what files a module should have access to is not definable in the OpenWhisk protocol, since it does not make use of Docker's host-to-guest mappings, so we need a way for users to specify these capabilities. OpenWhisk actions can have optional annotations attached to them, which are simple key-value pairs. We can use those to let users specify capabilities. For instance, using the \inl{wsk} cli:

\begin{minted}{sh}
wsk action create cache cache.zip --annotation dir "/tmp/cache"
\end{minted}

Here a new action named \inl{cache} is created from a file called \inl{cache.zip}. A key-value pair \inl{dir = /tmp/cache} is attached, letting the executor know that the module should have access to \inl{/tmp/cache}. With some minimal code changes to OpenWhisk's container abstraction, these annotations are then passed to the executor during \inl{initialize}. On the executor side, it is passed to each runtime wrapper which can then act on that data. It would be easily possible for a service provider to implement policies on top of it. Access could only be granted to the \inl{/tmp} directory in general; or a default cache directory \inl{/tmp/<container\_id>} could be created for each container, where information can be cached and shared among instances of the container, but only \emph{that} container has access to it. As WASI is currently under heavy development and does not, for example, specify any networking interfaces, the amount of capabilities we can grant is limited. Our prototype implements directory access and a function that simulates an HTTP GET request. The latter will be relevant for testing I/O-bound performance in Chapter \ref{chapter:evaluation}.


\begin{listing}[ht]
\begin{minted}{rust}
ow_wasm_action::pass_json!(handler);

pub fn handler(json: serde_json::Value)
       -> Result<serde_json::Value, anyhow::Error> {
    let param1 = json
        .get("param1")
        .ok_or_else(|| anyhow::anyhow!("Expected param1 to be present"))?
        .as_i64()
        .ok_or_else(|| anyhow::anyhow!("Expected param1 to be an i64"))?;

    let param2 = json
        .get("param2")
        .ok_or_else(|| anyhow::anyhow!("Expected param2 to be present"))?
        .as_i64()
        .ok_or_else(|| anyhow::anyhow!("Expected param2 to be an i64"))?;

    Ok(serde_json::json!({ "result": param1 + param2 }))
}
\end{minted}
    \caption{A simple add action in Rust that uses a macro from \inl{ow-wasm-action} to pass the received json to the \inl{handler} and return the \inl{Result} back to the runtime.}
    \label{listing:add-action-example}
\end{listing}

The \inl{run} method executes the module associated with the given container id and with the given parameter as input. As required by OpenWhisk, the parameter is an object in JavaScript Object Notation (JSON). The WebAssembly minimum-viable product (MVP), only supports integer data types natively. Higher-level data types, like strings, often differ in their implementation between languages, so a universal target format like Wasm cannot support a specific language's implementation, without shutting certain languages out or making it harder for them to be used. Making Wasm modules interoperable -- even those compiled from different source languages -- is an ongoing process with WebAssembly interface types\footnote{\url{https://hacks.mozilla.org/2019/08/webassembly-interface-types/}}. The \inl{wasm-bindgen} Rust library already makes this process easy for JavaScript-WebAssembly interoperability. Unfortunately, it does not work with any of the runtimes we use. Even if both our executor and the module are written in Rust originally, passing a raw pointer to the JSON object to the module is not possible, because WebAssembly is sandboxed and could not access the pointee. To work around this, we require that the actions written for our executor use our library \inl{ow-wasm-action}. Internally, the library allocates a global buffer of bytes. It exports three functions to work with this buffer, most notably one that returns a pointer to it. When the module is run, the runtime wrapper serializes the JSON object as bytes and -- via one of the exported functions -- allocates enough space in the buffer to store them. It then acquires the pointer to the global buffer and writes the JSON bytes into it. So the wrapper writes into the memory region that the Wasm module can read from; its linear memory. Since the module is unaware of the number of bytes that have been written, the runtime passes that as its \inl{argv} argument.
Once the write process is done, the instance's \inl{main} function -- implemented in our library -- is called. It reads the  bytes from the buffer and deserializes them into a JSON object. Then it calls the user-provided handler function with that object. The return value from the handler is serialized back into the buffer, from where the wrapper can read it, using a similar process.
Conveniently, a developer only has to implement a function that takes a JSON object and returns one in the success case; an error otherwise. Thus, this crude method of passing parameters can be completely hidden from action developers and only requires a single line of code to be included. A simple action can be seen in Listing \ref{listing:add-action-example}. The action development process is visualized in Figure \ref{fig:action-creation-flow}, which makes use of the \inl{binaryen}\footnote{\url{https://github.com/WebAssembly/binaryen}} toolchain to optimize the Rust-produced WebAssembly.
The mentioned \inl{ow-wasmtime-precompiler} is a simple wrapper around \inl{wasmtime}'s API, that creates a module from Wasm bytes, in turn kicking off the internal compilation process. Once the module is created, it is ready for execution. At this point, we serialize it to a file and zip it. This process precompiles the module for the current host architecture. \inl{wamrc} and \inl{wasmer compile} can also take a target argument to precompile for a different host, e.g. a Raspberry Pi with the \inl{aarch64} ISA. However, as of version \inl{0.26}, the \inl{wasmtime} command line interface (cli) has also gained similar capabilities.
% One point of interest is the memory allcator. Because Wasm's original use case is in the web browser, where the binary is downloaded just-in-time, the binary size matters. Thus, a smaller memory allocator was written for Rust, named \inl{wee_alloc}\footnote{\url{https://github.com/rustwasm/wee_alloc}}. It effectively trades performance for a smaller size. Since we precompile our WebAssembly the size difference is negligible. If we were not precompiling, this might be a more interesting feature to activate.

\begin{figure}
    \centering
    \includegraphics{figures/ActionCreationFlow.pdf}
    \caption{There are two phases in the action development. Our \inl{ow-wasm-action} library provides the abstraction to read the parameters the runtime has passed and to write the return value. To that end, it uses \inl{serde\_json}, a (de)serialization library to read and write the JSON objects. Afterwards, \inl{wasm-opt} runs an optimization pass on the produced WebAssembly, since not all runtimes run an extensive optimization pass.
    Depending on what runtime is compiled into the executor, the corresponding precompiler needs to be used. For \inl{wasmtime}, we have written the small \inl{ow-wasmtime-precompiler} program that precompiles the Wasm code into a format ready for execution, while \inl{wamrc} and \inl{wasmer compile} can be used for the others. Finally, the resulting precompiled binary is zipped which reduces the transmission size and instructs OpenWhisk cli to treat it as binary. The zip file is then ready to be uploaded to OpenWhisk.}
    \label{fig:action-creation-flow}
\end{figure}

The creation of the WASI context as well as the actual execution is slightly different for each runtime.

% The \inl{Store} is only needed to instantiate an \inl{Instance}, and should not be long-lived, according to the documentation. Thus, these two data structures are instantiated per-request, in each \inl{/run} invocation, and discarded afterwards. Evidently, this runtime's API is a very good fit for our use case.

\begin{description}[style=multiline, leftmargin=2.5cm, font=\bfseries]

    \item[Wasmtime] The \inl{wasmtime} runtime, as of version \inl{0.23}, uses \inl{cap-std}, a capability based implementation with a similar API as the Rust standard libary. We can construct its \inl{Dir} type and use it while building a \inl{WasiCtxBuilder}, ultimately yielding a \inl{WasiCtx}. The context is \emph{linked} with the module using the runtime's \inl{Linker}, which is responsible for resolving the imports the module needs, WASI or otherwise. Hence, we can also use it to provide functions as imports to the modules.
    To execute, we need to create a \inl{Store}, which requires a reference to an \inl{Engine}. Since one was instantiated globally, we can simply refer to it. The \inl{Store} in turn is needed to instantiate an \inl{Instance} from the \inl{Module} we deserialized during initialization. The \inl{Store} is neither thread-safe nor meant to be long-lived, according to the documentation. Thus, the \inl{Store} and the \inl{Instance} are instantiated per-request, in each \inl{/run} invocation, and discarded afterwards. It ensures that the memory allocated in the \inl{Store} does not stick around for the entire lifecycle of the \inl{Module}. Evidently, this runtime's API is a very good fit for our use case.

    \item[Wasmer] In \inl{wasmer}, we build up a \inl{WasiEnv} with a similar API as \inl{wasmtime}'s, including directory access. To provide functions to the module, the resulting \inl{ImportObject} can be combined with another one, by implementing a custom \inl{Resolver}. It simply maps the name of an import to a function, in our case. The module combined with the \inl{Resolver} then produces an \inl{Instance}. In contrast to \inl{wasmtime}, the necessary \inl{Store} was already created in the \inl{initialize} method, so the memory allocated in it does stick around until the entire container is removed.

    \item[Wamr] \inl{wamr}'s API is lower-level and less ergonomic compared to the other runtimes, due to it being a C rather than an idiomatic Rust API. On every \inl{run} invocation, we need to load the runtime, instantiate the module from the precompiled bytes and create an instance from it as well as create an execution environment. Only then can we start execution. Due to API limitations, we cannot frontload meaningful work to the \inl{init} method, so all of it needs to happen for every invocation.
    While this costs only performance, but otherwise implements what we need, other parts of the API are real restrictions. One is that function imports for WebAssembly modules need to be specified on -- what \inl{wamr} calls -- the runtime, which is the equivalent of the \inl{Engine} type in the other Wasm runtimes. Since it is global and instantiated only once, we can not define different imports for different modules, only one set of imports for all modules. While we do not need this flexibility for our examples, it may be desireable to have in a full-featured Wasm executor, e.g. only modules where the developer has set the permissions accordingly can work with networking functions. The other Wasm runtimes allow us to specify imports per instance. The API of \inl{wamr} is the least appropriate for our use case.

\end{description}

In summary, \inl{run} looks up the previously stored module or its bytes by the container id, creates a WASI context with the associated capabilities and potential imports, passes the parameters and calls the module.
% If an error occurs during the Wasm runtime setup, the outer \inl{Result} contains an \inl{anyhow::Error}, a generic, application-level error type. Since the Wasm module itself can also either succeed or fail, another \inl{Result} is nested inside, where both success and failure are represented by JSON objects.

% In what task or thread the module is executed, will be discussed shortly.

\subsubsection{Cleanup}
Finally, the \inl{destroy} function removes the container, which simply means freeing the memory taken up by the module. This is fast and thread-safe, since each container manages its own Wasm module. Once OpenWhisk has decided to destroy the container, it will synchronize internally to not call \inl{run} on it again.

An overview of the entire executor workflow can be seen in Figure \ref{fig:executor-overview}.

\begin{figure}
    \centering
    \includegraphics{figures/ExecutorOverview.pdf}
    \caption{Overview of the executor architecture. \inl{init} decodes the received bytes using \inl{base64} and unzips the result. From there, they are deserialized back into a \inl{Module} instance, in the case of \inl{wasmtime} and \inl{wasmer}. The module and capabilities are then stored in the HashMap. When a \inl{run} request comes in, the module and capabilities are looked up. The capabilities are transformed to a WASI context, which is used to instantiate an instance, together with the module. The parameters for this \inl{run} request are serialized to JSON and written into the instance's memory. Calling the \inl{main} function in turn invokes the actual developer-provided handler function. The read result is returned to OpenWhisk. A \inl{destroy} request simply translates to a remove operation on the HashMap. Note that this figure is accurate for \inl{wasmtime} and \inl{wasmer}, but not entirely for \inl{wamr}. The latter runtime's module creation is slightly different, as explained above.}
    \label{fig:executor-overview}
\end{figure}



\subsection{Enabling Concurrency}
\label{section:enabling-concurrency}
% For former type, \inl{async} Rust is an excellent choice, while it may not be ideal for the latter. There is a fundamental trade-off here. If the executor only worked with OS threads, it could support a much smaller number of blocking requests.

% Whether serverless workloads are primarily one or the other is not well researched. However, we can make an educated guess. As we have seen in chapter \ref{chapter:background}, the majority of serverless functions are written in JavaScript in the \inl{node.js} engine or in Python... 

% The executor consists of a web server that can take requests from the invoker asynchronously and concurrently.
%Because OpenWhisk doesn't communicate with the Wasm modules directly, but through the executor, the \inl{/start} endpoint simply returns the executors own network address.
% This endpoint also returns an identifier for the container, so that OpenWhisk can handle its life cycle, i.e. the above-mentioned mechanism of keeping it warm for some time, for a faster subsequent invocation. There is a chance that this mechanism may not be necessary for Wasm modules, as they are much more lightweight in their instantiation. We will come back to this question later in our optimization discussion.

In order for our executor to handle requests to these endpoints concurrently, we use Rust's \inl{async} features. It is a programming model that »lets you run a large number of concurrent tasks on a small number of OS threads« \cite{AsyncBook2021}. With this model, a large number of I/O-bound tasks can be handled with less resources than a thread-based model could.
Rust's async model works with \inl{Future}s -- similar to \inl{Promise}s in JavaScript -- which represent values that have not yet been computed. Contrary to \inl{Promise}s however, Rust's \inl{Future}s are lazy: They do nothing unless actively polled. This task falls to \inl{async} runtimes, of which \inl{async-std} provides one. In \inl{async-std}, the unit of execution is a \inl{Task}, which is similar to a \inl{Thread}, except it is driven to completion by the user space runtime instead of the OS. Many of those tasks are executed on the same thread, and by default, a \inl{Thread} per logical CPU core is spawned. Thus we have $M \cdot N$ tasks in total, where $M$ is the number of tasks per thread and $N$ the number of logical CPU cores. Because multiple \inl{Task}s are multiplexed onto one thread, it should not do a blocking or long-running operation, like reading from a file via the standard library's synchronous I/O functions. Instead, it should use the \inl{async} version of that method, from \inl{async-std}, which does not block the thread. Otherwise, all $M - 1$ tasks, which are executed on the same thread will be blocked, even though they may be able to make progress. Note that we could still process $N$ requests concurrently, even if tasks block, but that does not scale as well for I/O-bound tasks.
% This is why these kinds of async functions are also called cooperative routines (coroutines). When they need to wait for the OS to finish a task, they \emph{yield} to another routine, which can then continue actual work.

The question then becomes, what the execution of WebAssembly is bound by. That mostly depends on the WebAssembly module. If the module calculates a large prime number, executing it is CPU-bound; but if it makes an I/O-bound HTTP request, then so is the task running the module. We have explored the typical serverless workload in detail in Chapter \ref{chapter:background}. Most definitely, I/O-bound functions are a primary type of workload for serverless due to their frequent usage of external services. CPU-intensive functions on the other hand, are less common, but still need to be accommodated, if only to expand the range of applications that can leverage serverless.

Using this programming model enables us to potentially handle many concurent requests from OpenWhisk. However, since we embed Wasm runtimes and -- at the time of implementation -- calling WebAssembly functions is not \inl{async}, they block the thread they are running on, until the module has finished execution.
If the module's execution is CPU-bound, it essentially blocks the other \inl{Task}s on the thread from making progress until it is finished. This is less of an issue if we only have CPU-bound modules, given that no more work can be done concurrently than the CPU has logical cores. In a more realistic, mixed workload scenario, however, modules hogging the CPU could prevent I/O-bound tasks from concurrently making progress, even though they hardly need CPU time.
% Although, if the module uses synchronous I/O, it blocks the thread.
In both cases, only $N$ Wasm modules can be executed concurrently, directly corresponding to the number of available threads.
That means, we cannot execute modules on the same thread concurrently and that, essentially, both workload types are not well suited for this execution model.
Since one of the appealing offerings of serverless frameworks is their scalability, we clearly need a way to handle many more requests, even if they block.

To let other \inl{async} functions on the same thread make progress, while one executes the module, we can push the entire execution onto a thread pool. The function then has the simple job to asynchronously wait for that execution to finish and collect the result. That allows the other functions to process more requests in the meantime. Unfortunately, this effectively corresponds to a thread-based model where fewer I/O-bound tasks can be handled than possible with an asynchronous task model.

Recall that WebAssembly itself does not interact with the system, so it could not do blocking I/O without its (WASI) imports. If we could provide these imports as asynchronous functions, the async runtime that drives our executor could also drive them to completion. In fact, the Rust library implementing WASI capabilities \inl{wasi-cap-std-sync}\footnote{\url{https://docs.rs/wasi-cap-std-sync}} notes that an \inl{async} version is being worked on, as of April 2021. But until these APIs become available, we have to work around that with the thread pool.

To test the whether the thread pool model is able to handle more requests than blocking in the threads directly, we run two short experiments.

\begin{enumerate}
    \item CPU-bound: Calculating a large prime number.
    \item Network I/O-bound: Making a long-running (1 second) HTTP request.
\end{enumerate}

\begin{figure}
    \begin{center}
        \input{figures/async_std_default_thread_vs_thread_pool.pgf}
    \end{center}
    \caption{Executing blocking Wasm workloads under the \inl{async-std} default model with 8 threads, compared with running each in a thread from a pool.}
    \label{fig:default_vs_thread_pool}
\end{figure}

\begin{figure}
    \begin{center}
        \input{figures/async_std_default_thread_vs_thread_pool_mixed.pgf}
    \end{center}
    \caption{The average request duration and the average amount of requests finished per second under the default and thread pool execution model, sampled over 50 runs.}
    \label{fig:default_vs_thread_pool_mixed}
\end{figure}

Our test machine has 8 logical CPU cores, resulting in 8 threads being started for the async-std runtime. We thus send 16 concurrent requests to be able to see the potential blocking in the result. The first experiment showed no difference in the total execution time between the default and thread pool model. As we would expect, in the default model the CPU-bound task finishes the first 8 requests, before starting the second set. The execution in a thread pool executes all 16 requests concurrently, but each took roughly twice as long, resulting in no significant difference between the total execution time. The default model operates more along the first-come, first-serve principle, where an early request will also get a response more quickly. In this simplified example, where all workloads are CPU-bound, this is preferable, since the first 8 requests get their responses sooner.

The result of the second experiment is shown in Figure \ref{fig:default_vs_thread_pool}. In the default model, the first 8 requests block the other 8, while they wait for the HTTP request to finish, resulting in roughly 2 seconds of total execution time. Unsurprisingly, the thread pool model finishes in half the total execution time, since it can handle all 16 requests simultaneously, due to using a new thread from the pool for each blocking operation.

Both models seem to have a workload better suited to them. However, in a real-world, mixed-workload scenario, a long-running, CPU-bound function may prevent another I/O-bound one from initializing its computationally cheap request. A mixed scenario may therefore also be better served by the thread pool model.

In Figure \ref{fig:default_vs_thread_pool_mixed}, a mixed scenario is run, where 16 CPU-bound and 16 I/O-bound requests are sent concurrently. In the default model, the CPU-bound requests finish in about one third of the time they take under the thread pool model. This is again due to more requests being processed concurrently in the latter model. This is also why the standard deviation is slightly higher in that model. For I/O-bound workloads, there is a slight advantage for the default model. However, in the context of the plot on the right side, the thread pool model is able to handle more requests per second in general, but specifically excels at the I/O-bound ones. There is an inherent trade-off between serving a request as soon as possible, and the time it takes to finish serving it. It depends on the expected workload, which model should be used. Based on our research in Chapter \ref{chapter:background}, we would expect I/O-bound modules to be prevalent, so we use the thread pool model going forward.

% https://rust-lang.github.io/async-book/01_getting_started/02_why_async.html
% Thread pools can mitigate some of these costs, but not enough to support massive IO-bound workloads.
% We're basically back to thread pools with the spawn_blocking. I.e. async Wasm execution will be necessary to facilitate more blocking tasks on the same host than threads would be able to handle. Wasmtime has some async support, need to look at that more closely. 

\subsection{Implementation Alternatives}

Instead of caching modules, we may wish to cache instances. The case for reusing instances is to avoid the costly setup in the action itself, like connection pools to external resources or to avoid rebuilding a cache on every request.
We need to manage instances in that case, i.e. we need a pool of instances and a management system around it. When a new request comes in, we would ideally like to reuse an existing instance. That requires synchronizing with other threads to decide which thread gets to reuse an instance and which needs to create a new one. We would need per-instance synchronization to check whether an instance is currently in use or usable. It is questionable whether the synchronization overhead would be less costly than always creating a new instance, in particular because creating instances is a relatively cheap operation. 
Furthermore, with a new instance per request, we have per request isolation and fully stateless execution -- something that the action developer can rely upon. For these reasons we decided not to implement it.

As seen in Figure \ref{fig:executor-overview}, a WASI context is created per \inl{run} invocation. For example, if a module needs access to a file, creating the context requires opening file descriptors. Similarly, with a future networking API in WASI, this might require invoking more system calls. Frontloading this resource acquisition as much as possible can have net performance benefits. In particular, we should pre-create the WASI context, if possible. In \inl{wasmtime} this is not possible, because the \inl{WasiCtx} is not thread-safe. In \inl{wasmer} on other hand, the \inl{WasiEnv} is thread-safe and can be cheaply cloned to send across threads. Our current implementation needs one context per instance. That is due to the way we pass parameters to instances. This is not a fundamental limitation and could be done differently, which in turn would allow the \inl{wasmer} wrapper to, for example, lazily create one \inl{WasiEnv} per action the first time it is needed, and then reuse it for all future invocations of that action.

One potential inefficiency of our design is that our executor stores every container's module once. If OpenWhisk decides to create two containers \inl{A} and \inl{B}, both containing the same module \inl{M}, then \inl{M} will take up memory twice. One reason for that design is that in OpenWhisk vanilla, containers are completely separated from each other and no such optimization would be possible. Our executor is a more centralized solution and would allow for this optimization in the first place. Currently, we trade memory for speed and store the module per container. Thus, adding or removing a container is a fast insert or remove operation on the HashMap.
% Reasons against this optimization are that it increases complexity, is partially unnecessary and may impact performance.
% One reason against this design, however, is the initialization and eviction process.
Internally, the executor uses a thread-safe HashMap to store the modules and let many tasks or threads work with it simultaneously. A store-modules-once approach would require a global lock on the HashMap. During initialization, the executor needs to check whether the module already exists in the HashMap and if it does, increase its atomic reference count. This is already a critical section, since without synchronization, a concurrent destroy operation may have removed the module between the existence check and the increment. Consequently, this requires a global lock on the HashMap to avoid race conditions. This is probably not a performance issue, since lock contention is unlikely given that the average time between container starts or removals is much longer than the amount of time the lock needs to be held.

\begin{figure}
    \centering
    \includegraphics{figures/OpenWhiskConcurrencyLimitGreaterOne.pdf}
    \caption{An example of a container's lifecycle when OpenWhisk's concurrency limit is set to a value greater one for that action. Note that a container is only represented in-memory by its module and instances, which is why there is no explicit »container« label in the figure. The container is initialized with a module by OpenWhisk's call to \inl{/init}. Every \inl{/run} request to the same container id spawns a new instance of a WebAssembly module. In particular, two concurrent requests to \inl{/run} can be executed concurrently by the same container. In that case, the instances are instantiated from the same module. The instances are the actual »containers« here, since they are the ones who are isolated from each other and the host. They are short-lived and never reused to guarantee that isolation, in other words, there is a one-to-one relationship between \inl{/run} requests and instances. The Wasm module will stay in memory until OpenWhisk's explicit call to \inl{/destroy}, which occurs after a container has been idle for some threshold time.}
    \label{fig:openwhisk-concurrency-limit-greater-one}
\end{figure}

% We instead trade memory for speed and store the module per container. Thus, adding and removing a container is a trivial and fast insert or remove operation on the HashMap.
% Outside of such workloads, multiple containers with the same module are less likely to be started.

The question then is, whether it would help alleviate resource waste, that is, how often are multiple containers for the same action started?
This primarily happens under bursty or batch workloads. Outside of such workloads, multiple containers with the same module are less likely to be started. But as we have seen in Chapter \ref{chapter:background}, an analysis of open-source serverless applications showed 84\% of them having bursty workloads \cite{Eismann2021}. The likelihood of receiving concurrent requests for the same action and in turn having to create multiple containers is therefore quite high. Hence, it seems implementing this optimization makes sense.
However, OpenWhisk itself already has the potential to eliminate the waste with its per-action concurrency limit. This limit states how many concurrent invocations a container can handle at most. If we increase this value beyond its default of one, then OpenWhisk will send invocation requests for the same action to the same container, up to that limit, and without having to start a new container thereby duplicating the module. In particular, Figure \ref{fig:openwhisk-concurrency-limit-greater-one} shows how a module can be reused in the same container -- skipping the creation of a separate container entirely. Making use of this limit can thus be very important for performance and to avoid resource waste. With a concurrency limit of $c$, up to $c$ requests could be handled by a single container. If set appropriately for the underlying hardware and workload, no two containers need to be started on the same host and thus no resource waste would be necessary. Now, whether developers can be expected to set this limit properly is debatable. After all, their lack of knowledge about the hardware is rather the point, so this limit would perhaps better be set by the serverless operator.
Overall, because of these considerations we decided adding this executor-internal optimization was not necessary.
