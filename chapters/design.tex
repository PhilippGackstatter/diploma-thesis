\chapter{Design}
\label{chapter:design}

In this chapter, we lay out the requirements that an implementation of Wasm containers should satisfy in general. We introduce several WebAssembly runtimes and pick three of them for our implementation. We then expand on the requirements by looking at OpenWhisk specifically and how we need to modify it to enable our new runtime, but also detail other approaches that could be used to run WebAssembly workloads. Finally, we describe and discuss our concrete implementation in Apache OpenWhisk, how to achieve fast cold starts and high performance, while also examining the trade-offs involved.

\section{Core approach}

As we've seen in the previous chapter, there are a number of issues with serverless in general, but also serverless on the edge. And they are quite similar. In this section we will describe our idea for rectifying these issues to make serverless (edge) computing more viable.

A substantial cold start latency has emerged as the paramount issue of the serverless platform. It prevents serverless platforms from implementing true scale-to-zero, since functions are kept warm after invocations, wasting energy and resources. While we may not be able to get rid of the keep-alive completely, a significant reduction in cold start latency would allow a similarly large reduction in keep-alive time. Since energy and resources are even more precious on typical edge devices, this is also very important for their quality of service.
Serverless is employed by developers for bursty, unpredictable workloads, a pattern that is also common at the edge. In this scenario we see concurrent requests, which translate to concurrent cold starts. The more concurrent cold starts, the longer each one takes -- they slow each other down \cite{Cui2018}.
Operators are forced to choose between saving cost and resources or enabling a high quality of service, in particular shorter response times. While this trade-off is inevitable to some degree, the cold start exacerbates it. If we can alleviate that latency, we have a win-win for the serverless user and the operator.

Serverless enables the execution of \quot{polyglot tenant-provided code} \cite{Nastic2018}. A new runtime for serverless must keep up these principles of being programming language agnostic as well as being securely sandboxed, to allow for multi-tenancy. As described in the previous chapter, WebAssembly provides sandboxing and support for many languages, with support for more, likely to come. At the same time, it is a lighter-weight solution than virtualization with Docker containers. Thus, it seems like a good candidate to replace Docker containers in serverless platforms while alleviating the cold start latency. This is also why WebAssembly may fare better on low-power edge devices.

\citeauthor{Mendki2020} compared WebAssembly and Docker container startup times, and found WebAssembly being 91\% faster than containers and used 75\% less memory \cite{Mendki2020}. \citeauthor{Hall2019} find their WebAssembly-based serverless platform fared 60\% better on average, than the Docker container solution, for their concurrent \quot{Multiple Client, Multiple Access} workload \cite{Hall2019}. These promising results are the basis for our work and validate the core idea of our approach.

\section{Requirements}

The executor will be built with these concerns and requirements in mind:

\begin{enumerate}
    \item Be easily integratable with an existing serverless framework.
    \item Provide a cheap sandboxing mechanism to ensure multi-tenant capability and no adverse effects on the cold start latency.
    \item Be as close to native speed as possible to be a viable alternative to existing container runtimes.
    \item Run on devices with potentially different instruction set architectures (at least \inl{x86\_64} and \inl{ARM}).
    \item Be able to efficiently handle a large amount of I/O concurrently, due to all requests being proxied through.
    \item Be thread-safe.
\end{enumerate}

The requirements are discussed in more detail in the following sections. Finding a suitable serverless framework that we can modify and integrate with, will be the topic of the next section. In section \ref{chapter:background} we have seen how WebAssembly provides some of these requirements, such as sandboxing and relatively good execution speed. However, for our implementation we need concrete WebAssembly runtimes that provide these properties, so we will explore the options for WebAssembly runtimes and work out the differences. The choice for which language and libraries to implement our executor with, will be driven by how the Wasm runtime we choose can be integrated as well as having access to highly performant, concurrent I/O and good support for program correctness, including thread-safety.

\section{Serverless frameworks}

\section{WebAssembly runtimes}
\label{section:wasm-runtimes}

WebAssembly is a specification \footnote{\url{https://webassembly.org/specs/}}. Implementations adhering to this spec are WebAssembly runtimes. Since Wasm was originally designed for the web, the virtual machines of browsers, such as Google's V8 or Mozilla's SpiderMonkey, can also execute WebAssembly in addition to JavaScript. With the rise of Wasm outside the Web, a large number of standalone runtimes have emerged and many are actively being developed. This section will introduce a number of runtimes with different execution models, and justify our choice of runtimes to implement in our executor.


\subsection{Wasmtime}

Wasmtime \footnote{\url{https://github.com/bytecodealliance/wasmtime}} is a runtime that has support for Just-in-Time compilation, available for the x86\_64 and aarch64 instruction set architectures (ISA). It supports the latest WASI standards and is written in Rust and thus easily embeddable from there.
Wasmtime is closely related to the \inl{cranelift} project, which it uses for code generation. Thus, we expect wasmtime to be a high-quality JIT implementation.

\subsection{Lucet}

Lucet \footnote{\url{https://github.com/bytecodealliance/lucet}} is the compiler and runtime used for cloud provider fastly's Terrarium, a platform for running WebAssembly on edge devices \cite{fastly2019}. It consists of a compiler and a runtime to execute WebAssembly. The compiler, \inl{lucetc}, compiles Wasm modules to native object or shared object files. The complementary \inl{lucet-runtime} can then load the native code and call the contained Wasm functions. Because of this compilation model, execution can reach near-native speeds. However, the execution is not sandboxed by default, requiring mechanisms like \inl{seccomp-bpf} \footnote{\url{https://wiki.mozilla.org/Security/Sandbox/Seccomp}}, and the security model is actively being developed. It is currently only available on the x86\_64 ISA. Since we require support for edge devices, typically built on the arm ISA, we will not be using \inl{lucet}. Furthermore, the lack of full sandboxing makes other runtimes a better choice.

\subsection{Wasmer}

Wasmer \footnote{\url{https://github.com/wasmerio/wasmer}} is a WebAssembly runtime with support for the three major platforms and x86\_64 and aarch64 ISAs. Notably, wasmer has already reached version \inl{1.0}, indicating a certain level of maturity. The runtime offers three different compilers, \inl{singlepass}, \inl{cranelift} and \inl{LLVM}, whose compilation times get slower and execution times get faster, in that order, respectively. The runtime then has a JIT and a Native engine, which differ in the way the module is loaded. In particular, the Native engine produces a shared object, which can be loaded with a fast \inl{dlopen} call. The support for high-quality AoT compilation with \inl{LLVM} and fast startup times makes wasmer a good choice for our purpose.

\subsection{WASM3}

Due to Wasm's simplicity in the binary format, there is wide-spread interest for running it on embedded devices. An example of that is \inl{wasm3} \footnote{\url{https://github.com/wasm3/wasm3}}, a Wasm interpreter that claims to be the fastest. They argue in favor of interpretation over JIT or AOT compilation, since startup latency is more important in many situations. Furthermore, portability and security are easier to achieve with this approach and development with the virtual machine is easier. Still, a performance penalty of 4 to 5x compared to JIT or 12x compared to native execution, leads us to believe that no amount of startup latency gain can make up for the performance loss. Even though our primary goal is reducing the cold start latency, the execution that follows cannot lack behind Docker containers massively, if adoption is to succeed. The relation between startup and execution performance is too far out of balance in this case.

\newcommand{\linux}{\input{icons/linux.pdf_tex}}
\newcommand{\macos}{\input{icons/macos.pdf_tex}}
\newcommand{\windows}{\input{icons/windows.pdf_tex}}
\newcommand{\freebsd}{\input{icons/freebsd.pdf_tex}}
\newcommand{\android}{\input{icons/android.pdf_tex}}
\newcommand{\ios}{\input{icons/ios.pdf_tex}}

\begin{table}[h!]
    \centering
    \begin{tabular}{c | c | c | c}
        Runtime        & ISA support & Execution modes  & Platform support\\
        \hline
        \inl{wasmtime} & \inl{x86\_64}, \inl{aarch64} & AoT, JIT & \linux, \macos, \windows\\
        \inl{lucet}    & \inl{x86\_64} & AoT & \linux, \macos\\
        \inl{wasmer}   & \inl{x86}, \inl{x86\_64}, \inl{aarch64} & AoT, JIT & \linux, \macos, \windows\\
        \inl{wasm3}    & \inl{x86}, \inl{x86\_64}, \inl{arm}, \inl{RISC-V}, ...  & Interpreted      & \linux, \macos, \windows, \freebsd, \android, \ios\\
        \inl{wamr}     & \inl{x86}, \inl{x86\_64}, \inl{arm}, \inl{aarch64}, ... & AoT, Interpreted & \linux, \macos, \windows, \android, ... \\
    \end{tabular}
    \caption{WebAssembly runtime feature overview.}
    \label{table:wasm-runtime-overview}
\end{table}

\subsection{WebAssembly Micro Runtime}

However, there is still value in runtimes written specifically for embedded devices. The WebAssembly Micro Runtime (\inl{wamr}) \footnote{\url{https://github.com/bytecodealliance/wasm-micro-runtime/}} has support for all three mentioned execution modes. It has low memory usage and binary size. For example, the claimed binary size of the AoT runtime is just 50 kilo bytes. Further, the AoT runtime claims near-native speed. For execution on edge devices this may allow more modules to be executed compared to heaver-weight runtimes.

A summary of the runtimes we looked at can be found in table \ref{table:wasm-runtime-overview}.

\section{WebAssembly in OpenWhisk}

In this section we examine the possibilities for bringing WebAssembly support to OpenWhisk.
In the rest of this section we will explore the available options and discuss them in the context of these requirements.

\subsection{OpenWhisk's anatomy}

In order to understand the requirements for our Wasm-flavored OpenWhisk, we need to understand the design of OpenWhisk itself first.

\begin{figure}
    \includegraphics{figures/OpenWhiskActionInvocationFlow.pdf}
    \caption{Action invocation flow in Apache OpenWhisk based on the source code and documentation \cite{OpenWhiskSystemDesign}. Potentially many Invokers can exist in this setup.}
    \label{fig:openwhisk-action-invocation-flow}
\end{figure}

Figure \ref{fig:openwhisk-action-invocation-flow} sketches the design of OpenWhisk. Users interact with it through its API Gateway, which consists of \inl{nginx}, mostly for SSL termination, and the \inl{Controller}, which implements the main logic. It handles the creation of actions - OpenWhisk's name for serverless functions - authentication and authorization as well as invoking the action.
In the latter case, the controller uses its load balancer to determine which Invoker should handle the request, and publishes the request to an Apache Kafka queue, which the invoker is subscribed to. The controller and its load balancing take a central role in OpenWhisk, but in our figure it is represented by the API Gateway and the Load Balancer only, simply because we will not be modifying this part.
Our focus is on the Invoker. It is the part of OpenWhisk responsible for \emph{invoking} the action. A number of steps are involved in that process \cite{OpenWhiskSystemDesign}.

\begin{enumerate}
    \item It retrieves the action's code and execution permissions from an Apache CouchDB database, where the Controller stored it during action creation.
    \item It instructs the local Docker daemon to start a new container, based on the runtime that the action needs to execute. This would be a \inl{openwhisk/action-nodejs-v10} image for JavaScript actions but could also be a generic black-box image (\inl{openwhisk/dockerskeleton}), that can execute any binary, such as one written in Rust or C.
    \item Finally, it injects the action's code into the container, invokes it with the given parameters and returns the result \cite{OpenWhiskSystemDesign}.
\end{enumerate}

Because OpenWhisk supports many runtimes and even the mentioned black-box image, it needs a common protocol to communicate with all of them. The just-described step 3 is the gist of that protocol. Each runtime needs to implement a number of functions. The \inl{start} function creates a new container and returns its address. The following requests to the \inl{/init} endpoint are then sent to that address, where the container receives the code and takes whatever steps necessary to make the action ready for execution. Thus, over a containers life cycle, this endpoint is called exactly once. Once initialized, the \inl{/run} endpoint can be called multiple times to execute the action \cite{OpenWhiskSystemDesign}.

That implies that the container isn't destroyed immediately after it has been invoked. Indeed, OpenWhisk uses various optimizations of the described flow, in order to improve the system's performance. Among those are pre-warming containers or keeping the container running for some time after it has been invoked, before it is removed entirely. In that case, the runtimes \inl{destroy} function is used.
We will explore those optimizations in more depth later, to compare them to the optimizations in our Wasm runtime to better see where they coincide and differ \cite{OpenWhiskSystemDesign}.

\subsection{OpenWhisk on Kubernetes}

OpenWhisk can be deployed on a Kubernetes cluster, a container orchestration tool for automating deployment and scaling of containers \cite{Kub2021}. OpenWhisk has a \inl{ContainerFactoryProvider} service provider interface (SPI), which represents the underlying container management platform. It provides two implementations of that SPI \cite{OWKub2020}.

\begin{enumerate}
    \item The \inl{DockerContainerFactory} implements the architecture shown above, except that most components of OpenWhisk run in separate Kubernetes pods. In non-Kubernetes deployments, they usually run in separate Docker containers. The crucial part of this deployment is, that the Invoker runs in a single pod (potentially on multiple worker nodes), and also spawns all of the actions in docker containers inside that pod. Thus container management latency is rather small, but does not take advantage of Kubernetes' scaling mechanisms.
    \item The \inl{KubernetesContainerFactory} interfaces with the Kubernetes API to create and schedule actions in containers -- in turn contained in pods -- to run on different Kubernetes worker nodes. This trades higher container management latency for access to Kubernetes' scaling mechanisms.
\end{enumerate}

\subsection{Executing WebAssembly with OpenWhisk}

To execute Wasm modules in OpenWhisk, we have two similar options with similar trade-offs.

\begin{enumerate}
    \item Using one of a number of WebAssembly runtimes that are currently being developed and are in various stages of maturity. We would need to embed one of these runtimes in a program of our own that provides a similar interface as Docker. This WebAssembly container runtime would then be able to replace the Docker daemon in the architecture presented above, and run Wasm modules instead of Docker images. Since this program would sit in-between OpenWhisk and the Wasm runtime, we would have precise control over the management of the modules, i.e. whether the Wasm module is just-in-time compiled, interpreted or compiled to native code ahead-of-time; the caching layer that keeps initialized modules ready for fast execution; or the configuration of the system interface with which Wasm modules can interact with the underlying operating system.
    \item The second option is to use Kubernetes as the management system for our Wasm modules. Kubernetes is generic over the underlying container runtimes, and as such it runs on \inl{containerd}, \inl{CRI-O} and Docker \cite{Kub2021}. However, it is also possible to let Kubernetes pods run on other runtimes by implementing the \inl{kubelet} API. The \inl{krustlet} project provides a \inl{kubelet} implementation that can execute Wasm workloads \cite{Krustlet2021}. With that, we can now instruct Kubernetes to run Wasm modules for us, instead of the typical Docker container.
\end{enumerate}

We will explore those two options in more detail and discuss which may be a better fit for our requirements.

\subsection{Krustlet: The WebAssembly Kubelet}

As a \inl{kubelet}, \inl{krustlet} runs on Kubernetes worker nodes and accepts workloads of the \inl{wasm32-wasi} architecture. Just like a regular kubelet, krustlet will then pull the Wasm module from an OCI registry and run it, just like a Docker image is pulled from Docker Hub. This process allows us to schedule our Wasm workloads from an Invoker, similar to how the \inl{KubernetesContainerFactory} implementation schedules actions in Docker containers. However, this execution model is also different from the OpenWhisk model in some ways.

\begin{itemize}
    \item In OpenWhisk there is a split between runtime environment, provided through the Docker image, and the code that is stored in a database, retrieved from there and injected into the container. Krustlet expects the Wasm module to be the equivalent of the container, but with the code baked in. Thus no injection is necessary. This makes it slightly harder to integrate with the current OpenWhisk model, since the Wasm module doesn't need to be stored in the database, but rather in the OCI registry. That registry would need to be part of the Kubernetes cluster for latency reasons alone, but also since the images are the Wasm modules uploaded to OpenWhisk as actions. They are specific to that OpenWhisk deployment, rather than the generic OpenWhisk runtime environments, currently stored in Docker Hub. We can assume, that the latency introduced by retrieving the image from the registry is offset by not having to retrieve the code from the database, so both solutions should perform the same in that regard.
    \item Since there is no split anymore, every Wasm module needs to implement the OpenWhisk runtime protocol itself, instead of being able to rely on the runtime implementing it once. In practice, this means implementing the \inl{/init} and \inl{/run} endpoints, where the former would no longer do anything useful, since there is no code to inject. It follows that every Wasm module needs to include an HTTP server, increasing its size above what would be needed for only its functionality. Because network support in WASI is still under active development, the alternative is to implement a \inl{wasmcloud-actor}, which essentially provides capabilities such as networking to Wasm modules \cite{WC2021}. The function can then be run by calling the \inl{/run} endpoint. Just like containers in OpenWhisk keep running for some time after having been called once, the module would also continue to listen for more requests to avoid the cold-start, for some time.
\end{itemize}

% Although, if containers are independent of the thing they were spawned from, that makes the system more resilient, thus the slightly higher cost of implementing the OW protocl + HTTP Server may be offset.

While this approach has its merits, we believe that there would be higher friction for implementing it in OpenWhisk, than writing our own layer to Wasm runtimes. It provides more control over optimizing the cold-start latency as well as the execution performance, which are part of the requirements we laid out.

\subsection{An OpenWhisk-WebAssembly Executor}

Because the first-mentioned approach allows us to integrate Wasm into OpenWhisk with fewer changes to it, we choose to follow this path. We will implement a layer between OpenWhisk and different WebAssembly runtimes, in order to enable the execution of Wasm modules in OpenWhisk. We refer to this as the Wasm executor, to distinguish it from Wasm runtimes -- those responsible for the actual execution of Wasm modules. We aim to replace the Docker daemon in the architecture shown above, leveraging the existing OpenWhisk interface for container management.

\begin{figure}
    \includegraphics{figures/WasmOpenWhiskActionInvocationFlow.pdf}
    \caption{Invocation flow of a Wasm action in our modified Apache OpenWhisk. The Invoker \emph{injects} the code into the executor which \emph{creates} a Wasm Module ready for execution. It then \emph{instructs} the executor to \emph{invoke} the module with the parameters it passes. The result is passed back to the Invoker. Again, a deployer can define whether one or more Invokers exist.}
    \label{fig:wasm-openwhisk-action-invocation-flow}
\end{figure}

In figure \ref{fig:wasm-openwhisk-action-invocation-flow} we have sketched the high-level modifications that we need to make to OpenWhisk. In this invocation flow, we need to hook into OpenWhisk's invoker to enable its communication with the Wasm executor we will implement, instead of the Docker daemon. Fortunately, the Invoker is already well-separated from the concrete containerization technology through a Service Provider Interface (SPI). OpenWhisk already supports Docker, but also Kubernetes as the container management system, such that the SPI was a logical addition. We implement a \inl{WasmContainer} that extends OpenWhisk's \inl{Container} abstraction, which already handles container communication, such as calling the \inl{/init} and \inl{/run} endpoints.

% Figure that shows a zoomed-in version of the invoker and its SPIs (as well as the Wasm executor?) - Help better understand OW's design and what we added

In OpenWhisk's Docker container implementation, OpenWhisk only uses the Docker daemon to create a container, but then communicates with the container directly. In our implementation, every request is proxied through the Wasm executor. The advantage is, that not every Wasm module needs to implement the OpenWhisk protocol, which requires an HTTP server, which would both increase the module's size and its runtime overhead. The disadvantage is the single point of failure. That is offset by the bigger picture of OpenWhisk, where in a truly resilient deployment, we already have multiple Invokers. A failing Wasm executor would let the entire Invoker unit fail, but others could potentially take over in that scenario. That also presupposes, that the Wasm executor can handle a potentially large amount of requests.

The executor also needs to be fast in general, in order to aid in alleviating the cold start problem. That includes efficient handling of receiving the module's code from OpenWhisk, instantiating it and setting up the underlying Wasm executor. In order to enable execution on all kinds of platforms, including edge devices, another requirement for a useful Wasm executor is its ability to run on different instruction set architectures.

\section{Executor implementation}


To implement these requirements in Apache OpenWhisk we will take the following steps. The current Docker-based invoker will be replaced, to forward all the incoming requests to our new Wasm executor.

The Wasm executor will be written from scratch in the Rust programming language. There are a number of reasons for this choice.

\begin{itemize}
    \item It is a compiled language with performance close to that of C.
    \item It uses LLVM in the backend so it can be compiled for almost any architecture.
    \item It allows for \quot{fearless concurrency}, since the compiler can detect data races at compile time.
    \item It has good support for asynchronous I/O, so it should be able to efficiently handle a large amount of concurrent requests.
    \item Two out of the three Wasm runtimes implemented here are written in Rust, so embedding them is efficient and well documented.
\end{itemize}

\begin{listing}[ht]
    \begin{minted}{rust}
#[async_std::main]
async fn main() -> anyhow::Result<()> {
    #[cfg(feature = "wasmtime_rt")]
    let runtime = ow_wasmtime::Wasmtime::default();

    #[cfg(feature = "wasmer_rt")]
    let runtime = ow_wasmer::Wasmer::default();

    #[cfg(feature = "wamr_rt")]
    let runtime = ow_wamr::Wamr::default();

    let mut executor = tide::with_state(runtime);

    executor.at("/:container_id/init").post(core::init);
    executor.at("/:container_id/run").post(core::run);
    executor.at("/:container_id/destroy").post(core::destroy);

    executor.listen("127.0.0.1:9000").await.unwrap();

    Ok(())
}
\end{minted}
    \caption{The main function of our WebAssembly executor. The \inl{\#[cfg(feature = "...")]} macros allow us to define multiple runtimes while deciding at compile-time which of them to enable. Thus, we will have three separate binaries in the end, while each only contains the code necessary for its runtime.}
    \label{listing:wasm-executor-main}
\end{listing}

The entry point into our executor will be through HTTP endpoints. In order for our executor to handle requests concurrently, we use Rust's \inl{async} features. We will not make use of any advanced HTTP features, so the choice of the web framework doesn't matter too much. We use \inl{tide} \cite{Turon2021}, an asynchronous by-default web framework ideal for prototyping. It builds on top of \inl{async-std}, the asynchronous version of the Rust standard library. The main function in listing \ref{listing:wasm-executor-main} suits itself well to describe the architecture. The \inl{\#[async\_std::main]} macro starts this program in an asynchronous runtime, which enables us to use \inl{async} functions and \inl{await} them. The async runtime allows multiple concurrent requests to one of the endpoints, without us having to spawn threads explicitly. Although, we will come back to this later when it becomes necessary to spawn workloads onto dedicated threads -- and go into more detail on \inl{async-std}.

We can instantiate one of our three runtimes of which only one is compiled in based on the feature we pass to the compiler. It is used as state, which is shared among requests. To start a container, the \inl{WasmContainer} in OpenWhisk generates a new universally unique identifier (uuid), without having to communicate with the Wasm executor at all. This is possible because there is no setup for each Wasm container, that needs to happen ahead-of-time, like network namespace creation for Docker containers. The actual work happens in the endpoints, which are parameterized by the container id. \inl{/init} is called once by OpenWhisk per container, to initialize the container with the module's code. Although different for each runtime; in general, this endpoint will do any work that can be frontloaded and which takes a significant amount of time. \inl{/run} is invoked potentially many times, so any work that is not frontloaded would multiply there. Of course, \inl{/init} is the potential culprit for the cold start latency, so our focus is on reducing even the amount work this endpoint needs to handle. We describe the optimizations to that end, later in this chapter. Once OpenWhisk decides to destroy the container, it does so through \inl{/destroy}.
Finally, we give \inl{tide} a function pointer for each endpoint and listen on a local address.

\subsection{Fast cold starts}
In serverless platforms there are roughly three important steps, in order for a user's code to be executed. Uploading the code to the platform, initializing the execution environment (cold start), and running it. The two latter steps are already part of the execution path; they occurr when the user already wants to run the code. Thus, at the cold start, the action should already be well-optimized for execution and as little preperation ought to happen. WebAssembly is binary target format, however not one, that is ready for \emph{fast} execution. It can be interpreted right away -- thus a small cold start time -- but exeuction performance will massively lack behind compared to native code, which is unacceptable for serverless. So compilation to native code is essential for good runtime performance, but it also takes time for that to be applied to a Wasm module. Given this trade-off, it would seem that Just-in-Time compilation should be the best model. In \inl{wasmtime}, our JIT-runtime of choice, compilation of a simple, WASI-enabled module (1,6 MB) still takes 35 milliseconds (or 40 milliseconds when optimizing the Wasm for speed) on our test machine. This time represents 98\% of the entire setup time, including every other part of the runtime. Thus, unsuprisingly, the cold start is defined almost entirely by the compilation phase. For comparison, the cold start time of the \inl{hello-world} Docker image on the same machine is 452 milliseconds. This number is roughly in line with the cold start times of AWS Lambda or Google Cloud Function, that we've cited in chapter \ref{chapter:background}. While the JIT cold start is an order of magnitude faster than the startup times of Docker containers, it is arguably still on the same order of magnitude as the execution time of some actions. An improvement of another order of magnitude would likely be an acceptable cold start latency.
Furthermore, since the compilation phase takes up such a big chunk of the overall time, it is the most obvious starting point for optimizations.

In contrast to browsers, which receive JavaScript or WebAssembly just-in-time, and thus need to compile it in the same manner, a serverless platform takes ownership of a module earlier. When a user uploads a module, the platform has an almost unconstrained amount of time to apply optimizations to the module, and make it ready for a fast startup \emph{and} execution; solving the previously described dilemma. The key idea is to run the expensive module creation ahead of time, where time means the time of execution. It doesn't matter in this case, whether the actual execution model of the underlying runtime is JIT or AoT. It's obvious for AoT - the code needs to be compiled ahead of time in its entirety, by definition. As we've seen, JIT also has a cold start penalty and we can complete that step ahead of time, too. We refer to this as precompilation. Once the module is precompiled, we can serialize it and store it in the database, instead of the Wasm module itself. At execution time, we deserialize it and can immediately start execution.

Both \inl{wasmer} and \inl{wamr} support AoT compilation. \inl{wasmer} in particular, allows us to leverage \inl{LLVM} for highly optimized native code to produce a shared object. The runtime loads this via a fast \inl{dlopen} call. \inl{wamr} produces its own AoT format which it can then deserialize at runtime and achieve a fast startup. \inl{wasmtime} doesn't support AoT in the traditional sense, but we can still execute the expensive module creation ahead-of-time, and serialize and store the result. Thus, all the runtimes support some form of precompilation.

\subsection{Warm starts}

Even though the cold start of our executor is well-optimized now, cold starting for every single invocation would still be too expensive for sequential invocations. Furthermore, in order to implement and adhere to OpenWhisk's design, we already need to store the module between \inl{init} and \inl{run}. Thus, we can take advantage of that design and keep the module around for some threshold time, before it is evicted. This is how OpenWhisk works with Docker containers today. After a cold start, OpenWhisk pauses the container and unpauses it if a matching request comes in, in which case the action invocation experiences a warm start. After the threshold -- which defaults to 10 minutes -- OpenWhisk calls the \inl{destroy} function on the container, removing it entirely.

One inefficiency of our design is that our executor store modules per container, rather than just once. If OpenWhisk decides to create two containers \inl{A} and \inl{B}, both containing the same module \inl{M}, then \inl{M} will take up storage twice. One reason for this design is the eviction process. Internally, the executor uses a thread-safe HashMap to store the modules and let many tasks or threads read from it simultaneously. A store-modules-once approach would require an atomic reference counting mechanism and additional locking to act on it, i.e. testing whether the reference count is 0 is atomic, but checking \emph{and} removing it is a critical section. Instead of introducing an additional lock per module and the additional synchronization overhead that comes with it, we trade memory for speed and store the module per container. Thus, removing a container is a trivial and fast remove operation on the HashMap. This design is also less of an issue in light of OpenWhisk's per-action concurrency limit. This limit states how many concurrent invocations a container should handle at most. If we increase this value beyond its default of one, then OpenWhisk will send invocation requests for the same action to the same container, up to that limit, without duplicating the module in the executor's memory. In particular, figure \ref{fig:openwhisk-concurrency-limit-greater-one} shows how a module can be reused in the same container -- skipping the cold start entirely.

\begin{figure}
    \includegraphics{figures/OpenWhiskConcurrencyLimitGreaterOne.pdf}
    \caption{An example of a container's life cycle when OpenWhisk's concurrency limit is set to a value greater one for that action. Note that a container is only represented in-memory by its module and instances, which is why there is no explicit \quot{container} label in the figure. The container is initialized with a module by OpenWhisk's call to \inl{/init}. Every \inl{/run} request to the same container id spawns a new instance of a WebAssembly module. In particular, two concurrent requests to \inl{/run} can be executed concurrently by the container where the instances are instantiated from the same module. How many requests can be handled in this manner is determined by OpenWhisk's concurrency limit. The instances are the actual \quot{containers} here, since they are the ones who are isolated from each other as well as the host. They are short-lived and never reused to guarntee that isolation, i.e. there is a one-to-one relationship between \inl{/run} requests and instances. The Wasm module will stay in memory until OpenWhisk's explicit call to \inl{/destroy}, which occurs after a container has been idle for some threshold time.}
    \label{fig:openwhisk-concurrency-limit-greater-one}
\end{figure}


The result of keeping modules in the executor's memory, is that they are already deserialized and a new instance can be instantiated from it immediately. An instance is the runtime's representation of a callable Wasm module. If a module is the equivalent of a Docker image, then the instance is analogous to the running container. Note that, unlike Docker-based OpenWhisk, we do not keep instances in-memory (containers), but only the modules (images). The cost of instantiating an instance is negligible. On our test machine it took, on average, 340 microseconds to setup a \inl{wasmtime} instance and the parts needed for execution. This represents the overhead on every warm invocation. Moreover, two instances can execute concurrently, even if the module itself is not thread-safe, for instance when reentrant execution would be unsafe. Finally, implemented this way, it would also be safe to execute two requests from different users, since instances are isolated from each other. They operate on their own memory space and have their own WASI environment. Thus, caching modules rather than instances is a good trade-off.

\subsection{The WasmRuntime abstraction}

\begin{listing}[ht]
    \begin{minted}{rust}
pub trait WasmRuntime: Clone {
    fn initialize(
        &self,
        container_id: String,
        capabilities: ActionCapabilities,
        module: Vec<u8>,
    ) -> anyhow::Result<()>;

    fn run(
        &self,
        container_id: &str,
        parameters: serde_json::Value,
    ) -> Result<Result<serde_json::Value, serde_json::Value>, anyhow::Error>;

    fn destroy(&self, container_id: &str);
}
\end{minted}
    \caption{The \inl{WasmRuntime} abstraction that each Wasm runtimes implements.}
    \label{listing:wasm-executor-trait}
\end{listing}


The Wasm executor is generic over the underlying WebAssembly runtime, such that we can easily implement different ones and compare them. To that end, we abstract the common runtime tasks into a trait -- essentially an interface in Rust's terminology. It is shown in listing \ref{listing:wasm-executor-trait}. This trait is used as the state in our executor. Thus, each runtime can have a different state internally, while being able to share it across requests. Since all our runtime implementations follow this interface, they work in similar ways. We describe the final design for each function, while pointing out the important differences between runtimes. Afterwards, we discuss alternatives.

\subsubsection{Initialization}
The executor decodes the base64 string given by OpenWhisk and unzips it, before calling \inl{initialize} with the result. The function initializes the container identified by the given id. All runtimes store the code they're given in a thread-safe HashMap -- the \inl{dashmap} \footnote{\url{https://crates.io/crates/dashmap}} library in our case.

\begin{description}[style=multiline, leftmargin=2.5cm, font=\bfseries]

    \item[Wasmtime] In \inl{wasmtime}, we instantiate a single \inl{Engine} when the executor starts up, which is then used by all \inl{Module}s for compilation. The bytes passed through \inl{initialize} are deserialized into a \inl{Module}, which is a fast operation, and then stored in the internal HashMap. This is possible because a \inl{Module} is thread-safe so the containing HashMap can be safely shared across threads.

    \item[Wasmer] In \inl{wasmer} we store both an \inl{Engine} and a \inl{Store} globally, since a \inl{Module} requires both for instantiation. We will see whether and how that affects memory consumption in chapter \ref{chapter:evaluation}. Like in \inl{wasmtime}, the \inl{Module} is thread-safe.

    \item[Wamr] \inl{wamr} is originally written in C, and we use \inl{wamr\_sys} \footnote{\url{https://crates.io/crates/wamr\_sys}} bindings to embed it in Rust. Contrary to the other runtimes, a module in \inl{wamr} is not thread-safe. Thus, only the serialized bytes representing the module are stored in the HashMap. It follows that the deserialization happens in every \inl{run} call instead, which is slightly more expensive.

\end{description}

Both \inl{wasmtime} and \inl{wasmer} underline the importance of the validity of the bytes given to the respective \inl{deserialize} methods. Unlike a Wasm module, the compilation artifacts can not be validated in the same way since they already represent an initialized module. So they have to be trusted, when the corresponding Wasm module would not have to be trusted. In our prototype, the users actually supply the compilation artifacts themselves, for development convenience reasons. For an actual serverless framework making use of Wasm, it would have to take ownership of the Wasm module and do the precompilation itself, in order to trust the code.

\subsubsection{Execution}

All runtimes implemented support the WebAssembly System Interface (WASI). Recall that WASI follows a capability-based security model, which allows for fine-grained control over what the module has access to. Hence, building a WASI context means setting up those capabilities, like forwarding \inl{stdin} or receiving from \inl{stdout}, setting environment variables or \inl{argv}. By default, the module also has access to WASI APIs like \inl{random\_get} for high-quality randomness or \inl{clock\_time\_get} to attain values from various clocks. WASI also controls on a per file descriptor basis, what directories the module has access to. Specifically, the modules needs a preopened file descriptor in order to access files. The Wasm runtimes therefore open the files and pass them to the module. However, that information is not present in OpenWhisk since Docker doesn't require it, so we need a way for users to specify these capabilities.

OpenWhisk actions can have optional annotations attached to them, which are simple key-value pairs. We can use those to let users specify capabilities. For instance:

\begin{minted}{sh}
wsk action create cache cache.wasm --annotation dir "/tmp/cache"
\end{minted}

Here a new action named \inl{cache} is created from a file \inl{cache.wasm} with the \inl{wsk} cli tool. A key-value pair is attached with \inl{dir=/tmp/cache}. With some minimal code changes to OpenWhisk's container abstraction, these annotations are then passed to the executor during \inl{initialize}. On the executor side, it is passed to each runtime which can then act on that information. It would be easily possible for a service provider to implement policies on top of it. Access could only be granted to the \inl{/tmp} directory in general; or a default cache directory \inl{/tmp/<container\_id>} could be created for each container, where information can be cached but only \emph{that} container has access to it. As WASI is currently under heavy development and doesn't, for example, specify any network-related interfaces, the amount of capabilities is limited. Our prototype implements directory access and our self-provided HTTP GET request. The latter is relevant to implement a network I/O-bound function for our evaluation in chapter \ref{chapter:evaluation}.


\begin{listing}[ht]
    \begin{minted}{rust}
ow_wasm_action::pass_json!(handler);

pub fn handler(json: serde_json::Value) -> Result<serde_json::Value, anyhow::Error> {
    let param1 = json
        .get("param1")
        .ok_or_else(|| anyhow::anyhow!("Expected param1 to be present"))?
        .as_i64()
        .ok_or_else(|| anyhow::anyhow!("Expected param1 to be an i64"))?;

    let param2 = json
        .get("param2")
        .ok_or_else(|| anyhow::anyhow!("Expected param2 to be present"))?
        .as_i64()
        .ok_or_else(|| anyhow::anyhow!("Expected param2 to be an i64"))?;

    Ok(serde_json::json!({ "result": param1 + param2 }))
}
\end{minted}
    \caption{A simple add action in Rust that uses a macro from \inl{ow-wasm-action} to pass the received json to the \inl{handler} and return the \inl{Result} back to the runtime.}
    \label{listing:add-action-example}
\end{listing}

\inl{run} executes the module associated with the given container id and with the given parameters as input. As required by OpenWhisk, the parameter is a JSON object. The WebAssembly minimum-viable product (MVP), only supports integer data types natively. Higher-level data types often differ in their implementation details between languages, so a universal target format like Wasm cannot decide for one of them, without shutting certain languages out or making it harder for them to be used. Making Wasm modules compiled from different source languages interoperable is an ongoing process -- with WebAssembly interface types\footnote{\url{https://hacks.mozilla.org/2019/08/webassembly-interface-types/}}. The \inl{wasm-bindgen} Rust library already makes this process easy for JavaScript-WebAssembly interoperability. Unfortunately, it does not work with any of our runtimes. Even if both our runtime and the module is written in Rust originally, passing a pointer is not possible because WebAssembly is sandboxed and could not access the pointee. To pass a complex type like a JSON object to the module, we require that it uses a wrapper. Internally, the wrapper allocates a global buffer of bytes. It exports three functions to work with this buffer, most notably one that returns a pointer. When executing the module, the runtime serializes the JSON object as bytes and allocates enough space in the buffer to store them. It then acquires the pointer, which is relative to the module's memory's base pointer, offsets it appropriately and writes the JSON bytes into the buffer. Since the module doesn't know the number of bytes written, the runtime passes that as its argument. On the other side, the module reads the given number of bytes from the buffer and calls the user-provided handler function. Reading the return value of the action uses the same process. Conveniently, the user only has to implement a function that takes a JSON object and returns one in the success case; an error otherwise. Thus, this crude method of passing parameters can be completely hidden from action developers. A simple action can be seen in listing \ref{listing:add-action-example}. The action development process is visualized in figure \ref{fig:action-creation-flow}, which makes use of the \inl{binaryen}\footnote{\url{https://github.com/WebAssembly/binaryen}} toolchain to optimize the Rust-produced WebAssembly.
% One point of interest is the memory allcator. Because Wasm's original use case is in the web browser, where the binary is downloaded just-in-time, the binary size matters. Thus, a smaller memory allocator was written for Rust, named \inl{wee_alloc}\footnote{\url{https://github.com/rustwasm/wee_alloc}}. It effectively trades performance for a smaller size. Since we precompile our WebAssembly the size difference is negligible. If we were not precompiling, this might be a more interesting feature to activate.

\begin{figure}
    \includegraphics{figures/ActionCreationFlow.pdf}
    \caption{There are two phases in the action development. Our \inl{ow-wasm-action} library provides the abstraction to read the parameters the runtime has passed and to write the return value. To that end, it uses \inl{serde\_json}, a (de)serialization library to read and write the JSON objects. Afterwards, \inl{wasm-opt} runs an optimization pass on the produced WebAssembly, since the runtimes work better with already optimized Wasm.
        Depending on what runtime is compiled into the executor, the corresponding precompiler is used. For \inl{wasmtime}, we have written the small \inl{ow-wasmtime-precompiler} program that does that, while \inl{wamrc} and \inl{wasmer compile} can be used for the others. Finally, the resulting precompiled binary is zipped which reduces the transmission size and makes OpenWhisk treat it as binary.}
    \label{fig:action-creation-flow}
\end{figure}

The creation of the Wasi context as well as the actual execution is slightly different for each runtime.

\begin{description}[style=multiline, leftmargin=2.5cm, font=\bfseries]

    \item[Wasmtime] The \inl{wasmtime} runtime, as of version \inl{0.23}, uses \inl{cap-std}, a capability based implementation with a similar API as the Rust standard libary. We can construct its \inl{Dir} type and use it while building a \inl{WasiCtxBuilder}, ultimately yielding a \inl{WasiCtx}. The context is \emph{linked} with the module using the runtime's \inl{Linker}, which is similar to an operating system's dynamic linker. It resolves the imports that the module needs and instantiates it with them.
          To execute, we create a \inl{Store} per request, thus per \inl{run} invocation. This is because a \inl{Store} is neither thread-safe nor meant to be long-lived, according to the documentation. The \inl{Store} is used as a collection of the instances and the host-defined items, according to the documentation.

    \item[Wasmer] In \inl{wasmer} we build up a \inl{WasiEnv} with a very similar API as \inl{wasmtime}'s, including directory access. The module and environment are combined into an \inl{ImportObject} from which an \inl{Instance} is instantiated.

    \item[Wamr] \inl{wamr}'s API is slightly lower-level and less ergonomic compared to the other runtimes due to it being a C-API rather than an idiomatic Rust one.
          Two functions need to be called globally, for the runtime to be instantiated and torn down, \inl{wasm\_runtime\_init} and \inl{wasm\_runtime\_destroy}, respectively.
          Our implementation requires each runtime state to implement \inl{Clone}, a trait indicating that an object can be duplicated. A common pattern in Rust (also used by \inl{wasmtime} and \inl{wasmer}) is to implement the clone operation to produce a shallow copy. Internally, these objects typically use Rust's atomically reference-counted smart pointer (\inl{Arc}) to share object's across threads, safely. This makes it cheap to share the runtimes object across different threads while having access to the same underlying data. Thus, on each request a clone of the runtime is created and destroyed (dropped) at the end. Because of that we cannot call \inl{wasm\_runtime\_destroy} in the runtime's \inl{Drop} implementation. Instead, we use another internal Arc, which is instantiated with an object that calls the initialization function when it is created, and destroys the context when it is being dropped. Because the \inl{Arc} will live as long as the runtime and only calls the contained object's \inl{drop} function when all other references have gone away, this implements our requirements perfectly.

\end{description}

So in summary -- across all runtimes -- \inl{run} looks up the previously stored module with the container id, creates a WASI context with the associated capabilities, passes the parameters and executes the module's default function. If an error occurs during the Wasm runtime setup, the outer \inl{Result} contains an \inl{anyhow::Error}, a generic, application-level error type. Since the Wasm module itself can also either succeed or fail, another \inl{Result} is nested inside, where both success and failure are represented by JSON objects.

In what task or thread the module is executed, will be discussed shortly.

\subsubsection{Cleanup}
Finally, the \inl{destroy} function removes the container, which simply means freeing the memory taken up by the module. This is fast and thread-safe, since each container manages its own Wasm module. Once OpenWhisk has decided to destroy the container, it will synchronize internally to not call \inl{run} on it again.

\subsection{Enabling concurrency}

The executor consists of a web server that can take requests from the invoker asynchronously and concurrently. Because OpenWhisk doesn't communicate with the Wasm modules directly, but through the executor, the \inl{/start} endpoint simply returns the executors own network address. This endpoint also returns an identifier for the container, so that OpenWhisk can handle its life cycle, i.e. the above-mentioned mechanism of keeping it warm for some time, for a faster subsequent invocation. There is a chance that this mechanism may not be necessary for Wasm modules, as they are much more lightweight in their instantiation. We will come back to this question later in our optimization discussion.


Rust's async model works with \inl{Futures} -- also known as Promises in JavaScript -- which represent values that have not yet been computed. Contrary to \inl{Promises} however, Rust's \inl{Futures} are lazy: They do nothing unless actively polled. This task falls to \inl{async} runtimes, of which \inl{async-std} provides one. In \inl{async-std} the unit of execution is a \inl{Task}, which is similar to a \inl{Thread}, except it is driven to completion by the user space runtime instead of the OS. Many of those tasks are executed on the same thread, and by default, a \inl{Thread} per logical CPU core is spawned. Thus we have $M \cdot N$ tasks in total, where $M$ is the number of tasks per thread and $N$ the number of logical CPU cores.
From that, a second distinction is implied. A \inl{Task} should not do a blocking or long-running operation, like reading from a file via the standard library's \inl{std::fs::read\_to\_string}. Instead, it should use the \inl{async} version of that method, from \inl{async-std}, which does not block the thread. Otherwise, all other $M - 1$ tasks, which are executed on the same thread, will also be blocked, even though they may be able to make progress. Note that, we could still process $N$ requests concurrently, even if tasks block, but that doesn't scale well enough. This is why these kinds of async functions are also called cooperative routines (coroutines). When they need to wait for the OS to finish a task, they \emph{yield} to another routine, which can then continue actual work.

That becomes relevant for us, when we execute WebAssembly modules. Taking the module, setting up the Wasm runtime and executing it, is a long-running computation -- and one that could block as well. This would in turn block the thread we are currently running on, which means, only $N$ Wasm modules can be executed concurrently. This is not an issue for computationally-intensive workloads, where no more work can be done concurrently, than the CPU has logical cores. However, for I/O-bound workloads, such as requests over the network, this can quickly become a bottleneck. Since one of the appealing offerings of serverless frameworks is their elasticity, we clearly need a way to handle many more requests, even if they block.
To let other coroutines on the same thread make progress, while one executes the module, we can push the entire execution onto a thread pool. The coroutine then has the simple job to asynchronously wait for that execution to finish and collect the result. That allows the other coroutines to process more requests in the meantime.

To test the theory, whether the thread pool model is able to handle more requests, we run two short experiments.

\begin{enumerate}
    \item Compute-bound: Calculating a large prime number.
          % \item Filesystem-I/O-bound: Reading a 30 MB file from disk.
    \item Network-I/O-bound: Making a long-running (1 second) HTTP request.
\end{enumerate}

Our test machine has 8 logical CPU cores, resulting in 8 threads being started for the async-std runtime. We thus send 16 concurrent requests to be able to see the potential blocking in the result. The first experiment showed no difference in the total execution time between the default and thread pool model. As we would expect, in the default model the compute-bound task finishes the first 8 requests, before starting the second set. The execution in a thread pool executes all 16 requests concurrently, but each took roughly twice as long, resulting in no significant difference between the total execution time. The default model operates more along the first-come, first-serve principle, where an early request will also get a response more quickly. In this simplified example, where all workloads are compute-bound, this may be preferable.

\begin{figure}
    \begin{center}
        \input{figures/async_std_default_thread_vs_thread_pool.pgf}
    \end{center}
    \caption{Executing blocking Wasm workloads under the \inl{async-std} default model with 8 threads, compared with running each in a thread from a pool.}
    \label{fig:default_vs_thread_pool}
\end{figure}

The result of the second experiment is shown in figure \ref{fig:default_vs_thread_pool}. In the default model, the first 8 requests block the other 8, while they wait for the HTTP request to finish, resulting in roughly 2 seconds of total execution time. Unsurprisingly, the thread pool model finishes in half the total execution time, since it can handle all 16 requests simultaneously, due to using a new thread from the pool for each blocking operation.

Both models seem to have a workload better suited to them. However, in a real-world, mixed-workload scenario, a long-running, compute-bound function may prevent another I/O-bound one from initializing its computationally cheap request. A mixed scenario may therefore also be better served by the thread pool model.

\begin{figure}
    \begin{center}
        \input{figures/async_std_default_thread_vs_thread_pool_mixed.pgf}
    \end{center}
    \caption{The average request duration and the average amount of requests finished per second under the default and thread pool execution model, sampled over 50 runs.}
    \label{fig:default_vs_thread_pool_mixed}
\end{figure}

In figure \ref{fig:default_vs_thread_pool_mixed} a mixed scenario is run, where 16 compute-bound and 16 I/O-bound requests are executed simultaneously. In the default model, the compute-bound requests finish in about one third of the time they take under the thread pool model. This is again due to more requests being processed concurrently in the latter model. This is also why the standard deviation is slightly higher in that model. For I/O-bound workloads, there is a slight advantage for the default model. However, in the context of the plot on the right side, the thread pool model is able to handle more requests per second in general, but specifically excels at the I/O-bound ones. There is an inherent trade-off between serving a request as soon as possible, and the time it takes to finish serving it. It depends on the expected workload, which model should be used.
