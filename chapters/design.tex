\chapter{Design}
\label{chapter:design}

In this chapter, we lay out the requirements that an implementation of a WebAssembly container runtime should satisfy in general. We introduce several WebAssembly runtimes and pick three of them for our implementation. We then expand on the requirements by looking at OpenWhisk specifically and how we need to modify it to enable our new container runtime, but also detail other approaches that could be used to run WebAssembly workloads. Finally, we describe and discuss our concrete implementation in Apache OpenWhisk, how to achieve fast cold starts and high performance, while also examining the trade-offs involved.

\section{Core approach}

As we've seen in the previous chapter, there are a number of issues with serverless in general, but also serverless on the edge. In this section we will summarize the core problem and describe our idea for rectifying these issues to make serverless (edge) computing more viable.

A substantial cold start latency has emerged as the paramount issue of the serverless platform. It prevents serverless platforms from implementing true scale-to-zero, since functions are kept warm after invocations, wasting energy and resources. While we may not be able to get rid of the keep-alive completely, a significant reduction in cold start latency would allow a similarly large reduction in keep-alive time. Since energy and resources are even more precious on typical edge devices, this is also very important for their quality of service.
Average cold start times are on the same order of magnitude as average function execution times, thereby significantly impacting the quality of service a serverless platform can offer. Moreover, serverless is employed by developers for bursty, unpredictable workloads, a pattern that is also common at the edge. In this scenario we see concurrent requests, which translate to concurrent cold starts. The more concurrent cold starts, the longer each one takes \cite{Cui2018}.
Operators are forced to choose between saving cost and resources or enabling a high quality of service -- in particular shorter response times. While this trade-off is inevitable to some degree, the cold start exacerbates it. A new container runtime must alleviate that latency, to provide mutual benefits for the serverless user and the operator.

Serverless enables the execution of »polyglot tenant-provided code« \cite{Nastic2018}. A new runtime must keep up these principles of being programming language agnostic to continue enabling all developers to use the serverless platform.
To allow for multi-tenancy on the same physical host, containers need to be sandboxed securely. As described in the previous chapter, WebAssembly provides sandboxing and support for many languages, with support for more likely to come. At the same time, it is a lighter-weight solution than virtualization with Docker containers. In contrast to Docker, its isolation does not depend on operating system features, which may incurr a performance cost on the downside. Thus, it seems like a good candidate to replace Docker containers in serverless platforms while alleviating the cold start latency. This is also why WebAssembly may fare better on low-power edge devices.

\citeauthor{Mendki2020} compared WebAssembly and Docker container startup times, and found WebAssembly being 91\% faster than containers and used 75\% less memory \cite{Mendki2020}. \citeauthor{Hall2019} found their WebAssembly-based serverless platform fared 60\% better, on average, than the Docker container solution, for their concurrent »Multiple Client, Multiple Access« workload \cite{Hall2019}. These promising results are the basis for our work and validate the core idea of the approach.

\section{Requirements}

Some of the requirements for a WebAssembly container runtime are a direct consequence of the issues we have assessed. We will expand on others in the following sections. The summary of concerns and requirements are:

\begin{enumerate}
    \item Be easily integratable with an existing serverless framework.
    \item Be programming-language agnostic.
    \item Provide a cheap sandboxing mechanism to ensure multi-tenant capability and no adverse effects on the cold start latency.
    \item Be as close to native speed as possible to be a viable alternative to existing container runtimes.
    \item Run on devices with potentially different instruction set architectures (at least \inl{x86\_64} and \inl{ARM}).
    \item Be able to efficiently handle a large amount of I/O concurrently, due to all requests being proxied through.
    \item Be thread-safe.
\end{enumerate}

Finding a suitable serverless framework that we can modify and integrate with, will be the topic of the next section. In chapter \ref{chapter:background} and the preceding section we have seen how WebAssembly provides some of these requirements, such as sandboxing, good execution speed and language-agnosticism. However, for our implementation we need concrete WebAssembly runtimes that provide these properties, so we will explore the options for such runtimes and work out the differences. The choice for which language and libraries to implement our executor with will be driven by how the Wasm runtimes we choose can be integrated. Other concerns include having access to highly performant, concurrent I/O and support for correctness guarantees, including thread-safety.

\subsection{Apache OpenWhisk}

We use Apache OpenWhisk as the framework to implement WebAssembly support for. OpenWhisk is a production-ready framework, used as the basis for IBM's Cloud Functions. Thus, implementing Wasm support in OpenWhisk would show the viability of the approach in the real world. The framework is also frequently used by serverless researchers to implement new ideas such as pause containers to reduce cold start times \cite{Mohan2019} or Wasm support via \inl{node.js} \cite{Hall2019}, making it a well-respected and frequently used framework. OpenWhisk also has \emph{lean} components, which are lighter-weight replacements for its heavier ones, such as Kafka. These enable OpenWhisk to be deployed on edge devices, such as Raspberry Pis, on which we plan to evaluate our implementation. Furthermore, because OpenWhisk is already designed to be easily extensible with new language runtimes, the protocol is straightforward to implement.



\section{WebAssembly in OpenWhisk}

In this section we examine the possibilities for bringing WebAssembly support to OpenWhisk.
In the rest of this section we will explore the available options and discuss them in the context of these requirements.

\subsection{OpenWhisk's anatomy}

In order to understand the requirements for our Wasm-flavored OpenWhisk, we need to understand the design of OpenWhisk itself first.

\begin{figure}
    \includegraphics{figures/OpenWhiskActionInvocationFlow.pdf}
    \caption{The invocation flow of an action in Apache OpenWhisk, based on the source code and documentation \cite{OpenWhiskSystemDesign}. Potentially many Invokers can exist on different hosts in this setup.}
    \label{fig:openwhisk-action-invocation-flow}
\end{figure}

Figure \ref{fig:openwhisk-action-invocation-flow} sketches the design of OpenWhisk. Users interact with it through its API Gateway, which consists of \inl{nginx}, mostly for SSL termination, and the \inl{Controller}, which implements the main logic. It handles the creation of actions - OpenWhisk's name for serverless functions - authentication and authorization as well as invoking the action.
In the latter case, the controller uses its load balancer to determine one of the potentially many \inl{Invoker}s, that should handle the request. The \inl{Invoker}s are subscribed to an Apache Kafka queue, to which the load balancer publishes the request. The controller and its load balancing apparatus take a central role in OpenWhisk, but in our figure it is represented by the API Gateway and load balancer only, simply because we will not be modifying this part.
Our focus is on the \inl{Invoker}. It is the part of OpenWhisk responsible for \emph{invoking} the action. A number of steps are involved in that process \cite{OpenWhiskSystemDesign}.

\begin{enumerate}
    \item It retrieves the action's code and execution permissions from an Apache CouchDB database, where the Controller stored it during action creation.
    \item It instructs the Docker daemon to start a new container, based on the runtime that the action needs to execute. This could be a \inl{openwhisk/action-nodejs-v10} image for JavaScript actions, but could also be a generic \inl{openwhisk/dockerskeleton} black-box image, that can execute any binary, such as one written in Rust or C.
    \item Finally, it injects the action's code into the container, invokes it with the given parameters and returns the result \cite{OpenWhiskSystemDesign}.
\end{enumerate}

Because OpenWhisk supports many runtimes and even the mentioned black-box image, it needs a common protocol to communicate with all of them. The just-described step 3 is the gist of that protocol. Each runtime needs to implement a number of functions. The \inl{start} function creates a new container and returns its address. The following requests to the \inl{/init} endpoint are then sent to that address, where the container receives the code and takes the necessary steps to make the action ready for execution. Thus, over a containers life cycle, this endpoint is called exactly once. Once initialized, the \inl{/run} endpoint can be called multiple times to execute the action \cite{OpenWhiskSystemDesign}.

That implies that the container is not destroyed immediately after it has been invoked. Indeed, OpenWhisk, just like other known serverless platforms, uses various optimizations of the described flow, in order to improve the system's performance. Among those are pre-warming containers or keeping the container running for some time after it has been invoked. Both have the same effect: When the user wants to invoke the action, no initialization is necessary and \inl{/run} can be called immediately. These optimizations exist precisely because the cold start has been recognized as an expensive operation. In the absence of invocations and some threshold time elapsed, the container is removed entirely. In that case, the container runtime's \inl{destroy} function is called, which removes the container, thereby freeing up the memory it uses \cite{OpenWhiskSystemDesign}. These optimizations are part of the OpenWhisk logic independent of the underlying container runtime. Our container runtime will profit from them too, but we will have to take them into account in our design.

\subsection{OpenWhisk on Kubernetes}

OpenWhisk can be deployed on a Kubernetes cluster, a container orchestration tool for automating deployment and scaling of containers \cite{Kub2021}. OpenWhisk has a service provider interface (SPI) called \inl{ContainerFactoryProvider}, which represents the underlying container management platform. It provides two implementations of that SPI \cite{OWKub2020}.

\begin{enumerate}
    \item The \inl{DockerContainerFactory} implements the architecture shown above, except that most components of OpenWhisk run in separate Kubernetes pods. In non-Kubernetes deployments, they usually run in separate Docker containers. The crucial part of this deployment is that the Invoker runs in a single pod and also spawns all of the docker containers inside that pod. Multiple Invokers can run on different worker nodes. Because OpenWhisk talks directly to the local Docker daemon, container management latency is rather small, but this SPI implementation does not take advantage of Kubernetes' scaling mechanisms.
    \item The \inl{KubernetesContainerFactory} is the counterpart. It interfaces with the Kubernetes API to create and schedule actions in containers -- in turn contained in pods -- to run on different Kubernetes worker nodes. This trades higher container management latency for access to Kubernetes' scaling and scheduling mechanisms.
\end{enumerate}

\subsection{Executing WebAssembly with OpenWhisk}

To execute Wasm modules in OpenWhisk, we have two similar options with similar trade-offs.

\begin{enumerate}
    \item The first is using one of a number of WebAssembly runtimes that are currently being developed and are in various stages of maturity. We would need to embed one of these runtimes in a program of our own that provides a similar interface as Docker. This WebAssembly container runtime would then be able to replace the Docker daemon in the architecture presented above, and run Wasm modules instead of Docker images. Since this program would sit in-between OpenWhisk and the Wasm runtime, we would have precise control over the management of the modules, i.e. whether the Wasm module is just-in-time compiled, interpreted or compiled to native code ahead-of-time; the caching layer that keeps initialized modules ready for fast execution; or the configuration of the system interface with which Wasm modules can interact with the underlying operating system.
    \item The second option is to use Kubernetes as the management system for our Wasm modules. Kubernetes is generic over the underlying container runtimes, and as such it runs on \inl{containerd}, \inl{CRI-O} and Docker \cite{Kub2021}. However, it is also possible to let Kubernetes pods run on other runtimes by implementing the \inl{kubelet} API. The \inl{krustlet} project provides a \inl{kubelet} implementation that can execute Wasm workloads \cite{Krustlet2021}. With that, we can now instruct Kubernetes to run Wasm modules for us, instead of the typical Docker container.
\end{enumerate}

We will explore those two options in more detail and discuss which may be a better fit for our requirements.

\subsection{Krustlet: The WebAssembly Kubelet}

As a \inl{kubelet}, \inl{krustlet} runs on Kubernetes worker nodes and accepts workloads of the \inl{wasm32-wasi} architecture. Just like a regular kubelet, krustlet will then pull the Wasm module from an OCI registry and run it, just like a Docker image is pulled from Docker Hub. This process allows us to schedule our Wasm workloads from an Invoker, similar to how the \inl{KubernetesContainerFactory} implementation schedules actions in Docker containers. However, this execution model is also different from the OpenWhisk model in some ways.

\begin{itemize}
    \item In OpenWhisk there is a split between runtime environment, provided through the Docker image, and the code that is stored in a database, retrieved from there and injected into the container. Krustlet expects the Wasm module to be the equivalent of the container, but with the code baked in. Thus no injection is necessary. This makes it slightly harder to integrate with the current OpenWhisk model, since the Wasm module doesn't need to be stored in the database, but rather in the OCI registry. That registry would need to be part of the Kubernetes cluster for latency reasons alone, but also since the images are the Wasm modules uploaded to OpenWhisk as actions. They are specific to that OpenWhisk deployment, rather than the generic OpenWhisk runtime environments, currently stored in Docker Hub. We can assume, that the latency introduced by retrieving the image from the registry is offset by not having to retrieve the code from the database, so both solutions should perform the same in that regard.
    \item Since there is no split anymore, every Wasm module needs to implement the OpenWhisk runtime protocol itself, instead of being able to rely on the runtime implementing it once. In practice, this means implementing the \inl{/init} and \inl{/run} endpoints, where the former would no longer do anything useful, since there is no code to inject. It follows that every Wasm module needs to include an HTTP server, increasing its size above what would be needed for only its functionality. Because network support in WASI is still under active development, the alternative is to implement a \inl{wasmcloud-actor}, which essentially provides capabilities such as networking to Wasm modules \cite{WC2021}. The function can then be run by calling the \inl{/run} endpoint. Just like containers in OpenWhisk keep running for some time after having been called once, the module would also continue to listen for more requests to avoid the cold-start, for some time.
\end{itemize}

% Although, if containers are independent of the thing they were spawned from, that makes the system more resilient, thus the slightly higher cost of implementing the OW protocl + HTTP Server may be offset.

While this approach has its merits, we believe that there would be higher friction for implementing it in OpenWhisk, than writing our own layer to Wasm runtimes. It provides more control over optimizing the cold-start latency as well as the execution performance, which are part of the requirements we laid out.

\subsection{An OpenWhisk-WebAssembly Executor}

Because the first-mentioned approach allows us to integrate Wasm into OpenWhisk with fewer changes to it, we choose to follow this path. We will implement a layer between OpenWhisk and different WebAssembly runtimes, in order to enable the execution of Wasm modules in OpenWhisk. We refer to this as the Wasm executor, to distinguish it from Wasm runtimes -- those responsible for the actual execution of Wasm modules. We aim to replace the Docker daemon in the architecture shown above, leveraging the existing OpenWhisk interface for container management.

\begin{figure}
    \includegraphics{figures/WasmOpenWhiskActionInvocationFlow.pdf}
    \caption{Invocation flow of a Wasm action in our modified Apache OpenWhisk. The Invoker \emph{injects} the code into the executor which \emph{creates} a Wasm Module ready for execution. It then \emph{instructs} the executor to \emph{invoke} the module with the parameters it passes. The result is passed back to the Invoker. Again, a deployer can define whether one or more Invokers exist.}
    \label{fig:wasm-openwhisk-action-invocation-flow}
\end{figure}

In figure \ref{fig:wasm-openwhisk-action-invocation-flow} we have sketched the high-level modifications that we need to make to OpenWhisk. In this invocation flow, we need to hook into OpenWhisk's invoker to enable its communication with the Wasm executor we will implement, instead of the Docker daemon. Fortunately, the Invoker is already well-separated from the concrete containerization technology through a Service Provider Interface (SPI). OpenWhisk already supports Docker, but also Kubernetes as the container management system, such that the SPI was a logical addition. We implement a \inl{WasmContainer} that extends OpenWhisk's \inl{Container} abstraction, which already handles container communication, such as calling the \inl{/init} and \inl{/run} endpoints.

% Figure that shows a zoomed-in version of the invoker and its SPIs (as well as the Wasm executor?) - Help better understand OW's design and what we added

In OpenWhisk's Docker container implementation, OpenWhisk only uses the Docker daemon to create a container, but then communicates with the container directly. In our implementation, every request is proxied through the Wasm executor. The advantage is, that not every Wasm module needs to implement the OpenWhisk protocol, which requires an HTTP server, which would both increase the module's size and its runtime overhead. The disadvantage is the single point of failure. That is offset by the bigger picture of OpenWhisk, where in a truly resilient deployment, we already have multiple Invokers. A failing Wasm executor would let the entire Invoker unit fail, but others could potentially take over in that scenario. That also presupposes, that the Wasm executor can handle a potentially large amount of requests.

The executor also needs to be fast in general, in order to aid in alleviating the cold start problem. That includes efficient handling of receiving the module's code from OpenWhisk, instantiating it and setting up the underlying Wasm executor. In order to enable execution on all kinds of platforms, including edge devices, another requirement for a useful Wasm executor is its ability to run on different instruction set architectures.

\section{WebAssembly runtimes}
\label{section:wasm-runtimes}

WebAssembly is a specification\footnote{\url{https://webassembly.org/specs/}} and implementations adhering to this spec are WebAssembly runtimes. Since Wasm was originally designed for the web, the virtual machines of browsers, such as Google's V8 or Mozilla's SpiderMonkey, can also execute WebAssembly in addition to JavaScript. With the rise of Wasm outside the Web, a large number of standalone runtimes have emerged and many are actively being developed. This section will introduce a number of runtimes with different execution models, and justify our choice of runtimes to implement in our executor.


\subsection{Wasmtime}

Wasmtime\footnote{\url{https://github.com/bytecodealliance/wasmtime}} is a runtime that has support for Just-in-Time compilation, available for the x86\_64 and aarch64 instruction set architectures (ISA). It is written in Rust and thus easily embeddable from there. Support for the latest WASI standards and future proposals for Wasm are implemented.
It is developed as part of the Bytecode Alliance\footnote{\url{https://bytecodealliance.org/}}, with founding members like Mozilla, fastly, Intel and RedHat. The \inl{cranelift} code generator is developed as part of the alliance and \inl{wasmtime} uses it for Just-in-Time compilation. The code generator's documentation points out that the backend for \inl{x86\_64} is »the most complete and stable; other architectures are in various stages of development \cite{Alliance2021}.« 
Thus it will most likely run on an \inl{aarch64} Raspberry Pi, but performance may not be on-par with an \inl{LLVM}-based runtime.

\subsection{Lucet}

Lucet\footnote{\url{https://github.com/bytecodealliance/lucet}} is the compiler and runtime used for cloud provider fastly's Terrarium, a platform for running WebAssembly on edge devices \cite{fastly2019}. It consists of a compiler and a runtime to execute WebAssembly. The compiler, \inl{lucetc}, compiles Wasm modules to native object or shared object files. The complementary \inl{lucet-runtime} can then load the native code and call the contained Wasm functions. Because of this compilation model, execution can reach near-native speeds. However, the execution is not sandboxed by default, requiring mechanisms like \inl{seccomp-bpf}\footnote{\url{https://wiki.mozilla.org/Security/Sandbox/Seccomp}}, and the security model is actively being developed. It is currently only available on the \inl{x86\_64} ISA. Since we require support for edge devices, typically built on the arm ISA, we will not be using \inl{lucet}. Furthermore, the lack of mature sandboxing makes other runtimes a better choice.

\subsection{Wasmer}

Wasmer\footnote{\url{https://github.com/wasmerio/wasmer}} is a WebAssembly runtime with support for the three major platforms and \inl{x86\_64} and \inl{aarch64} ISAs. Notably, wasmer has already reached version \inl{1.0}, indicating a certain level of maturity. The runtime offers three different compilers, \inl{singlepass}, \inl{cranelift} and \inl{LLVM}, whose compilation times get slower and execution times get faster, in that order, respectively. The runtime has a JIT and a native engine, which differ in the way the module is loaded. In particular, the native engine produces a shared object, which can be loaded with a fast \inl{dlopen} call. The support for high-quality AoT compilation with \inl{LLVM} and fast startup times makes wasmer a promising choice for our purpose.

\subsection{WASM3}

Due to Wasm's simplicity in the binary format, there is wide-spread interest for running it on embedded devices. An example of that is \inl{wasm3}\footnote{\url{https://github.com/wasm3/wasm3}}, a Wasm interpreter that claims to be the fastest. They argue in favor of interpretation over JIT or AoT compilation, since startup latency is more important in many situations. Furthermore, portability and security are easier to achieve with this approach and development with the virtual machine is simpler. Still, a performance penalty of 4 to 5x compared to JIT or 12x compared to native execution, leads us to believe that no amount of startup latency gain can make up for the performance loss. Even though our primary goal is that of reducing the cold start latency, the execution that follows cannot massively lack behind Docker containers, if adoption is to succeed. The relationship between startup and execution performance is too far out of balance in this case.

\subsection{WebAssembly Micro Runtime}

However, there is still value in runtimes written specifically for embedded devices. The WebAssembly Micro Runtime\footnote{\url{https://github.com/bytecodealliance/wasm-micro-runtime/}} (\inl{wamr}) has support for all three mentioned execution modes. It has low memory usage and binary size. For example, the claimed binary size of the AoT runtime is just 50 kilo bytes. Further, the AoT runtime claims near-native speed. For execution on edge devices, where memory is scarce, this runtime may allow more modules to be kept in-memory compared to heaver-weight runtimes.

A summary of the runtimes we looked at can be found in table \ref{table:wasm-runtime-overview}. The diversity of execution modes sticks out -- clearly no single mode is the best for all scenarios. Except for \inl{lucet}, all runtimes have good platform support. However, particularly for edge devices, the lack of support for the 32-bit arm architecture can be challenging in the Rust-written runtimes (\inl{wasmtime}, \inl{wasmer}, \inl{lucet}). The support in the other ones, written in C, is there.

\newcommand{\linux}{\input{icons/linux.pdf_tex}}
\newcommand{\macos}{\input{icons/macos.pdf_tex}}
\newcommand{\windows}{\input{icons/windows.pdf_tex}}
\newcommand{\freebsd}{\input{icons/freebsd.pdf_tex}}
\newcommand{\android}{\input{icons/android.pdf_tex}}
\newcommand{\ios}{\input{icons/ios.pdf_tex}}

\begin{table}[h!]
    \centering
    \begin{tabular}{c | c | c | c}
        Runtime        & ISA support & Execution modes  & Platform support\\
        \hline
        \inl{wasmtime} & \inl{x86\_64}, \inl{aarch64} & AoT, JIT & \linux, \macos, \windows\\
        \inl{lucet}    & \inl{x86\_64} & AoT & \linux, \macos\\
        \inl{wasmer}   & \inl{x86}, \inl{x86\_64}, \inl{aarch64} & AoT, JIT & \linux, \macos, \windows\\
        \inl{wasm3}    & \inl{x86}, \inl{x86\_64}, \inl{arm}, \inl{RISC-V}, ...  & Interpreted      & \linux, \macos, \windows, \freebsd, \android, \ios\\
        \inl{wamr}     & \inl{x86}, \inl{x86\_64}, \inl{arm}, \inl{aarch64}, ... & AoT, Interpreted & \linux, \macos, \windows, \android, ... \\
    \end{tabular}
    \caption{WebAssembly runtime feature overview.}
    \label{table:wasm-runtime-overview}
\end{table}

\section{Executor implementation}


To implement these requirements in Apache OpenWhisk we will take the following steps. The current Docker-based invoker will be replaced, to forward all the incoming requests to our new Wasm executor.

The Wasm executor will be written from scratch in the Rust programming language. There are a number of reasons for this choice.

\begin{itemize}
    \item It is a compiled language with performance close to that of C.
    \item It uses LLVM in the backend so it can be compiled for almost any architecture.
    \item It allows for \quot{fearless concurrency}, since the compiler can detect data races at compile time.
    \item It has good support for asynchronous I/O, so it should be able to efficiently handle a large amount of concurrent requests.
    \item Two out of the three Wasm runtimes implemented here are written in Rust, so embedding them is efficient and well documented.
\end{itemize}

\begin{listing}[ht]
    \begin{minted}{rust}
#[async_std::main]
async fn main() -> anyhow::Result<()> {
    #[cfg(feature = "wasmtime_rt")]
    let runtime = ow_wasmtime::Wasmtime::default();

    #[cfg(feature = "wasmer_rt")]
    let runtime = ow_wasmer::Wasmer::default();

    #[cfg(feature = "wamr_rt")]
    let runtime = ow_wamr::Wamr::default();

    let mut executor = tide::with_state(runtime);

    executor.at("/:container_id/init").post(core::init);
    executor.at("/:container_id/run").post(core::run);
    executor.at("/:container_id/destroy").post(core::destroy);

    executor.listen("127.0.0.1:9000").await.unwrap();

    Ok(())
}
\end{minted}
    \caption{The main function of our WebAssembly executor. The \inl{\#[cfg(feature = "...")]} macros allow us to define multiple runtimes while deciding at compile-time which of them to enable. Thus, we will have three separate binaries in the end, while each only contains the code necessary for its runtime.}
    \label{listing:wasm-executor-main}
\end{listing}

The entry point into our executor will be through HTTP endpoints. In order for our executor to handle requests concurrently, we use Rust's \inl{async} features. We will not make use of any advanced HTTP features, so the choice of the web framework doesn't matter too much. We use \inl{tide} \cite{Turon2021}, an asynchronous by-default web framework ideal for prototyping. It builds on top of \inl{async-std}, the asynchronous version of the Rust standard library. The main function in listing \ref{listing:wasm-executor-main} suits itself well to describe the architecture. The \inl{\#[async\_std::main]} macro starts this program in an asynchronous runtime, which enables us to use \inl{async} functions and \inl{await} them. The async runtime allows multiple concurrent requests to one of the endpoints, without us having to spawn threads explicitly. Although, we will come back to this later when it becomes necessary to spawn workloads onto dedicated threads -- and go into more detail on \inl{async-std}.

We can instantiate one of our three runtimes of which only one is compiled in based on the feature we pass to the compiler. It is used as state, which is shared among requests. To start a container, the \inl{WasmContainer} in OpenWhisk generates a new universally unique identifier (uuid), without having to communicate with the Wasm executor at all. This is possible because there is no setup for each Wasm container, that needs to happen ahead-of-time, like network namespace creation for Docker containers. The actual work happens in the endpoints, which are parameterized by the container id. \inl{/init} is called once by OpenWhisk per container, to initialize the container with the module's code. Although different for each runtime; in general, this endpoint will do any work that can be frontloaded and which takes a significant amount of time. \inl{/run} is invoked potentially many times, so any work that is not frontloaded would multiply there. Of course, \inl{/init} is the potential culprit for the cold start latency, so our focus is on reducing even the amount work this endpoint needs to handle. We describe the optimizations to that end, later in this chapter. Once OpenWhisk decides to destroy the container, it does so through \inl{/destroy}.
Finally, we give \inl{tide} a function pointer for each endpoint and listen on a local address.

\subsection{Fast cold starts}
In serverless platforms there are roughly three important steps, in order for a user's code to be executed. Uploading the code to the platform, initializing the execution environment (cold start), and running it. The two latter steps are already part of the execution path; they occurr when the user already wants to run the code. Thus, at the cold start, the action should already be well-optimized for execution and as little preperation ought to happen. WebAssembly is binary target format, however not one, that is ready for \emph{fast} execution. It can be interpreted right away -- thus a small cold start time -- but exeuction performance will massively lack behind compared to native code, which is unacceptable for serverless. So compilation to native code is essential for good runtime performance, but it also takes time for that to be applied to a Wasm module. Given this trade-off, it would seem that Just-in-Time compilation should be the best model. In \inl{wasmtime}, our JIT-runtime of choice, compilation of a simple, WASI-enabled module (1,6 MB) still takes 35 milliseconds (or 40 milliseconds when optimizing the Wasm for speed) on our test machine. This time represents 98\% of the entire setup time, including every other part of the runtime. Thus, unsuprisingly, the cold start is defined almost entirely by the compilation phase. For comparison, the cold start time of the \inl{hello-world} Docker image on the same machine is 452 milliseconds. This number is roughly in line with the cold start times of AWS Lambda or Google Cloud Function, that we've cited in chapter \ref{chapter:background}. While the JIT cold start is an order of magnitude faster than the startup times of Docker containers, it is arguably still on the same order of magnitude as the execution time of some actions. An improvement of another order of magnitude would likely be an acceptable cold start latency.
Furthermore, since the compilation phase takes up such a big chunk of the overall time, it is the most obvious starting point for optimizations.

In contrast to browsers, which receive JavaScript or WebAssembly just-in-time, and thus need to compile it in the same manner, a serverless platform takes ownership of a module earlier. When a user uploads a module, the platform has an almost unconstrained amount of time to apply optimizations to the module, and make it ready for a fast startup \emph{and} execution; solving the previously described dilemma. The key idea is to run the expensive module creation ahead of time, where time means the time of execution. It doesn't matter in this case, whether the actual execution model of the underlying runtime is JIT or AoT. It's obvious for AoT - the code needs to be compiled ahead of time in its entirety, by definition. As we've seen, JIT also has a cold start penalty and we can complete that step ahead of time, too. We refer to this as precompilation. Once the module is precompiled, we can serialize it and store it in the database, instead of the Wasm module itself. At execution time, we deserialize it and can immediately start execution.

Both \inl{wasmer} and \inl{wamr} support AoT compilation. \inl{wasmer} in particular, allows us to leverage \inl{LLVM} for highly optimized native code to produce a shared object. The runtime loads this via a fast \inl{dlopen} call. \inl{wamr} produces its own AoT format which it can then deserialize at runtime and achieve a fast startup. \inl{wasmtime} doesn't support AoT in the traditional sense, but we can still execute the expensive module creation ahead-of-time, and serialize and store the result. Thus, all the runtimes support some form of precompilation.

\subsection{Warm starts}

Even though the cold start of our executor is well-optimized now, cold starting for every single invocation would still be too expensive for sequential invocations. Furthermore, in order to implement and adhere to OpenWhisk's design, we already need to store the module between \inl{init} and \inl{run}. Thus, we can take advantage of that design and keep the module around for some threshold time, before it is evicted. This is how OpenWhisk works with Docker containers today. After a cold start, OpenWhisk pauses the container and unpauses it if a matching request comes in, in which case the action invocation experiences a warm start. After the threshold -- which defaults to 10 minutes -- OpenWhisk calls the \inl{destroy} function on the container, removing it entirely.

One inefficiency of our design is that our executor store modules per container, rather than just once. If OpenWhisk decides to create two containers \inl{A} and \inl{B}, both containing the same module \inl{M}, then \inl{M} will take up storage twice. One reason for this design is the eviction process. Internally, the executor uses a thread-safe HashMap to store the modules and let many tasks or threads read from it simultaneously. A store-modules-once approach would require an atomic reference counting mechanism and additional locking to act on it, i.e. testing whether the reference count is 0 is atomic, but checking \emph{and} removing it is a critical section. Instead of introducing an additional lock per module and the additional synchronization overhead that comes with it, we trade memory for speed and store the module per container. Thus, removing a container is a trivial and fast remove operation on the HashMap. This design is also less of an issue in light of OpenWhisk's per-action concurrency limit. This limit states how many concurrent invocations a container should handle at most. If we increase this value beyond its default of one, then OpenWhisk will send invocation requests for the same action to the same container, up to that limit, without duplicating the module in the executor's memory. In particular, figure \ref{fig:openwhisk-concurrency-limit-greater-one} shows how a module can be reused in the same container -- skipping the cold start entirely. Making use of this limit can be very important for performance. As we've seen in chapter \ref{chapter:background}, an analysis of open-source serverless applications showed 84\% of them having bursty workloads \cite{Eismann2021}. Thus, the likelihood of a serverlesss framework receiving concurrent requests and in turn having to create multiple containers is very likely. With a concurrency limit $c$, appropriate for the underlying hardware and workload, up to $c$ requests could be handled by a single container.


\begin{figure}
    \includegraphics{figures/OpenWhiskConcurrencyLimitGreaterOne.pdf}
    \caption{An example of a container's life cycle when OpenWhisk's concurrency limit is set to a value greater one for that action. Note that a container is only represented in-memory by its module and instances, which is why there is no explicit \quot{container} label in the figure. The container is initialized with a module by OpenWhisk's call to \inl{/init}. Every \inl{/run} request to the same container id spawns a new instance of a WebAssembly module. In particular, two concurrent requests to \inl{/run} can be executed concurrently by the container where the instances are instantiated from the same module. How many requests can be handled in this manner is determined by OpenWhisk's concurrency limit. The instances are the actual \quot{containers} here, since they are the ones who are isolated from each other as well as the host. They are short-lived and never reused to guarntee that isolation, i.e. there is a one-to-one relationship between \inl{/run} requests and instances. The Wasm module will stay in memory until OpenWhisk's explicit call to \inl{/destroy}, which occurs after a container has been idle for some threshold time.}
    \label{fig:openwhisk-concurrency-limit-greater-one}
\end{figure}


The result of keeping modules in the executor's memory, is that they are already deserialized and a new instance can be instantiated from it immediately. An instance is the runtime's representation of a callable Wasm module. If a module is the equivalent of a Docker image, then the instance is analogous to the running container. Note that, unlike Docker-based OpenWhisk, we do not keep instances in-memory (containers), but only the modules (images). The cost of instantiating an instance is negligible. On our test machine it took, on average, 340 microseconds to setup a \inl{wasmtime} instance and the parts needed for execution. This represents the overhead on every warm invocation. Moreover, two instances can execute concurrently, even if the module itself is not thread-safe, for instance when reentrant execution would be unsafe. Finally, implemented this way, it would also be safe to execute two requests from different users, since instances are isolated from each other. They operate on their own memory space and have their own WASI environment. Thus, caching modules rather than instances is a good trade-off.

\subsection{The WasmRuntime abstraction}

\begin{listing}[ht]
    \begin{minted}{rust}
pub trait WasmRuntime: Clone {
    fn initialize(
        &self,
        container_id: String,
        capabilities: ActionCapabilities,
        module: Vec<u8>,
    ) -> anyhow::Result<()>;

    fn run(
        &self,
        container_id: &str,
        parameters: serde_json::Value,
    ) -> Result<Result<serde_json::Value, serde_json::Value>, anyhow::Error>;

    fn destroy(&self, container_id: &str);
}
\end{minted}
    \caption{The \inl{WasmRuntime} abstraction that each Wasm runtimes implements.}
    \label{listing:wasm-executor-trait}
\end{listing}


The Wasm executor is generic over the underlying WebAssembly runtime, such that we can easily implement different ones and compare them. To that end, we abstract the common runtime tasks into a trait -- essentially an interface in Rust's terminology. It is shown in listing \ref{listing:wasm-executor-trait}. This trait is used as the state in our executor. Thus, each runtime can have a different state internally, while being able to share it across requests. Since all our runtime implementations follow this interface, they work in similar ways. We describe the final design for each function, while pointing out the important differences between runtimes. Afterwards, we discuss alternatives.

\subsubsection{Initialization}
The executor decodes the base64 string given by OpenWhisk and unzips it, before calling \inl{initialize} with the result. The function initializes the container identified by the given id. All runtimes store the code they're given in a thread-safe HashMap -- the \inl{dashmap} \footnote{\url{https://crates.io/crates/dashmap}} library in our case.

\begin{description}[style=multiline, leftmargin=2.5cm, font=\bfseries]

    \item[Wasmtime] In \inl{wasmtime}, we instantiate a single \inl{Engine} when the executor starts up, which is then used by all \inl{Module}s for compilation. The bytes passed through \inl{initialize} are deserialized into a \inl{Module}, which is a fast operation, and then stored in the internal HashMap. This is possible because a \inl{Module} is thread-safe so the containing HashMap can be safely shared across threads.

    \item[Wasmer] In \inl{wasmer} we store both an \inl{Engine} and a \inl{Store} globally, since a \inl{Module} requires both for instantiation. We will see whether and how that affects memory consumption in chapter \ref{chapter:evaluation}. Like in \inl{wasmtime}, the \inl{Module} is thread-safe.

    \item[Wamr] \inl{wamr} is originally written in C, and we use \inl{wamr\_sys} \footnote{\url{https://crates.io/crates/wamr\_sys}} bindings to embed it in Rust. Contrary to the other runtimes, a module in \inl{wamr} is not thread-safe. Thus, only the serialized bytes representing the module are stored in the HashMap. It follows that the deserialization happens in every \inl{run} call instead, which is slightly more expensive.

\end{description}

Both \inl{wasmtime} and \inl{wasmer} underline the importance of the validity of the bytes given to the respective \inl{deserialize} methods. Unlike a Wasm module, the compilation artifacts can not be validated in the same way since they already represent an initialized module. So they have to be trusted, when the corresponding Wasm module would not have to be trusted. In our prototype, the users actually supply the compilation artifacts themselves, for development convenience reasons. For an actual serverless framework making use of Wasm, it would have to take ownership of the Wasm module and do the precompilation itself, in order to trust the code.

\subsubsection{Execution}

All runtimes implemented support the WebAssembly System Interface (WASI). Recall that WASI follows a capability-based security model, which allows for fine-grained control over what the module has access to. Hence, building a WASI context means setting up those capabilities, like forwarding \inl{stdin} or receiving from \inl{stdout}, setting environment variables or \inl{argv}. By default, the module also has access to WASI APIs like \inl{random\_get} for high-quality randomness or \inl{clock\_time\_get} to attain values from various clocks. WASI also controls on a per file descriptor basis, what directories the module has access to. Specifically, the modules needs a preopened file descriptor in order to access files. The Wasm runtimes therefore open the files and pass them to the module. However, that information is not present in OpenWhisk since Docker doesn't require it, so we need a way for users to specify these capabilities.

OpenWhisk actions can have optional annotations attached to them, which are simple key-value pairs. We can use those to let users specify capabilities. For instance:

\begin{minted}{sh}
wsk action create cache cache.wasm --annotation dir "/tmp/cache"
\end{minted}

Here a new action named \inl{cache} is created from a file \inl{cache.wasm} with the \inl{wsk} cli tool. A key-value pair is attached with \inl{dir=/tmp/cache}. With some minimal code changes to OpenWhisk's container abstraction, these annotations are then passed to the executor during \inl{initialize}. On the executor side, it is passed to each runtime which can then act on that information. It would be easily possible for a service provider to implement policies on top of it. Access could only be granted to the \inl{/tmp} directory in general; or a default cache directory \inl{/tmp/<container\_id>} could be created for each container, where information can be cached but only \emph{that} container has access to it. As WASI is currently under heavy development and doesn't, for example, specify any network-related interfaces, the amount of capabilities is limited. Our prototype implements directory access and our self-provided HTTP GET request. The latter is relevant to implement a network I/O-bound function for our evaluation in chapter \ref{chapter:evaluation}.


\begin{listing}[ht]
    \begin{minted}{rust}
ow_wasm_action::pass_json!(handler);

pub fn handler(json: serde_json::Value) -> Result<serde_json::Value, anyhow::Error> {
    let param1 = json
        .get("param1")
        .ok_or_else(|| anyhow::anyhow!("Expected param1 to be present"))?
        .as_i64()
        .ok_or_else(|| anyhow::anyhow!("Expected param1 to be an i64"))?;

    let param2 = json
        .get("param2")
        .ok_or_else(|| anyhow::anyhow!("Expected param2 to be present"))?
        .as_i64()
        .ok_or_else(|| anyhow::anyhow!("Expected param2 to be an i64"))?;

    Ok(serde_json::json!({ "result": param1 + param2 }))
}
\end{minted}
    \caption{A simple add action in Rust that uses a macro from \inl{ow-wasm-action} to pass the received json to the \inl{handler} and return the \inl{Result} back to the runtime.}
    \label{listing:add-action-example}
\end{listing}

\inl{run} executes the module associated with the given container id and with the given parameters as input. As required by OpenWhisk, the parameter is a JSON object. The WebAssembly minimum-viable product (MVP), only supports integer data types natively. Higher-level data types often differ in their implementation details between languages, so a universal target format like Wasm cannot decide for one of them, without shutting certain languages out or making it harder for them to be used. Making Wasm modules compiled from different source languages interoperable is an ongoing process -- with WebAssembly interface types\footnote{\url{https://hacks.mozilla.org/2019/08/webassembly-interface-types/}}. The \inl{wasm-bindgen} Rust library already makes this process easy for JavaScript-WebAssembly interoperability. Unfortunately, it does not work with any of our runtimes. Even if both our runtime and the module is written in Rust originally, passing a pointer is not possible because WebAssembly is sandboxed and could not access the pointee. To pass a complex type like a JSON object to the module, we require that it uses a wrapper. Internally, the wrapper allocates a global buffer of bytes. It exports three functions to work with this buffer, most notably one that returns a pointer. When executing the module, the runtime serializes the JSON object as bytes and allocates enough space in the buffer to store them. It then acquires the pointer, which is relative to the module's memory's base pointer, offsets it appropriately and writes the JSON bytes into the buffer. Since the module doesn't know the number of bytes written, the runtime passes that as its argument. On the other side, the module reads the given number of bytes from the buffer and calls the user-provided handler function. Reading the return value of the action uses the same process. Conveniently, the user only has to implement a function that takes a JSON object and returns one in the success case; an error otherwise. Thus, this crude method of passing parameters can be completely hidden from action developers. A simple action can be seen in listing \ref{listing:add-action-example}. The action development process is visualized in figure \ref{fig:action-creation-flow}, which makes use of the \inl{binaryen}\footnote{\url{https://github.com/WebAssembly/binaryen}} toolchain to optimize the Rust-produced WebAssembly.
The mentioned \inl{ow-wasmtime-precompiler} is a crude wrapper around \inl{wasmtime}'s API that creates a module from Wasm bytes, which kicks off the internal compilation process. Once the module is created, it is ready for execution. At this point, we serialize it to a file and zip it. This process compiles the module for the current host architecture. \inl{wamrc} and \inl{wasmer compile} can also take a target argument to precompile for a different host, e.g. a Raspberry Pi with the \inl{aarch64} ISA. However, as of version \inl{0.26}, the \inl{wasmtime} cli has also gained similar capabilities.
% One point of interest is the memory allcator. Because Wasm's original use case is in the web browser, where the binary is downloaded just-in-time, the binary size matters. Thus, a smaller memory allocator was written for Rust, named \inl{wee_alloc}\footnote{\url{https://github.com/rustwasm/wee_alloc}}. It effectively trades performance for a smaller size. Since we precompile our WebAssembly the size difference is negligible. If we were not precompiling, this might be a more interesting feature to activate.

\begin{figure}
    \includegraphics{figures/ActionCreationFlow.pdf}
    \caption{There are two phases in the action development. Our \inl{ow-wasm-action} library provides the abstraction to read the parameters the runtime has passed and to write the return value. To that end, it uses \inl{serde\_json}, a (de)serialization library to read and write the JSON objects. Afterwards, \inl{wasm-opt} runs an optimization pass on the produced WebAssembly, since not all runtimes run an extensive optimization pass.
    Depending on what runtime is compiled into the executor, the corresponding precompiler needs to be used. For \inl{wasmtime}, we have written the small \inl{ow-wasmtime-precompiler} program that precompiles the Wasm code into a format ready for execution, while \inl{wamrc} and \inl{wasmer compile} can be used for the others. Finally, the resulting precompiled binary is zipped which reduces the transmission size and instructs OpenWhisk cli to treat it as binary. The zip file is then ready to be uploaded to OpenWhisk.}
    \label{fig:action-creation-flow}
\end{figure}

The creation of the Wasi context as well as the actual execution is slightly different for each runtime.

\begin{description}[style=multiline, leftmargin=2.5cm, font=\bfseries]

    \item[Wasmtime] The \inl{wasmtime} runtime, as of version \inl{0.23}, uses \inl{cap-std}, a capability based implementation with a similar API as the Rust standard libary. We can construct its \inl{Dir} type and use it while building a \inl{WasiCtxBuilder}, ultimately yielding a \inl{WasiCtx}. The context is \emph{linked} with the module using the runtime's \inl{Linker}, which is similar to an operating system's dynamic linker. It resolves the imports that the module needs and instantiates it with them.
          To execute, we create a \inl{Store} per request, thus per \inl{run} invocation. This is because a \inl{Store} is neither thread-safe nor meant to be long-lived, according to the documentation. The \inl{Store} is used as a collection of the instances and the host-defined items, according to the documentation.

    \item[Wasmer] In \inl{wasmer} we build up a \inl{WasiEnv} with a very similar API as \inl{wasmtime}'s, including directory access. The module and environment are combined into an \inl{ImportObject} from which an \inl{Instance} is instantiated.

    \item[Wamr] \inl{wamr}'s API is slightly lower-level and less ergonomic compared to the other runtimes due to it being a C-API rather than an idiomatic Rust one.
          Two functions need to be called globally, for the runtime to be instantiated and torn down, \inl{wasm\_runtime\_init} and \inl{wasm\_runtime\_destroy}, respectively.
          Our implementation requires each runtime state to implement \inl{Clone}, a trait indicating that an object can be duplicated. A common pattern in Rust (also used by \inl{wasmtime} and \inl{wasmer}) is to implement the clone operation to produce a shallow copy. Internally, these objects typically use Rust's atomically reference-counted smart pointer (\inl{Arc}) to share object's across threads, safely. This makes it cheap to share the runtimes object across different threads while having access to the same underlying data. Thus, on each request a clone of the runtime is created and destroyed (dropped) at the end. Because of that we cannot call \inl{wasm\_runtime\_destroy} in the runtime's \inl{Drop} implementation. Instead, we use another internal Arc, which is instantiated with an object that calls the initialization function when it is created, and destroys the context when it is being dropped. Because the \inl{Arc} will live as long as the runtime and only calls the contained object's \inl{drop} function when all other references have gone away, this implements our requirements perfectly.

\end{description}

So in summary -- across all runtimes -- \inl{run} looks up the previously stored module with the container id, creates a WASI context with the associated capabilities, passes the parameters and executes the module's default function. If an error occurs during the Wasm runtime setup, the outer \inl{Result} contains an \inl{anyhow::Error}, a generic, application-level error type. Since the Wasm module itself can also either succeed or fail, another \inl{Result} is nested inside, where both success and failure are represented by JSON objects.

In what task or thread the module is executed, will be discussed shortly.

\subsubsection{Cleanup}
Finally, the \inl{destroy} function removes the container, which simply means freeing the memory taken up by the module. This is fast and thread-safe, since each container manages its own Wasm module. Once OpenWhisk has decided to destroy the container, it will synchronize internally to not call \inl{run} on it again.

An overview of the entire executor workflow can be seen in figure \ref{fig:executor-overview}.

\begin{figure}
    \includegraphics{figures/ExecutorOverview.pdf}
    \caption{An overview of the executor architecture. \inl{init} decodes the received bytes using \inl{base64} and unzips the result. From there they are deserialized back into a \inl{Module} instance, in the case of \inl{wasmtime} and \inl{wasmer}. The module and capabilities are then stored in the HashMap. When a \inl{run} request comes in, the module and capabilities are looked up. The capabilities are transformed to a WasiContext (named differently under different runtimes), which is used to instantiate an instance, together with the module. The parameters for this \inl{run} request are serialized to JSON and written into the instance's memory. When its \inl{main()} function is invoked, the JSON is deserialized and passed to the actual handler function. The result is returned to OpenWhisk. A \inl{destroy} request simply translates to a remove operation on the HashMap. Note that this figure is accurate for \inl{wasmtime} and \inl{wasmer}, but not entirely for \inl{wamr}. The latter runtime's module creation is slightly different, as explained above.}
    \label{fig:executor-overview}
\end{figure}

\subsection{Implementation alternatives}
Instead of caching modules some may wish to cache instances instead. The case for reusing instances is to avoid costly setup in the action itself, like connection pools to external resources or to avoid rebuilding a cache on every request.
We need to manage instances in that case, i.e. we need a pool of instances and a management system around it. When a new request comes in, we would ideally like to reuse an existing instance. That requires synchronizing with other threads, to decide which thread gets to reuse an instance and which needs to create a new one. We would need per-instance synchronization to check whether an instance is currently in use or usable. It is questionable whether the synchronization overhead would be less costly than always creating a new instance, in particular because creating instances is a relatively cheap operation. For this reason we decided against it.

% As seen in figure \ref{fig:executor-overview}, a Wasi context is created per \inl{run} invocation. If a module needs access to a file for instance, creating the context requires requires opening file descriptors. Similarly, with a future network API in Wasi, this might require invoking more system calls. These calls may not be expensive per-se, but as we've seen in this work as a whole, frontloading resource instantiation as much as possible can have net performance benefits. In particular, if it's possible to pre-create the Wasi Context we should do it. In \inl{wasmtime} this is not possible, because the \inl{WasiCtx} is not thread-safe. In \inl{wasmer} on other hand, the \inl{WasiEnv} is thread-safe and can be cheaply cloned to send across threads. With both runtimes, we are creating one context per instance.

% Furthermore, with a new instance per request, we have per request isolation and fully stateless execution -- something that the action developer can rely upon.



\subsection{Enabling concurrency}

The executor consists of a web server that can take requests from the invoker asynchronously and concurrently. Because OpenWhisk doesn't communicate with the Wasm modules directly, but through the executor, the \inl{/start} endpoint simply returns the executors own network address. This endpoint also returns an identifier for the container, so that OpenWhisk can handle its life cycle, i.e. the above-mentioned mechanism of keeping it warm for some time, for a faster subsequent invocation. There is a chance that this mechanism may not be necessary for Wasm modules, as they are much more lightweight in their instantiation. We will come back to this question later in our optimization discussion.


Rust's async model works with \inl{Futures} -- also known as Promises in JavaScript -- which represent values that have not yet been computed. Contrary to \inl{Promises} however, Rust's \inl{Futures} are lazy: They do nothing unless actively polled. This task falls to \inl{async} runtimes, of which \inl{async-std} provides one. In \inl{async-std} the unit of execution is a \inl{Task}, which is similar to a \inl{Thread}, except it is driven to completion by the user space runtime instead of the OS. Many of those tasks are executed on the same thread, and by default, a \inl{Thread} per logical CPU core is spawned. Thus we have $M \cdot N$ tasks in total, where $M$ is the number of tasks per thread and $N$ the number of logical CPU cores.
From that, a second distinction is implied. A \inl{Task} should not do a blocking or long-running operation, like reading from a file via the standard library's \inl{std::fs::read\_to\_string}. Instead, it should use the \inl{async} version of that method, from \inl{async-std}, which does not block the thread. Otherwise, all other $M - 1$ tasks, which are executed on the same thread, will also be blocked, even though they may be able to make progress. Note that, we could still process $N$ requests concurrently, even if tasks block, but that doesn't scale well enough. This is why these kinds of async functions are also called cooperative routines (coroutines). When they need to wait for the OS to finish a task, they \emph{yield} to another routine, which can then continue actual work.

That becomes relevant for us, when we execute WebAssembly modules. Taking the module, setting up the Wasm runtime and executing it, is a long-running computation -- and one that could block as well. This would in turn block the thread we are currently running on, which means, only $N$ Wasm modules can be executed concurrently. This is not an issue for computationally-intensive workloads, where no more work can be done concurrently, than the CPU has logical cores. However, for I/O-bound workloads, such as requests over the network, this can quickly become a bottleneck. Since one of the appealing offerings of serverless frameworks is their elasticity, we clearly need a way to handle many more requests, even if they block.
To let other coroutines on the same thread make progress, while one executes the module, we can push the entire execution onto a thread pool. The coroutine then has the simple job to asynchronously wait for that execution to finish and collect the result. That allows the other coroutines to process more requests in the meantime.

To test the theory, whether the thread pool model is able to handle more requests, we run two short experiments.

\begin{enumerate}
    \item Compute-bound: Calculating a large prime number.
          % \item Filesystem-I/O-bound: Reading a 30 MB file from disk.
    \item Network-I/O-bound: Making a long-running (1 second) HTTP request.
\end{enumerate}

Our test machine has 8 logical CPU cores, resulting in 8 threads being started for the async-std runtime. We thus send 16 concurrent requests to be able to see the potential blocking in the result. The first experiment showed no difference in the total execution time between the default and thread pool model. As we would expect, in the default model the compute-bound task finishes the first 8 requests, before starting the second set. The execution in a thread pool executes all 16 requests concurrently, but each took roughly twice as long, resulting in no significant difference between the total execution time. The default model operates more along the first-come, first-serve principle, where an early request will also get a response more quickly. In this simplified example, where all workloads are compute-bound, this may be preferable.

\begin{figure}
    \begin{center}
        \input{figures/async_std_default_thread_vs_thread_pool.pgf}
    \end{center}
    \caption{Executing blocking Wasm workloads under the \inl{async-std} default model with 8 threads, compared with running each in a thread from a pool.}
    \label{fig:default_vs_thread_pool}
\end{figure}

The result of the second experiment is shown in figure \ref{fig:default_vs_thread_pool}. In the default model, the first 8 requests block the other 8, while they wait for the HTTP request to finish, resulting in roughly 2 seconds of total execution time. Unsurprisingly, the thread pool model finishes in half the total execution time, since it can handle all 16 requests simultaneously, due to using a new thread from the pool for each blocking operation.

Both models seem to have a workload better suited to them. However, in a real-world, mixed-workload scenario, a long-running, compute-bound function may prevent another I/O-bound one from initializing its computationally cheap request. A mixed scenario may therefore also be better served by the thread pool model.

\begin{figure}
    \begin{center}
        \input{figures/async_std_default_thread_vs_thread_pool_mixed.pgf}
    \end{center}
    \caption{The average request duration and the average amount of requests finished per second under the default and thread pool execution model, sampled over 50 runs.}
    \label{fig:default_vs_thread_pool_mixed}
\end{figure}

In figure \ref{fig:default_vs_thread_pool_mixed} a mixed scenario is run, where 16 compute-bound and 16 I/O-bound requests are executed simultaneously. In the default model, the compute-bound requests finish in about one third of the time they take under the thread pool model. This is again due to more requests being processed concurrently in the latter model. This is also why the standard deviation is slightly higher in that model. For I/O-bound workloads, there is a slight advantage for the default model. However, in the context of the plot on the right side, the thread pool model is able to handle more requests per second in general, but specifically excels at the I/O-bound ones. There is an inherent trade-off between serving a request as soon as possible, and the time it takes to finish serving it. It depends on the expected workload, which model should be used.
