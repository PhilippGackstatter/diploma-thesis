\chapter{Conclusion}
\label{chapter:conclusion}

In this work, we implemented a WebAssembly container runtime for Apache OpenWhisk. The main goal was to examine the performance differences between the WebAssembly and Docker container runtimes in terms of cold start latency and execution performance. In this chapter we conclude the work by answering the research questions posed in Chapter \ref{chapter:introduction}.

\section{Research Questions}

\subsubsection*{RQ1: By how much can WebAssembly runtimes alleviate the cold start latency?}

Our WebAssembly executors can reduce the cold start latency, on average, by more than 99.5\% on a Raspberry Pi compared to OpenWhisk using Docker. This is in part due to Docker reaching the memory limits of the Pi and swapping. However, even on our server machine, where enough memory is available, the cold start time can be reduced by around 94\%, on average. The executors behave more consistently in the cold start than the Docker container runtime. Among the Wasm runtimes, \inl{wasmer} takes more time for the cold start than the others, being between 47\% and 61\% slower. The \inl{wamr} and \inl{wasmtime} runtime behave very similar with \inl{wamr} taking the lead, due to its computationally cheap initialization procedure.

When only accounting for the overhead introduced by the executors themselves, \inl{wasmtime} introduces less than 10 ms, and \inl{wasmer} less than 20 ms on the server machine. In comparison with Docker startup times, which are always far north of 100 ms \cite{Wang2018, Manner2018}, this is an order of magnitude lower.
On the Raspberry Pi, the WebAssembly runtimes bring the cold start overhead into the range of 112 ms to 274 ms, which is on the same order of magnitude as Docker on a server machine.
Furthermore, we have seen in our tests -- what is also confirmed by other research \cite{Mohan2019} -- that Docker startup times rise under increasing concurrency, while the Wasm executors behave consistently across the number of concurrent requests. Since bursty workloads are especially prevalent in serverless, this is an equally important property. With these results, we expect Wasm executors to perform similarly outside of the OpenWhisk context.

\subsubsection*{RQ2: How do the performance characteristics of the WebAssembly and Docker container runtimes differ?}

In the mixed and realistic workload we can best see the holistic picture because it includes cold starts. Here, the WebAssembly executors are always faster than Docker. In the mixed workload, the WebAssembly executors are between 1.6 and 3$\times$ faster on the server, and 2.4 to 4.2$\times$ faster on the Pi. In the realistic workload, we find Docker's overall performance to be strongly linked to the cold start time. The WebAssembly executors, however, are not noticeably susceptible to cold starts, showing much more consistent behavior. In the realistic workload, \inl{wasmer} takes the lead and processes around 95\% of requests strictly faster than all other container runtimes. For sufficiently long requests, either CPU- or I/O-bound, the cold start time of \inl{wasmer} does not significantly affect its performance, even though it is twice that of \inl{wasmtime} or \inl{wamr}.
% In particular for low number of concurrent requests, Docker scales very poorly compared to Wasm executors, which may be inherent to OpenWhisk as indicated by \cite{??}

For I/O-bound workloads, the used Docker image shows a higher overhead even when the execution time itself is fixed. It confirms that Docker has a higher inherent management cost due to implementing OS-level virtualization, rather than in user space.
The Wasm executors scale linearly for I/O-bound workloads, up to the limits imposed by OpenWhisk and later the underlying operating system, due to their threading model. Once Wasm runtimes allow handling I/O asynchronously -- which is starting to appear -- we expect this performance to be even better while consuming fewer resources.

On the other side, Docker handles pure CPU-bound workloads very well. It proves once more that Docker containers can be very performant -- once they are running. The issue is getting to that point.
In terms of pure CPU utilization, \inl{wasmer} managed 88\% of the throughput compared to the native binary in a Docker container, with the other runtimes falling behind more quickly. Our AssemblyScript experiment showed, however, that the prevalent \inl{node.js} runtime would perform much worse under such workloads, indicating that they are either not very common in today's FaaS or that this level of performance is good enough. Since WebAssembly shows to be several factors faster, this is an overall improvement.

\subsubsection*{RQ3: What are the resource costs for keeping containers warm in both container runtimes?}

The cost for keeping containers warm is relatively consistent for Docker independent of module size, for the image we tested. The in-memory size of WebAssembly containers is primarily determined by the module they hold. The \inl{wasmer} runtime shows the strongest such effect closely followed by \inl{wasmtime}. Due to not keeping modules in memory between requests, \inl{wamr} shows very little memory usage and exemplifies the use case when memory would be very constrained and a priority over speed. WebAssembly modules can be optimized to contain only the necessary code with LTO compilation in Rust or with AssemblyScript by default, in which case the module is in the lower kilobytes range.
Overall, for these low-sized WebAssembly modules, the Wasm executors can be expected to consume less memory than the Docker image we tested and thus hold multiple containers in the same amount of memory.

\subsubsection*{RQ4: Which WebAssembly runtime is the most suitable for use on edge devices?}

% To answer this question, we take into account everything we have learned during both the design and evaluation phase. 
Both \inl{wasmtime} and \inl{wasmer} have an API that allows us to efficiently implement our main requirements for modules: An initialize once, run often lifecycle. Unfortunately, \inl{wamr}'s API is less suitable to that task and it may be part of why it lacks in performance. It has the lowest throughput of Wasm executors in I/O- and CPU-bound workloads, as well as the mixed and realistic workload. The memory usage, on the other hand, is lowest in terms of idle footprint and container size, from a pure numbers perspective. Its API also showed limitations that the others did not have, that is, specifying function imports on a per-module basis is not possible. A certain upside is its support for more instruction set architectures than the others, which may become relevant for heterogeneous edge devices. From a performance perspective however, unless memory is very constrained, it is the least suitable of our selected runtimes.

It then comes down to the other two runtimes. The main advantage of \inl{wasmer} is its ahead-of-time compilation that produces very high quality native code, evident by its performance in CPU-bound workloads. This seems to be coupled with being 47\% slower in cold starts than \inl{wasmtime}, which handles this task particularly well. On the Raspberry 
Pi and in a mixed workload, the margin to \inl{wasmer} is slimmer than on the server.
In our realistic workload, \inl{wasmer} showed that the slower cold start did not saliently affect its throughput compared to \inl{wasmtime}, while Docker did show its performance noticeably linked to its cold start.
Both runtimes support primarily \inl{x86\_64} and \inl{aarch64}, which covers a wide range of devices, but particularly \inl{arm32} support is missing as yet. How relevant that is depends on the planned use of the serverless platforms and whether its worker nodes would run on devices with that or other ISAs.
Both of these runtimes are perform well under various circumstances, but based on our general-purpose mixed and realistic workloads \inl{wasmer} takes the lead, even with a slightly higher memory usage.

% I think we can add another RQ about architectural and systems aspects to integrate a wasm runtime with a serverless framework

% - API
% - ISA
% - mem usage
% - WASI support (wamr non-ideal api)
% - maintainability (Rust v C)
% - async I/O 
% - fine-grained system access (perhaps somewhat comparable to the AWS IAM system)

\subsubsection*{RQ5: How suitable is WebAssembly for serverless execution?}

We find WebAssembly to be a very suitable target format for serverless functions. This comes down to a number of points.

\begin{description}[style=multiline, leftmargin=2.5cm, font=\bfseries]

    \item[Language Support] The high-performance languages C, C++ and Rust already have excellent support for Wasm as a compilation target. Based on a curated list\footnote{\url{https://github.com/appcypher/awesome-wasm-langs}}, many more languages such as AssemblyScript, C\#, Lua, Go and Zig are considered production-ready. Work for Python, Java and Kotlin support is also underway.
    Ultimately, that means developers do not have to learn a new language to write serverless functions, which was previously enabled by packaging languages' runtimes in Docker images. With WebAssembly, serverless operators can polish their support for WebAssembly instead of maintaining multiple Docker images with different language runtimes in different versions and for different ISAs. This reduction in complexity also reduces the attack surface.

    \item[Performance \& Cold Start] The performance of WebAssembly primarily depends on the compilation strategy used by the runtime. We have seen up to 88\% of the CPU-bound performance of Docker and better performance in I/O-bound workloads. The latter will improve even more once Rust runtimes are able to execute the WASI I/O functions asynchronously. With AssemblyScript, we have seen that the same program code will run faster when compiled to WebAssembly than in a \inl{node.js} runtime, making this target even more attractive to TypeScript developers. The lightweight isolation of WebAssembly means a fast cold start without sacrificing security. The APIs of our Rust runtimes proved powerful enough to implement a fast startup procedure with precompilation. We essentially use WebAssembly as a universal intermediate representation of a program, that can be compiled to a more performant version.

    \item[System Access] The WebAssembly system interface allows for secure access to resources on the host. It enables more fine-grained access control than Docker without a significant overhead. We have seen how a Wasm executor can give a module access on a per-function basis and per-file for fileystem access.

\end{description}

The key takeaway is that WebAssembly proves to be a suitable target format due to a performant isolation mechanism with fast startup times.

% "In principle, its scale-to-zeroproperty (i.e., unused containers are deallocated from the platform)suits very well energy-aware IoT use cases with intermittent ap-plications" \cite{Aslanpour2021}

% -> Less cold start latency means less time keeping containers alive (mentioned in background - serverless workload).

% Standardization: Other attempts for speeding-up serverless have been successful, but didn't catch on. We assume this is due to custom-written application frameworks that aren't industry-proven. This is different in our case, since the public-facing interface that serverless developers need to interact with is WebAssembly, whose adoption rate is increasing.

% Two difference actions need two different containers in docker terms. With the Wasm executor, too, but the containers are much lighter weight and can be kept around at less resource costs.

% \citeauthor{Shahrad2020} have found that, on average, 50\% of functions execute for more than 1s, that 80\% of functions execute for more than 100ms, and 9X\% of functions execute for more than 10ms (or similar, perhaps need to do the actual calculations on the data set). Thus our cold start latencies are not on the same order of magnitude as 9X\% of function execution times, but faster.

% What requests per second can a raspberry handle, how much for a EC2 instance (or similar)? Can we scale up horizontally, vertically?

% \subsection{Lessons Learned}

% The lessons learned are two-fold.

% One is that OpenWhisk is unsuitable as a serverless framework for resource-constrained devices.

% In our evaluation we had to make a choice to get a feasible scope. One option was to compare Rust programs compiled to WebAssembly in WebAssembly executors with equivalent JavaScript functions executed in Docker-\inl{node.js} runtimes. to compare a valid Wasm use case with the prevalent language and runtime of today's serverless. Most likely, the results of the I/O-bound workload tests would have been much different. At the same time, OpenWhisk vanilla would not have been able to show the high level of performance only achieved with native code in the CPU-bound load test. Still, the native code makes it less comparable to how serverless functions are usually written at present. To address this last gap, we utilize a recent language effort.

% Different serverless framework needed, more suitable for edge.
% \citeauthor{McGrath2017} provide a comparison where OpenWhisk is performing worst.
% \citeauthor{Mohan2019} independently find that the scaling issue lies not within OpenWhisk but with the container runtime. This confirms that a new approach is called for. Still OW shows worse performance overall than comparable frameworks (McGrath).

% Passing parameters with WASI could be much easier

\section{Open Challenges \& Future Work}

There are a number of open challenges to create a fully production-ready WebAssembly container runtime.

Even though WebAssembly has left the experimental phase and is standardized, so far it is only an MVP; a minimum viable product. Some important features are proposed but not yet implemented. These include interface types, i.e. passing complex types from host to module or module to module, multi-threading and atomics or a garbage collector (GC), which will make it easier for GC'ed languages to be compiled to Wasm.

We also previously mentioned running WASI functions asynchronously, which would allow for non-blocking I/O and thus much better performance for I/O-bound workloads. This will be added to \inl{wasi-cap-std-sync} and will then be usable by WebAssembly runtimes that implement support for it, like \inl{wasmtime}. For other runtimes, which do not build on this library, it is unclear if and when support will land.

Sharing resources fairly among tenants is important for serverless frameworks. While memory can be limited by the host of a Wasm module, limiting CPU shares is not possible. Due to popularity in smart contracts, WebAssembly runtimes have added gas metering, i.e. limiting the number of instructions that a module can execute before it is interrupted. It achieves a limitation of the CPU but the technique is unsuitable for serverless since the typical model is to pay by execution time rather than instructions and it adds book-keeping overheads.

Our executor is stateless without exception, that is, each function is executed in a new \inl{Instance}. Some functions, however, may wish to cache data from an external service in memory to improve latency. A fully stateless system does not allow for such optimizations, while caching \inl{Instance}s rather than \inl{Module}s and invoking them repeatedly would.

At the end of Chapter \ref{chapter:design} we concluded that neither of the threading models we examined was strictly superior. If a priori knowledge exists about the type of workload, then we may wish to run multiple \inl{Invoker}s with different threading models. If the workload type is annotated on the function, the load balancer could dispatch to the most appropriate \inl{Invoker} and improve performance.

While we have seen great leaps in performance on a Raspberry Pi, the cold start times of just the executor are still beyond 100 ms. This is on the same order of magnitude that current FaaS offerings perform on, but as we explored previously, it may not be fast enough for latency-critical applications. Therefore, other techniques like those explored in the related work may need to be implemented to bring the startup latency further down.

% keep-alive
% improve performance on Raspberry Pi further, since 112-274ms on average
% OpenWhisk issues, 4 container limit, ...