\chapter{Conclusion}
\label{chapter:conclusion}

"In principle, its scale-to-zeroproperty (i.e., unused containers are deallocated from the platform)suits very well energy-aware IoT use cases with intermittent ap-plications" \cite{Aslanpour2021}

-> Less cold start latency means less time keeping containers alive (mentioned in background - serverless workload).

Other attempts for speeding-up serverless have been successful, but didn't catch on. We assume this is due to custom-written application frameworks that aren't industry-proven. This is different in our case, since the public-facing interface that serverless developers need to interact with is WebAssembly, whose adoption rate is increasing.

Two difference actions need two different containers in docker terms. With the Wasm executor, too, but the containers are much lighter weight and can be kept around at less resource costs.

\citeauthor{Shahrad2020} have found that, on average, 50\% of functions execute for more than 1s, that 80\% of functions execute for more than 100ms, and 9X\% of functions execute for more than 10ms (or similar, perhaps need to do the actual calculations on the data set). Thus our cold start latencies are not on the same order of magnitude as 9X\% of function execution times, but faster.

What requests per second can a raspberry handle, how much for a EC2 instance (or similar)? Can we scale up horizontally, vertically?

\section{Open challenges}

- multi threading not in Wasm
- limiting CPU time (research if possible)
- limiting memory
- spectre mitigation
- currently instances will be removed every time, i.e. an instance needs to load something from cache every time. A peristent instance could hold it in memory and return it even faster. Not technically impossibled (just a bit more complex), but currently unimplemented.
- »as ​​time ​​spent ​​waiting on ​​network ​​(function ​​executions ​​or ​​otherwise)is ​​wasted ​​by ​​both ​​provider ​and ​customer« \citeauthor{Fox2017} -> Need for asynchronous execution of serverless functions (relationship with AWS Lambda, where node.js functions are asynchronous?)
- In Section 3.4.4, you end the discussion by saying that it depends on the expected workload which model should be used. Is it technically possible to do something to take advantage of potential a priori knowledge about the workload? For example, by annotating functions/modules as I/O or CPU bound, could we have the load balancer dispatch them to different invokers operating with a different concurrency model? If so, things like that could hint to topics for future work and could be discussed somewhere. 
