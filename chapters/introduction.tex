\chapter{Introduction}
\label{chapter:introduction}

\section{Motivation}

% Large number of IoT devices at the edge
% low latency and privacy sensitive use cases cannot be facilitated by the cloud
% some examples of low lat or privacy sensitive use cases
% in theory, serverless is a perfect fit for edge, event-driven, unpredictable
% serverless requires no infrastructure setup from the user side, can be used whenever
% cloud tends to be at centralized locations, far away -> higher latency through physical distance
% cold start issue in serverless introduces significant additional latency
% simply bringing serverless to the edge wouldn't work well because of cold start & resource requirements
% Need a different approach for the serverless edge

% Enter Wasm, containerisation but lightweight
% Take the serverless approach, but replace the commonly used docker with Wasm
% Can accommodate resource-constrained devices better and generally be faster

% Aim of the work
% building this container runtime and evaluating it

With the growing number of Internet of Things (IoT) devices, the limitations of today's cloud computing platforms have become apparent. Low latency and data-intensive use cases cannot be facilitated by the cloud, due to its physical distance and sheer amount of data that all edge devices would have to transmit \cite{Aslanpour2021}.
Hence, application scenarios at the edge involving data analytics \cite{Nastic2017}, machine learning \cite{Rausch2019} or augmented reality \cite{Baresi2019}, require moving away from the cloud-centric model. Serverless computing -- most commonly in the form of Function-as-a-Service (FaaS) -- has emerged as a solution.
In this computing model, developers can execute functions without having to specify how the necessary infrastructure is set up. Instead, the allocation of computing resources is automated, which enables a higher elasticity. This property is well-suited to the resource- and energy-constrained environments of edge devices. Here, overprovisioning is even less desireable than in the cloud model \cite{Aslanpour2021}.
Because resources can be utilized whenever required, this model is ideal for event-driven architectures, such as those typically found in IoT scenarios, where the time at which devices come online or a sensor receives input is unpredictable. Therefore, it may happen that a serverless framework gets only a few requests, but at other times, needs to process a large amount of simultaneous requests.
To enable this elasticity, one user's function might run on the same host as that of another's. To isolate these instances, serverless frameworks make use of OS-level virtualization through containers -- usually of the Docker flavor.
Next to security, it also enables packaging language runtimes, so developers can write functions almost language-agnostic, and statelessness due to cheap create and destroy operations. Although, cheap is relative. Historically, containers have been compared to the heavier-weight virtual machines, where the creation is indeed much more time intensive. However, on the time scale of serverless, where an incoming request ought to be finished as soon as possible but the container needs to be created first, it turns out to be not so cheap after all. Not pre-allocating resources to avoid overprovisioning is the point of serverless, but that presupposes that the allocation itself is timely.

When a function is first executed, its container must be started -- referred to as the cold start. This can introduce a latency of a few hundred milliseconds to seconds \cite{Manner2018, Wang2018}. Consider that that on average, 50\% of serverless functions execute for less than one second \cite{Shahrad2020}. Thus, the latency observed by the end user can often be double of what the function itself is responsible for. This infrastructure-induced latency is unacceptable for latency sensitive applications, and thereby most user-facing ones.
To make matters worse, these cold start latencies get worse under concurrent workloads, precisely those supposed to be well-handled by serverless platforms \cite{Mohan2019}.

Once running, so-called warm containers can be reused for subsequent invocations, making those as fast as the underlying hardware offers or only limited by the language's runtime.

Thus, a new approach should as fast as possible, while also greatly reducing the cold start. Essentially, another technology leap is required, similar to that from virtual machines to containers; from heavyweight to lightweight.

One technology that might facilitate this leap is called WebAssembly.

% The setup of the execution environment may thus contribute a significant part to the end-to-end time it takes to invoke a function for the first time. 

% owever, most of these attempts are at early stages,and  architectural  and  design  assumptions  behind  such  approaches  need  to  bereevaluated, for example, to address the challenges described \cite{Nastic2018}

% Expand what moving away from cloud-centric means, how to do it. Maybe \cite{Aslanpour2021}
% Cold-start issues at serverless offerings of cloud-providers



This is most often facilitated by the use of a container technology, such as Docker. The container enables practical execution of any program, as it contains all its dependencies. Through this mechanism, it also isolates one executing function from another to securely enable multi-tenancy on shared physical hosts. While this works well on the level of Software-as-a-Service, it has problems on the finer-grained Function-as-a-Service (FaaS) level. Once containers are running, they allow execution of programs at native speeds. However, in serverless environments, the need to quickly create and destroy functions arises. When a function is first executed, its container must be started. This can introduce latency in the order of a few hundred milliseconds to seconds \cite{Manner2018, Wang2018} and quickly gets worse under concurrent workloads \cite{Mohan2019}. The setup of the execution environment may thus contribute a significant part to the end-to-end time it takes to invoke a function for the first time. This has been coined the cold-start problem.
% This problem has been partially addressed by keeping containers \quot{warm} in-between requests. However this is opposed to the serverless promise of being able to scale to zero.

To enable low-latency edge computing on resource-constrained devices, serverless is an ideal framework. Single functions are lightweight enough to be deployed on these devices, where entire applications are not. But executing functions in container runtimes on resource-constrained devices will only exacerbate the cold-start problem. Thus, a more scalable and lightweight runtime is needed. One way to achieve this, is to replace the container runtime with one based on WebAssembly (Wasm) such as in \cite{Hall2019} and \cite{Murphy2020}. Wasm is a portable, binary instruction format for memory-safe, sandboxed execution in a virtual machine \cite{W3C2020}. Its portability means that it can be executed wherever an executor exists. Thus, a serverless computing platform only needs to provide a Wasm runtime for the instruction set architectures of its edge devices. Then, a function can be easily moved between architecturally heterogeneous devices. Its sandboxing features enables Wasm to be used for the execution of arbitrary, user-supplied code without compromising the security of a multi-tenant system. Finally, Wasm executors can be created and destroyed in a matter of microseconds. The cloud provider Fastly has recently begun offering execution of Wasm modules on its FaaS platform through the use of their \inl{lucet} executor. On their blog they write:

\begin{quote}
  \quot{Lucet can instantiate WebAssembly modules in under 50 microseconds, with just a few kilobytes of memory overhead. By comparison, Chromiumâ€™s V8 engine takes about 5 milliseconds, and tens of megabytes of memory overhead, to instantiate JavaScript or WebAssembly programs.} \cite{fastly2019}
\end{quote}

Thus, Wasm runtimes may not suffer from a noticeable cold-start.
In summary, Wasm allows for secure, portable and fast execution. With these properties it may be a good replacement for containers on edge computers. One of the founders of Docker even commented:

\begin{quote}
  \quot{If WASM+WASI existed in 2008, we wouldn't have needed to created Docker. That's how important it is. Webassembly on the server is the future of computing.} \cite{Hykes2019}
\end{quote}

% \begin{itemize}
%   \item IoT devices which consume edge resources frequently, are often event-driven. This is a great fit for serverless, as the function can be called exactly when it is needed, provisioned by the platform, then destroyed
%   \item While Docker introduces almost no overhead for cpu-intensive workloads, the cold-start problem may be exacerbated by the less powerful hardware
% \item In containers, the runtime for an interpreted language such as Python or JavaScript needs to be shipped. If we can compile these languages to Wasm, then no additional runtime needs to be shipped. This is code that doesn't need to be downloaded on every invocation from the database.
% \end{itemize}

% Since the edge computing paradigm is already well-aligned with cloud-based serverless computing, but lacks a solution that works well on limited resources, a solution that works well on edge devices should also be suitable for use in cloud-based environments. Serverless functions should already be as lightweight as possible in both scenarios, so the goals for both are well-aligned.

This promising outlook shall be examined more closely in this work. We will implement a Wasm runtime, using different Wasm executors, in Apache OpenWhisk and compare it to the existing container runtime.

\section{Aim of the Work}

The goal of this work is to evaluate the viability of WebAssembly in serverless computing in the context of edge as well as cloud computing. We asses that by implementing a WebAssembly-based runtime for serverless computing -- leveraging different WebAssembly runtimes underneath -- to answer these research questions.

\begin{enumerate}
  % \item What are the challenges in adding a Wasm runtime to OpenWhisk? 

  % \item What are the benefits and drawbacks of a WebAssembly runtime in OpenWhisk?

  % \item What are the differences in creating a function using WebAssembly rather than containers?

  % \item How suitable are the various WebAssembly runtimes for the use on heterogeneous edge computers?
  % Which of the evaluated WebAssembly runtimes is the most appropriate one for the use on edge computers / in a heterogeneous environment?
  % \begin{itemize}
  %   \item Performance
  %   \item heterogeneous -> Architecture support
  %   \item heterogeneous -> Which one scales from a raspi pi to a powerful cloudlet?
  % \end{itemize}

  \item By how much can the cold-start problem be alleviated under the Wasm runtimes?

  \item How do the performance characteristics of the Wasm and container runtimes differ?

  \item What are the resource costs for keeping functions warm in both approaches?

  \item Which Webassembly runtime is most suitable for the use on heterogeneous edge devices?



        %   \item How do the wasm-based runtimes fare against the existing container solution in OpenWhisk? Can the cold-start problem be alleviated? Are there use-cases where containers are still more appropriate?
        %   \begin{itemize}
        %     \item Performance comparison
        %     \item cold-start?
        %     \item for highly-invoked functions, maybe containers are still better due to native speed. But lucet may be a serious contender here.
        %     \item If something like JavaScript can actually be compiled to wasm and then to native code, it may be a lot faster than running in a node.js environment. So there may even be potential for interpeted languages to gain performance!
        %   \end{itemize}
\end{enumerate}

The first research question aims to examine whether our core hypothesis holds. That is, can serverless runtimes provision WebAssembly-based containers more quickly than traditional Docker-based containers? If so, how much faster, and with what optimizations in place? We will evaluate that by measuring the cold start latency for both technologies under appropriate and realistic workloads.

The second question seeks answers to the performance of both container technologies at runtime. More specifically, at what speed can code be executed and what resources, such as CPU or memory, are required. We will evaluate this under similar circumstances as before.

The third question should help us find out, what the cost of not being able to truly scale-to-zero is.

The fourth question is directed at finding the WebAssembly runtime best suited for the execution of modules on low-energy and resource-constrained edge devices. We will use a single-board computer such as a Raspberry Pi to run our serverless environment on and evaluate the different runtimes from there.

\section{Contributions}

Our contributions are a modification of Apache OpenWhisk, that is able to call our Wasm-based serverless runtime. That runtime will be implemented from scratch using various WebAssembly runtimes as the actual execution engines for Wasm modules.
We will assess the current research space for what constitutes serverless use-cases and typical workloads, in order to make a realistic evaluation of our system. That same evaluation will be run against the original Apache OpenWhisk using Docker containers as part of their execution pipeline. That will result in our final assessment of both sandboxing technologies by answering our posed research questions.

\section{Structure of the Work}

In this chapter we have given a introduction to our research problem and our motivation for solving it. In chapter \ref{chapter:background}, we expand on the topics introduced here to establish a common background for the remainder of the work. In chapter \ref{chapter:design} we discuss approaches for potential solutions to our problem and the challenges involved. We settle on one approach and explore its design space, discussing trade-offs along the way and the differente WebAssembly runtimes we utilize. In chapter \ref{chapter:evaluation} we evaluate \todo{what?}.
In chapter \ref{chapter:conclusion} we summarize the results of our work and give an overview of open research questions.