\chapter{Evaluation}
\label{chapter:evaluation}

In this chapter, we evaluate our WebAssembly executor. First we discuss and define the types of workload we use in our experiments. Then we go into details of our methodology, the experimental setup, which metrics we measure and what hardware we deploy on. All that will lead to answers for our research question.

\section{Methodology}

\subsection{Goals}

Recall our research questions.

\begin{enumerate}
    \item By how much can WebAssembly runtimes alleviate the cold start latency?
  
    \item How do the performance characteristics of the WebAssembly and Docker container runtimes differ?
  
    \item What are the resource costs for keeping functions warm in both approaches?
  
    \item Which WebAssembly runtime is the most suitable for use on edge devices?
\end{enumerate}

% In order to answer these questions, we need to measure 
% These questions guide our evaluation approach.

\subsection{Workload Types}

In Chapter \ref{chapter:background} we described the serverless workload in detail, which guided decisions in our implementation. Specifically, we already used that research to get an idea about what concurrency model may be a better fit for our executor.

This research workload represents the current \emph{cloud} workload and may or may not be transferable for workloads at the edge. Because of that, we also take some examples of visionary use cases into account and infer scenarios, i.e. the order of cold starts and warm starts, as well as workload types, i.e. whether functions are CPU- or I/O-bound. This includes both serverless edge workloads as well as those where latency is critical.

% To measure the performance of the serverless platform in a meaningful way, we reviewed the literature for use cases, where latency is critical; in turn, where the cold start time matters most. From these examples as well as characterizations of the workload types in section \ref{section:serverless_workload}, we infer an evaluation workload.


\begin{description}[style=multiline, leftmargin=2.5cm, font=\bfseries]

    \item[Vital Signs Analysis] \citeauthor{Nastic2017} introduce measuring a patient's vital signs with wearable sensors during a situation of crisis \cite{Nastic2017}. Since lives are at stake, decisions must be made quickly. The scenario is both a latency-critical and an edge computing application. It serves as an example, where a serverless platform on edge computers can provide lower latency than sending the data to the cloud. Thus, part of the latency is reduced by moving the computation physically closer to the sensor. However, the second important factor for latency is the platform's execution time. When a sensor comes online and sends its first request, the execution and its cold start, on a platform under load, may take an unacceptable amount of time. Once sensors are online, they need to analyze data in real time and continuously. This is a scenario where high performance and efficient usage of the limited resources is required and one, where the same functions are called repeatedly -- warm starts.
    The workload itself is data analytics, but not described in more detail. We classify this as as CPU-bound, because we can assume it involves data preprocessing, statistical analysis and perhaps even training a machine learning model.

    \item[Edge AI] \citeauthor{Rausch2019} describe a motivational use cases for AI at the serverless edge. A personal bio sensor measures blood sugar levels and sends it to an edge device, which refines a pre-trained model with that data and serves the model for inferencing \cite{Rausch2019}. The workload for this type of serverless function needs a significant amount of CPU-time for the machine learning tasks but also writes and reads parts of the model to and from disk. This workload cannot be unambiguously classified as either CPU- or file-I/O-bound, as it is rather both at once. Considering that measuring blood sugar levels is sufficient in greater intervals, e.g. 10 minutes, or whenever the user does a manual measurement, each of those would spawn a function that does the analysis and training, but could be discarded once it is finished. That is, keeping the function warm would be unnecessary and wasting resources. If the user actively took the measurement and waits for the analysis results, a low latency is required for a good quality of experience. Thus, the cold start of the analysis function should be as short as possible.
    
    \item[Augmented Reality] \citeauthor{Huang2012} implemented a mobile augmented reality (AR) system that analyzes images of book spines locally, e.g. on a smartphone, and then sends the features to the cloud to find a match in a database \cite{Huang2012}. \citeauthor{Baresi2019} propose to offload the image recognition task to a more powerful edge device, to save on battery on the hosting device \cite{Baresi2019}. AR applications require a low latency to provide a good user experience. Offloading such recognition tasks to nearby and more powerful edge devices, presupposes their capability to execute them at high speed. This is a scenario where a serverless platform receives a high number of concurrent CPU-bound requests.

    \item[APIs] According to \citeauthor{Eismann2021a} -- who conducted a manual analysis of serverless applications -- 28\% of their analyzed applications were APIs \cite{Eismann2021a}. Even with today's cold start latency, developers are already using serverless for user-facing applications, where latency is rather important. However, as \citeauthor{Leitner2019} noted in their interviews with practitioners, many voiced concerns about the high latency associated with cold starts \cite{Leitner2019}. If the latency could be reduced significantly, even the APIs of latency-sensitive applications may be build on serverless. A useful API is typically accessing or modifying state. Since serverless functions are stateless, they need to communicate with other services such as databases, event buses, logging services or they might even call other functions. This composition requires network I/O, which should not block other functions from making progress.
    % since the function spends most of its time waiting for the call to the external resource to complete.

\end{description}

We infer two workloads from these, while also taking into account our previous research. Both CPU- and network I/O-bound workloads are a primary type that ought to be handled well by a serverless platform. Some limitations apply due to WASI's immaturity or serverless idiosyncrasies. In particular, to model a full machine learning example, we should access an already trained model and run predictions on it. However, due to the lack of proper networking in WASI, we cannot retrieve it from an external service. Due to vanilla OpenWhisk using Docker containers, storing the model on disk would require granting each container access to the model's path on the host, which is not officially supported by the platform. However, since file I/O is faster than network I/O, proper support for the latter implies support for the former.

% It is also one less parameter to take into account when reasoning about the results.
We write the test actions in Rust and compile them to \inl{wasm32-wasi} and \inl{aarch64} or \inl{x86\_64} target respectively, depending on whether we run a WebAssembly or the Docker container runtime and which hardware we test on. This ensures that our results aren't distorted by two different implementations in separate languages. In order to execute the native binary in OpenWhisk, we use its blackbox feature. It is implemented by its \inl{dockerskeleton} image, which lets us execute any action that adheres to a JSON-in, JSON-out protocol. We could also use the official Rust support by OpenWhisk, however, that takes a Rust source file and compiles it during the initialization phase -- massively increasing the cold start latency. We think using the blackbox version makes for a fair comparison between both container runtime types, since it measures the startup time of the respective container implementations, making it much more comparable to other serverless platforms. Although interpreted languages, such as JavaScript or Python, make up the overwhelming share of serverless languages, we use native functions to compare the precompiled WebAssembly to this optimum in performance.
OpenWhisk makes its docker runtime images available for the \inl{x86\_64} architecture only, so we need to build this image for \inl{aarch64}\footnote{\url{https://hub.docker.com/r/pgackst/dockerskeleton}}, in order to execute on a Raspberry Pi. The concrete implementations of our evaluation actions are the following programs.

\begin{description}[style=multiline, leftmargin=2.5cm, font=\bfseries]
    \item[CPU-bound] For this workload, we repeatedly hash a bytestring in a loop using the \inl{blake3}\footnote{\url{https://crates.io/crates/blake3}} algorithm. This is a convenient choice since its reference implementation is in Rust. Hashing is CPU-bound and free from system calls, so it is a good candidate for this workload type. We choose the number of iterations such that the completion takes roughly 100ms in a native binary on the respective hardware, in order to have a non-trivial amount of work to be done for each invocation -- more than OpenWhisk needs for its scheduling internally.
    \item[Network I/O-bound] For this workload we could make an HTTP request to a local server. However, due to the lack of networking in WASI, we instead use a simple \inl{sleep} which has the same perceived effect. It always takes exactly 300ms to complete.
\end{description}


% Note that we deliberately do not use no_std or other mechanisms to make the modules small, so as to represent a real-world applicable use-case where Rust's std library would most likely be used

\subsection{Setup}

We want to measure the cold start latency and action execution time across all our evaluations. To that end, we utilize OpenWhisk's activation record, which is a collection of data resulting from each action invocation.  Recall that OpenWhisk either initializes and runs the action, in the case of a cold start, or skips the initialization and runs it in a warm container. Each activation record contains a \inl{waitTime} annotation, which is the time between the arrival of the request and the provisioning of a container. It's important to note, that this applies only to Docker, since a Wasm container does not have to be provisioned in the same way and is instead created during the initialization. However, the \inl{waitTime} does not include that initialization time, which is recorded as a separate annotation: \inl{initTime}. Thus, for a fair comparison between both container runtimes, we define the cold start time as \inl{waitTime + initTime}. Measured this way, the cold start time is simply the time between OpenWhisk receiving the request and the container being ready for execution. Since both times are part of the latency the user perceives, we believe this to be an accurate measurement. It is important to note that this allows for a fair comparison in relative terms, that is, the recorded cold start times are not made up entirely of the container runtimes' startup cost, but also of OpenWhisk internal operations, which may account for a significant portion. This may limit the comparability to cold start measurements from other research.
% TODO: Did we?
To facilitate this, we provide the pure startup time of WebAssembly executors as well.

The record also contains a \inl{duration} field which is the duration of the entire execution, including the potential \inl{initTime}. To measure the pure performance of the container runtimes, we subtract \inl{initTime} from \inl{duration}, if necessary. In figure \ref{fig:evaluation-time-measurement} we show an overview of measurements we calculate.

\begin{figure}
    \begin{center}
        \includegraphics{figures/EvaluationTimeMeasurement.pdf}
    \end{center}
    \caption{The two durations we calculate to measure the system's performance.}
    \label{fig:evaluation-time-measurement}
\end{figure}

% With this setup alone, it is difficult to measure the cold start latency, since end - start would include the execution time. To lift this restriction, we additionally measure the wall clock time inside the action, when it enters its main function. Furthermore -- to maximize precision -- we also measure the timestamp at which the main function exits and ignore the activation end timestamp as a result. That leads us to our setup in figure \ref{fig:evaluation-time-measurement}. Now we can accurately measure the time it takes to get the underlying container technology initialized by taking the difference of the entry and start timestamp.

% start - end includes Wasm module's memory allocation and serialization, could mention that and reference its description in the design chapter
% Possibly cite https://www.sciencedirect.com/science/article/pii/B9780124201583000137?via%3Dihub



There are a two papers measuring serverless performance which describe their approach in more detail. We use those as a basis for our evaluation.

\begin{itemize}
    \item \citeauthor{McGrath2017} use concurrency tests to evaluate a platform's ability to scale \cite{McGrath2017}. The concurrency test starts by issuing a request to the platform and reissues it as soon as it completes. At a fixed interval, it adds another request up to an upper bound. In this manner, the load increases over time until a plateau is reached, at which the number of completed requests per second peaks.
    \item \citeauthor{Hall2019} characterize access patterns to serverless function based on a real-world scenario. They describe three different patterns: Non-concurrent requests from a single client; multiple clients making a single request, possibly concurrent; multiple clients making multiple requests, possibly concurrent \cite{Hall2019}.
\end{itemize}

Based on these, we use two similar tests to measure cold start times and throughput.

\begin{itemize}
    \item To measure cold start times we first configure OpenWhisk's deallocation time as 10 seconds. We can then send requests at intervals exceeding 10 seconds to always trigger cold starts. In order to measure the effect of concurrency on cold starts, we send a $i$ number of concurrent requests, where $i \in \{1,...,N\}$ and $N$ is an upper bound we determine during the tests. After each iteration, we wait for OpenWhisk to destroy all containers before continuing. That results in $i$ concurrent cold starts. This measurement will aid in answering research question 1.

    \item To evaluate the performance of the systems, we will run concurrent requests under a mixed workload consisting of equal amounts of CPU- and I/O-bound actions for all container runtimes. We use the approach by \citeauthor{McGrath2017} in their concurrency test, and pick workload types at random. This is to ensure that we evaluate the system for both types and because it is closest to a real-world scenario. It is also similar to the approach of \citeauthor{Hall2019}, in that we have multiple concurrent requests but multiple clients are replaced by diverse workload types. We run an iteration of this test before starting the measurement, in order to pre-warm containers. This is to test the system in a warm state and measure the performance of WebAssembly containers compared to native binaries in Docker containers. At some points during the test run the system will decide to scale up, if the load crosses a certain threshold, and perform a cold start of another container, which would be included in the test. This would be the most realistic performance for the Vital Signs Analysis example we gave earlier, i.e. continuous load on a function with the occasional cold start. Additionally, we run purely CPU- and I/O-bound workloads in a similar manner to test their performance in isolation. This measurement will help us answer research question 2.

    \item This test aims to evaluate, what the resource costs of actions are, specifically their code size and the amount of memory they consume while being held in-memory, ready for execution. We use the Docker API to measure each container's memory footprint and measure the allocated memory of the WebAssembly executor process to infer the costs for WebAssembly containers. This measurement supports the answer to research question 3.
\end{itemize}

Finally, all measurements and the experience of the design phase will lead us to a conclusion on research question 4.

Our test procedure follows this pattern:

\begin{enumerate}
    \item Compile OpenWhisk standalone version in our modified wasm version (openwhisk-wasm) as well as the latest available version, which is \inl{1.0.0} (openwhisk-vanilla).
    \item Start the OpenWhisk version under test on the test machine.
    \item Use the local \inl{wsk} command line tool to create the test action(s).
    \item From a separate machine, start the test procedure, which will send requests to the test machine.
\end{enumerate}

\subsection{Deployment}

We run our evaluation on an edge device as well as a \inl{x86\_64} desktop machine. The edge device is a Raspberry Pi Model 3B, which has 1 GB of RAM and is running the latest version of the 64-bit Raspberry Pi OS (version \inl{2020-08-20}). We are using the 64-bit version, because the \inl{wasmtime} runtime cannot be compiled for the 32-bit ARM architecture. The desktop has an Intel® Xeon® E3-1231 v3 (3.40 GHz) server-grade CPU with 4 physical cores and 8 logical threads. Mass storage is a Samsung SSD 850 EVO -- which is where Docker images are stored and loaded from -- it has 8 GB of RAM and is running Ubuntu 20.04.2 LTS.

Due to difficulties in deploying OpenWhisk on the Raspberry Pi with Kubernetes and ansible, we use the standalone Java version for evaluation. The standalone version uses OpenWhisk's \emph{lean} components for internal messaging and load balancing. These are components written specifically for deployment on resource-constrained devices. Since we are not benchmarking OpenWhisk itself, but rather the underlying container technology, we do not believe this to be a threat to validity.

\section{Cold Start}

\begin{figure}
    \begin{center}
        \input{figures/pi-cold-start-separate-hash.pgf}
    \end{center}
    \caption{The cold start time with a CPU-bound workload on a Raspberry Pi with both Docker and Wasm-based container runtimes.}
    \label{fig:pi-cold-start-hash}
\end{figure}

During testing, various issues arose that we want to discuss, in order to give context about the deployment process. 
The first version of the test created our CPU-bound test action, called \inl{hash}, and then ran concurrent requests against it. In the result we noticed that for 5 and more concurrent requests, there were only 4 cold starts, independent of the underlying container runtime. It turned out that OpenWhisk would only create 4 containers and let other requests wait until one of the currently executing ones finishes, so it can reuse the container. This is a fair optimization, but it does not let us test how 5 or more cold starts actually affect each other. So, instead of creating just one action, we create $N$ actions with different names but the same code. OpenWhisk will treat these as different actions and execute them in separate containers accordingly. However, it will still only allocate 4 containers at a time, waiting for one of them to complete before removing it and starting the next. Thus, the performance of the container removal operation also becomes important, as it blocks the provisioning of the next. It is implicitly part of the cold start time as we define it, since a container that needs to wait due to a slow remove operation from another container will have a higher \inl{waitTime}.
% This isn't reflected in the cold start latency, but we can look at the \inl{waitTime} annotation of OpenWhisk's activation record instead. It measures how long the activation spent waiting in OpenWhisk's queues. If the removal operation of Wasm runtimes is shorter than dockers, then Wasm action activations should have to spend less time waiting than their docker counterparts. Although, this value would also be affected by longer cold starts, so it can only provide a partial answer to this question.
% While testing the deployment with Wasm runtimes showed no performance issues, the vanilla OpenWhisk deployment was harder to test. 

Vanilla OpenWhisk showed further issues in its default configuration. On every container start it executes a Docker pull operation, even if the image was locally present, which meant checking if the local image was up to date with the remote version, resulting in a higher \inl{waitTime} than necessary. Therefore we turned this behavior off, which improved cold start times.
% The Raspberry Pi we use has 913 MiB of total memory. The OS needs 210 MiB and OpenWhisk uses 185 MiB of memory after startup.

The result of our cold start test can be seen in figure \ref{fig:pi-cold-start-hash}. With two running Docker containers, OpenWhisk starts to approach the limits of the Raspberry Pi's memory. If OpenWhisk decides to start a third container, the system runs out of memory and freezes. Only after increasing the swap memory from the default 100 MiB to 1024 MiB were we able to test up to 6 concurrent cold starts. This is reflected in the figure, with the Docker runtime having orders of magnitude higher cold start latencies than any of the Wasm runtimes. At 5 and 6 concurrent requests, the OpenWhisk log files showed Docker commands timing out and the testing time became unreasonably long -- as reflected in the figure -- which is why we stopped testing at that point. Note that the scale is logarithmic rather than linear, which may be suprising.

Even at 2 concurrent requests, the Wasm executors have a cold start time of around one second, increasing with more requests. \inl{wasmtime} has the most consistent behaviour, followed by \inl{wamr} and \inl{wasmer}. Overall, the runtimes are very similar in their startup performance, especially \inl{wasmtime} and \inl{wamr}, even though they are written in different languages and use different compilation strategies. The Wasm executors have, on average, less than 1\% the cold start time than docker.

% Look at cold start time only for wasm runtimes, then reference the docker cold start time on an x86 host and draw a comparison. I.e. if the runtimes achieve a startup time of ~100ms on an edge device and docker achieves the same on an x86 host, then we can perhaps conclude that the Wasm runtimes are suitable/acceptable as container runtimes for the serverless edge.

\begin{figure}
    \begin{center}
        \input{figures/pc-cold-start-separate-hash.pgf}
    \end{center}
    \caption{The cold start time with a CPU-bound workload on a Desktop machine with both Docker and Wasm-based container runtimes. The docker values range between 869 ms and 2873 ms, while the Wasm values generally range from 47 ms to 180 ms.}
    \label{fig:pc-cold-start-hash}
\end{figure}

For comparison, the cold start times on our \inl{x86\_64} desktop machine are in figure \ref{fig:pc-cold-start-hash}. The relative differences are similar to the Raspberry Pi. In particular, the Wasm executors behave more predictably on both machines. The average improvement in cold start times are 94\% for \inl{wasmtime} and \inl{wamr} and 93\% for \inl{wasmer}.
% The improvements are very consistent across the number of concurrent requests, with a standard deviation of less than 0.01 percentage points for all runtimes.

These comparisons give a good holistic picture of the performance of both runtimes in the OpenWhisk context. However, because the measurements include other OpenWhisk specifics, they are less useful for a general container runtime comparison. Thus, we plotted \inl{initTime} for the Wasm executors in Figure \ref{fig:pc-pi-cold-start-wasm-only}. This time represents the duration of \inl{/init} endpoint on the executor. It provides comparability to serverless platforms other than OpenWhisk since it isolates the overhead introduced \emph{only} by the executor.

Independent of the hardware, \inl{wasmer} is about X\% slower than the other runtimes. At initialization time, the runtime receives the bytes and deserializes them into a \inl{Module}, similar to \inl{wasmtime}. However, since we use the \inl{NativeEngine} in \inl{wasmer}, 
%The highest cold start time \inl{wasmtime} and \inl{wamr}

\begin{figure}
    \begin{center}
        \input{figures/pc-pi-cold-start-wasm-only.pgf}
    \end{center}
    \caption{The cold start time with a CPU-bound workload on the Raspberry Pi only for the Wasm executors and with a confidence interval of 95\%.}
    \label{fig:pc-pi-cold-start-wasm-only}
\end{figure}

% Run experiment: cold start, then warm start -- see relationship between the two, for docker & wasm

% Does OpenWhisk allocate containers corresponding to the number of logical CPUs?
% median vs mean
% how many runs is data based on?
% use a 30MB or sth module to test WAMR impl performance then

\section{Throughput}

\begin{figure}
    \begin{center}
        \input{figures/pi-pc-load-mixed.pgf}
    \end{center}
    \caption{The throughput in requests per second with an equal amount of I/O- and CPU-bound workload on a Raspberry Pi and a Desktop machine with both Docker and Wasm-based container runtimes.}
    \label{fig:pi-pc-load-mixed}
\end{figure}

In the previous cold start test it became apparent that running OpenWhisk vanilla with Docker on the Raspberry Pi is challenging. Because OpenWhisk runs four containers under heavy load, it puts too much strain on our test edge device. Thus, we make use of the previously introduced concurrency limit that we can set for each action. We set the limit to 10, such that 10 actions can be executed concurrently in the same container. With this setup, we can run the load test for the Pi, because OpenWhisk only creates two containers -- at least up to a certain threshold. Log collection is another process that tends to fail often, so we use \inl{--logsize 0} when creating the action, to prevent OpenWhisk from attempting to collect them. We use the same parameters for creating the Wasm actions. With that we get the result in figure \ref{fig:pi-pc-load-mixed}. OpenWhisk vanilla manages to handle up to 10 concurrent requests on the Pi, at which point it creates a third container, which forces the Pi to use its swap memory and become orders of magnitude slower. Hence, at this point we stopped testing. The workload we run is mixed and made up of an equal amount of I/O- and CPU-bound actions. Whether one or the other is executed is determined at random, such that, on average, half will be I/O-bound and half will be CPU-bound. We execute two runs and take the average for each number of concurrent requests. Note that the CPU-bound workload is chosen such that the execution time is similar on the Raspberry and the Desktop machine, since the blocking I/O-bound workload also takes the same amount of time on both devices. That makes the results more comparable. This test is executed with Apache JMeter\footnote{\url{https://jmeter.apache.org/}}, a load testing tool. 

\begin{table}[h!]
    \centering
    \begin{tabular}{c | c | c}
        Runtime        & Raspberry Pi & Desktop\\
        \hline
        \inl{wasmtime} & 3.8x & 2.2x\\
        \inl{wasmer}   & 4.2x & 3.0x\\
        \inl{wamr}     & 2.4x & 1.6x\\
    \end{tabular}
    \caption{Average factor by which the respective WebAssembly executors are faster than Docker in the mixed workload scenario.}
    \label{table:pi-pc-load-mixed-improvements}
\end{table}

We can immediately see a a stronger difference in all container runtimes than in the cold start test.
The factors of improvements are listed in table \ref{table:pi-pc-load-mixed-improvements}, where we can see, that on both devices, the WebAssembly executors are better able to handle a mixed workload. The executors perform similarly on both hardware classes when examining the order of fastest to slowest. However, on the Desktop, \inl{wasmer} has a greater lead on \inl{wasmtime} than on the Pi. Both of these runtimes perform with higher fluctuations than \inl{wamr}, which has a very consistent behaviour.

\inl{wasmer} likely takes the lead here, because we configured it to use the \inl{LLVM} compiler toolchain which produces high-quality native code and can be opened by the runtime with a fast \inl{dlopen} call. On the other hand, \inl{wamrc} also uses \inl{LLVM} for ahead-of-time compilation, but performs worse. One contributing factor may be that the \inl{wamr} API does not allow us to thread-safely initialize the module once and run it often. The module needs to be instantiated from the raw bytes on every \inl{run} call. Because of that, the implementation is less efficient compared to the other executors. However, ... we will run an evaluation for that later -- comparing instantiation times of the Wasm executors.
\inl{wasmtime} is configured to use the \inl{cranelift} JIT compiler. Because of the inherent trade-off such a compiler has to make between compilation speed and code quality, it is perhaps unsuprising that it is less performant than the \inl{LLVM}-based wasmer compiler. Interestingly, it does perform better than \inl{wamr}.

To get a more isolated picture of the container runtimes performance, we test the workloads separately on our Desktop machine. The I/O-bound test is supposed to test the container runtimes' ability to handle concurrent network requests. As we've explored in chapter \ref{chapter:background}, serverless functions are frequently used with external services, such as databases. This test emulates this behaviour by blocking for 300ms, implemented using a simple thread sleep. In the Wasm executors, this is implemented with the capabilities, which allows the Wasm runtimes to supply native host functions as imports to the module. In this instance, we create our action using \inl{--annotation net\_access true} with the \inl{wsk} cli. The Wasm executor then provides the \inl{http.get} function as an import to the module. Each executor implements this with the aforementionend \inl{sleep}. The module can then use these functions as if they were part of the module. This way, we could easily implement an actual HTTP GET request. However, the lack of interface types means the module can only call native functions with integer parameters and return values, such that the practical use is still inhibited.

\begin{figure}
    \begin{center}
        \input{figures/pc-load-block.pgf}
    \end{center}
    \caption{The throughput with an I/O-bound workload on a Desktop machine with both Docker and Wasm-based container runtimes. The y-axis shows the throughput in requests per second, while the x-axis shows the number of concurrent requests, each of which is repeated 50 times to generate a consistent load over a short period of time.}
    \label{fig:pc-load-block}
\end{figure}

\begin{figure}
    \begin{center}
        \input{figures/pc-load-hash.pgf}
    \end{center}
    \caption{The throughput with a CPU-bound workload on a Desktop machine with both Docker and Wasm-based container runtimes. The y-axis shows the throughput in requests per second, while the x-axis shows the number of concurrent requests, each of which is repeated 50 times to generate a consistent load over a short period of time.}
    \label{fig:pc-load-hash}
\end{figure}

Cold starts should not affect this test, so a warmup phase is executed before gathering data, which lets OpenWhisk create the containers and keep them warm. The throughput is plotted in Figure \ref{fig:pc-load-block}. OpenWhisk imposes the same 4 container limit for all container runtimes, which puts a limit on the throughput for I/O-bound workloads. However, again we can use the concurrency limit to increase performance -- here we set it to 3. The \inl{dockerskeleton} we use in OpenWhisk vanilla performs very poorly for I/O-bound workloads. For instance, two concurrent requests take twice the time that one request takes. This may be a limitation of the action proxy used in the \inl{dockerskeleton}, where a blocking requests blocks other requests in the same container. It can only achieve more throughput by starting new containers. Thus, the concurrency limit essentially delays the optimal I/O-bound throughput for the Docker version until OpenWhisk creates 4 containers -- its maximum. This explains the plateauing at regular intervals in the figure. The Wasm executors on the other hand, have no such issue and can essentially scale up to the 4 container limit imposed by OpenWhisk. Since we set the concurrency limit to 3, and $3 \cdot 4 = 12$, we reach the saturation at that point. A higher concurrency limit would consequently increase the possible throughput. Since blocking takes the same amount of time in every Wasm runtime, this performance is mainly determined by the executor itself and its threading model -- something we have explored at the end of Chapter \ref{chapter:design}. Other than the OpenWhisk limitations, the performance rests on the number of threads that can be spawned on the system.

Because the concurrency limit has such a pronounced effect on the performance, we also plotted the same workload with a concurrency limit of one. Given what we just described, it is no surprise that at 4 concurrent requests the system's throughput is saturated. This test therefore shows the direct comparison of container performance, even when the execution time is predetermined. Evidently, Docker has a larger overhead than the Wasm executors, all else being equal.

% Would be interesting to evaluate this in node.js -- which is primed for I/O

In figure \ref{fig:pc-load-hash} is the result of the fully CPU-bound load test. This test shows a large performance gap among the Wasm executors, but also to Docker. On average, the Wasm executors process only 39\% (\inl{wamr}), 57\% (\inl{wasmtime}) and 88\% (\inl{wasmer}) of the throughput that Docker achieves.

Docker was particularly hard to evaluate for this workload. OpenWhisk -- at least in the standalone version -- is hardly able to saturate the CPU, because the system will not create enough containers to fully utilize the CPU.
This likely occurrs because each container is run with the \inl{--cpu-shares} Docker option, limiting the share of the CPU the container is allowed use. Hence, if not enough containers are created, some invocations have to wait for others to complete, even though the CPU isn't fully utilized.
By increasing the concurrency limit, we can work around that and let OpenWhisk execute actions in the same container to fully leverage the assigned CPU shares. Increasing this limit too much, forces more actions to share the same CPU shares, while not creating new containers.
However, increasing the memory on the container, which in turn makes OpenWhisk increase the CPU shares, does not have a positive effect either. Rather, it delays OpenWhisk's decision to create new containers even longer, resulting in even poorer performance. After some trial and error we arrived at values which seemed to leverage the CPU rather well, the default 256 MB of memory and the concurrency limit set to 3.
The Wasm executors showed no such signs, simply because no memory or CPU limiting is implemented in the first place. This will have to be a feature of a fully production-ready Wasm executor, however.

Cold start latencies and resource costs for keeping containers warm are ignored in this test. With that in mind, the test shows that for a steady stream of highly concurrent CPU-bound requests, using Docker is the best option. In particular, if the container is likely to be reused many times. This comes perhaps not unsuprisingly, given that the code executing is Rust compiled to a native \inl{x86\_64} binary. There is hardly a way to generate faster code. In fact, if we compare the pure execution time of our wasm action outside the serverless context, we see the native binary being even faster. Here we precompile our action for wasmer and then measure only the pure function execution time, without the setup of necessary contexts. In that case, wasmer achieves 60\% of the throughput compared to the native binary. Compared to the 88\% from above, it indicates that OpenWhisk is unable to fully utilize the CPU, either because of the discussed configuration or because of internal overhead, like queuing and container management. 

Note that using native code in serverless functions is not a particularly popular approach. Recall the serverless workload types we described in chapter \ref{chapter:background}. We noted that functions that exhaust the CPU performance are most likely more of a niche and, that if the executor can offer performance on the level of \inl{node.js}, it cannot be considered a step back. Even more, if we can report performance gains, it performs better than today's typical serverless function. In light of this, when a fully CPU-bound WebAssembly program reaches 88\% of the performance of a native binary, we can count it as an advancement. 

% To put this into more perspective, we run a experiment that runs a workload in \inl{node.js}, arguably the go-to serverless runtime, and in WebAssembly. To that end, we write a function in AssemblyScript\footnote{\url{https://www.assemblyscript.org}}, a strict variant of TypeScript. We can then compile the same function with a TypeScript compiler for the \inl{node.js} runtime and one for our Wasm executors. This also shows a very practical way, how developers could utilize these WebAssembly executors: Simply use their existing TypeScript functions and compile them to WebAssembly. The assumption is, that a precompiled WebAssembly binary should offer much higher performance than the JIT-based \inl{node.js}.

\section{Resource Usage}

This test aims to evaluate, what the resource costs are for keeping actions in memory and ready for execution.
To that end, we run our cold start test again, and measure the amount of memory for each container once the actions have finished execution, but before OpenWhisk removes them. At this point, Docker containers have been paused by OpenWhisk. We use the memory usage output of \inl{docker stats} for each container as the basis for evaluation. For the Wasm executors we read the resident set size (rss) of the entire process. Hence, we also measure the overhead of the web framework we run on, but also the Wasm runtimes' objects which are instantiated globally, such as \inl{Engine}s. Thus, this method is slightly imprecise in that regard, but gives us an estimate for the average total cost of keeping a container in memory. Ultimately, the entire memory consumption is what counts, so we measure everything. This method works in favor of Docker, since we do not measure the Docker daemon's memory.
For the Wasm executors, we divide the measured amount of memory by the number of containers that are kept alive at that point, to get an estimated memory usage for each container. The amount of memory consumed when idling, i.e. after startup, are 7.07 MiB for \inl{wasmtime}, 4.23 MiB for \inl{wamr} and 5.28 MiB for \inl{wasmer}. At first glance, it may seem that \inl{wasmtime} fares the worst. However, this may be due to the fact that \inl{wasmtime}'s API is the most suitable to our needs, because it lets us instantiate more of the necessary data structures ahead of time than the other runtimes. We are very happy to trade this memory for more performance later.

\begin{table}[h!]
    \centering
    \begin{tabular}{c | c | c}
        Compilation mode & WebAssembly & Native Binary\\
        \hline
        Release & 1.51 MiB & 3.80 MiB\\
        Debug   & 4.76 MiB & 6.46 MiB\\
        \hline
        Ratio   & 3.15x    & 1.70x\\
    \end{tabular}
    \caption{Size of the hash module when compiled to \inl{wasm32-wasi} and \inl{x86\_64-unknown-linux-musl} targets respectively, in release or debug mode.}
    \label{table:hash-binary-size}
\end{table}

For the Wasm executors, we expect the amount of consumed memory to depend primarily on the size of the Wasm module. So we run this test twice. Once with the previously used \inl{hash} module, compiled in release mode, and once with the module compiled in debug mode. The debug mode increases the module's size by more than three times. The sizes of the Wasm modules and the native binaries are given in table \ref{table:hash-binary-size}, to provide context for the following results.

\begin{figure}
    \begin{center}
        \input{figures/pc-memory.pgf}
    \end{center}
    \caption{The memory usage of Docker and Wasm container runtimes.}
    \label{fig:pc-memory}
\end{figure}

The measurements are in figure \ref{fig:pc-memory}. These confirm that for the Wasm runtimes, the in-memory container size is largely dependent on the module size, simply because the container \emph{is} the module in memory, plus the necessary supporting data structures. In particular, the modules are held in memory for the fastest possible access. For Docker on the other hand, we have to explore in some more detail how an action is executed to understand the results. In order to run native binaries, we use the \inl{dockerskeleton} image. When a container is created from that image, the so-called action proxy starts. This is a proxy service that implements the familiar \inl{/init} and \inl{/run} endpoints. It is generic over the underlying executable, so it can run bash or perl scripts as well as any native binary. When \inl{/run} is invoked and the initialized action is executed, the script or binary runs, its result is read by the proxy and returned. Consequently, after the binary finished execution, the memory of its process is freed as usual. Hence, when we measure the memory in between action executions, we only measure the amount needed for the action proxy and the rest of the container. It does not matter whether the action itself is 1 MiB or 10 MiB, the memory cost of keeping the container warm is the same. This is confirmed by our measurement. We can expect Docker containers to be consistent in their memory usage independent of the concrete action, even though different runtime images may have different sizes to begin with.

For the comparison between different module sizes, the results show \inl{wasmer}'s containers to increase by 3.04x, \inl{wasmtime}'s by 2.39x and \inl{wamr}'s by 1.36x. Docker's in-memory container size only increases by 2.5\%. Within the Wasm runtimes, the results show that \inl{wasmer} and \inl{wasmtime} are more likely to trade memory for performance, than \inl{wamr}, whose primary goal is to have a small footprint.

Wasm runtimes do not fundamentally have to keep modules in memory. The command line versions of \inl{wasmtime} or \inl{wasmer} cache compiled modules on the filesystem by default. \inl{wasmer} would be a prime-candidate for this type of optimization, because in the AoT mode that we use -- with LLVM and \inl{NativeEngine} -- it needs to write the bytes it gets to a temporary file, since \inl{dlopen} only works on files. Thus it already implements this internally, and doing this process in the executor itself would be entirely possible. That in turn would allows us to save on that memory in between executions.

% It may increase cold start times slightly, though with todays SSD drives, probably not by much. This is again a trade off between performance and memory, though probably not proportional.
