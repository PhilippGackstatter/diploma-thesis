\chapter{Evaluation}
\label{chapter:evaluation}

\section{Methodology}

\subsection{Goals}

Recall our research questions.

\begin{enumerate}
    \item By how much can the cold-start problem be alleviated under the Wasm runtimes?
  
    \item How do the performance characteristics of the Wasm and container runtimes differ?
  
    \item What are the resource costs for keeping functions warm in both approaches?
  
    \item Which Webassembly runtime is most suitable for the use on heterogeneous edge devices?
\end{enumerate}

These questions guide our evaluation approach. First we describe the type of workload we run on OpenWhisk in both its base version and its modified Wasm version. Then we describe how our measurements are taken and how we evaluate each of the research questions. 

\subsection{Workload Types}

To measure the performance of the serverless platform in a meaningful way, we reviewed the literature for use cases, where latency is critical; in turn, where the cold start time matters most. From these examples as well as characterizations of the workload types in section \ref{section:serverless_workload}, we infer an evaluation workload.

\begin{description}[style=multiline, leftmargin=2.5cm, font=\bfseries]

    \item[Vital Signs Analysis] \citeauthor{Nastic2017} introduce measuring a patient's vital signs as a latency-critical application. Decisions must be made quickly so it serves as an example, where a serverless platform on edge computers can provide lower latency than sending the data to the cloud \cite{Nastic2017}. Thus, part of the latency is reduced by physically moving the serverless platform closer to the edge device. However, the second important factor for latency is the platform's execution latency, i.e. its cold start time. Reducing the latter from multiple seconds down to a few milliseconds can make an important difference in that use case. The workload is data analytics, which can be classified as CPU-bound, because of the involved data preprocessing, analysis and perhaps even machine learning model training.
    \item[Edge AI] \citeauthor{Rausch2019} describe a motivational use cases for AI at the serverless edge. The first is similar to the just-introduced example. A personal bio sensor measures blood sugar levels and sends it to an edge device, which refines a pre-trained model with that data and serves the model for inferencing \cite{Rausch2019}. The workload for this type of serverless function needs a significant amount of CPU-time for the machine learning tasks but also writes and reads parts of the model to and from disk. This workload cannot be unambiguously classified as either CPU- or file-I/O-bound, as it is rather both at once.
    \item[APIs] According to \citeauthor{Eismann2021a} -- who conducted a manual analysis of serverless applications -- 28\% of their analysed applications were APIs \cite{Eismann2021a}. Even with today's cold start latency, developers are already using serverless for user-facing applications, where latency is rather important. If the latency could be reduced, even latency-critical APIs may be able to use serverless. A useful API is typically accessing or modifying state. Since serverless functions are stateless, they need to communicate with other services such as databases, event buses, logging services or they might even call other functions. This composition can be classified as network-I/O-bound, since the function spends most of its time waiting for the call to the external resource to complete.

\end{description}

We infer two workloads from these. Both CPU- and network-I/O-bound workloads are a primary type that ought to be handled well by a serverless platform. Some limitations apply due to WASI's immaturity or serverless idiosyncracies. In particular, to model a full machine learning example, we should access an already trained model and run predictions on it. However, due to the lack proper network accessibility in WASI we cannot retrieve it from an external service. Due to vanilla OpenWhisk using Docker containers, storing the model on disk would require granting each container access to the model's path on the host, which is not trivially done. We write the actions in Rust and compile it to the \inl{wasm32-wasi} and \inl{aarch64-unknown-linux-musl} target respectively, depending on whether we run a WebAssembly-based executor or the Docker-based one. This ensures that our results aren't distorted by two different implementations in separate languages. It is also one less parameter to take into account when reasoning about the results. In order to execute the native binary in OpenWhisk, we use its blackbox feature. It is implemented by its \inl{dockerskeleton} image, which lets us execute any action that adheres to a JSON-in, JSON-out protocol. We could also use the official Rust support by OpenWhisk, however, that takes a Rust source file and compiles it during the initialization phase, massively distorting the cold start latency. We think using the blackbox version makes for a fair comparison between both container runtime types.
OpenWhisk makes its docker runtime images available for the \inl{x86\_64} architecture only, so we need to build this image for \inl{aarch64}\footnote{\url{https://hub.docker.com/r/pgackst/dockerskeleton}}, in order to execute on our Raspberry Pi. The concrete implementations of our evaluation actions are the following programs.

\begin{description}[style=multiline, leftmargin=2.5cm, font=\bfseries]
    \item[CPU-bound] For this workload, we repeatedly hash a bytestring in a loop using the \inl{blake3} \todo{blake3 footnote} algorithm. This is a convenient choice since its reference implementation is in Rust. Hashing is CPU-bound and free from system calls, so it is a good candiadate for this workload type. We choose the number of iterations so that the completion takes roughly 100ms on the native system, in order to have a non-trivial amount of work to be done for each invocation -- more than OpenWhisk needs for its scheduling internally.
    \item[Network-I/O-bound] For this workload we make an HTTP request to a local server which always takes exactly 100ms to complete.
\end{description}

% Note that we deliberately do not use no_std or other mechanisms to make the modules small, so as to represent a real-world applicable use-case where Rust's std library would most likely be used

\subsection{Setup}

We want to measure the cold start latency and action execution time across all our evaluations. To that end, we utilize OpenWhisk's activation record, which is a collection of data resulting from each action invocation.  Recall that OpenWhisk either initializes and runs the action, in the case of a cold start, or skips the initilization and runs it in a warm container. Each activation record contains a \inl{waitTime} annotation, which is the time between the arrival of the request and the provisioning of a container. It's important to note, that this applies only to docker, since a Wasm container is only represented as a Wasm module, which is created during the initialization. However, the \inl{waitTime} does not include that initialization time, which is recorded as a separate annotation: \inl{initTime}. Thus, for a fair comparison between both container runtimes, we define the cold start time as \inl{waitTime + initTime}. Measured this way, the cold start time is simply the time between OpenWhisk receiving the request and the container being ready for execution. Since both times are part of the latency the user perceives, we believe this to be an accurate measurement. It's important to note that this allows for a fair comparison in relative terms, but the recorded cold start times are not made up entirely of the container runtimes' startup cost, but also of OpenWhisk internals, which may account for a significant portion.

The record also contains a \inl{duration} field which is the duration of the entire execution, including the potential \inl{initTime}. To measure the pure performance of the container runtimes, we subtract \inl{initTime} from \inl{duration}, if necessary. In figure \ref{fig:evaluation-time-measurement} we show an overview of measurements we calculate.

\begin{figure}
    \begin{center}
        \includegraphics{figures/EvaluationTimeMeasurement.pdf}
    \end{center}
    \caption{The two durations we calculate to measure the system's performance.}
    \label{fig:evaluation-time-measurement}
\end{figure}

% With this setup alone, it is difficult to measure the cold start latency, since end - start would include the execution time. To lift this restriction, we additionally measure the wall clock time inside the action, when it enters its main function. Furthermore -- to maximize precision -- we also measure the timestamp at which the main function exits and ignore the activation end timestamp as a result. That leads us to our setup in figure \ref{fig:evaluation-time-measurement}. Now we can accurately measure the time it takes to get the underlying container technology initialized by taking the difference of the entry and start timestamp.

% start - end includes Wasm module's memory allocation and serialization, could mention that and reference its description in the design chapter
% Possibly cite https://www.sciencedirect.com/science/article/pii/B9780124201583000137?via%3Dihub



There are a few papers measuring serverless performance which also describe their approach in more detail. We use those as a basis for our evaluation.

\begin{itemize}
    \item \citeauthor{McGrath2017} use concurrency and backoff tests. The concurrency test starts by issuing a request to the platform and reissues it as soon as it completes. At a fixed interval, it adds another request up to an upper bound. The performance metric is responses per second. The backoff test measures cold start times by issuing single requests at increasing intervals, in order to trigger the platforms deallocation of warm containers \cite{McGrath2017}.
    \item \citeauthor{Hall2019} characterize serverless function access patterns based on a real-world scenario. They describe three different patterns: Non-concurrent requests from a single client; multiple clients making a single request, possibly concurrent; multiple clients making multiple requests, possibly concurrent \cite{Hall2019}.
\end{itemize}

In the interest of reducing complexity, we will run multiple evaluations and measure for the least amount of dimensions. Those evaluations are:

\begin{itemize}
    \item For the cold start latency evaluation we will send concurrent requests to our deployment. The goal is to examine the effect of the Wasm runtime's execution model (i.e. JIT, AoT or interpreted) on the cold start latency.
    We also run it against the Docker-based baseline OpenWhisk. We use a similar approach as \citeauthor{McGrath2017} in their backoff test. That is, in each step, sending $i$ concurrent requests and waiting for complete container deallocation inbetween steps, in order to trigger $i$ cold starts in every test. We run $N$ tests, so that $i \in {1..N}$ and $N$ is the upper bound we determine during the tests. We only use the CPU-bound workload, because the workload type does not matter for the cold start. 
    In our case, we can fix the interval, since we can configure the deallocation time as well. Specifically, we set the \inl{idle-container} timeout to 10 seconds, meaning that OpenWhisk will remove the container after 10 seconds if no suitable request has come in during that time. This measurement will aid in answering research question 1.

    \item To evaluate the performance of the system we will run concurrent requests against an edge device under a mixed workload consisting of equal amounts of CPU- and I/O-bound actions. We repeat this measurement for all our Wasm runtimes and for baseline OpenWhisk. We use the approach by \citeauthor{McGrath2017} in their concurrency test, and pick workload types at random. This is to ensure that we evaluate the system for both types and because it is closer to a real-world scenario than measuring one type in isolation. It is also similar to the approach of \citeauthor{Hall2019}, in that we also have multiple concurrent requests but multiple clients are replaced by diverse workload types, which is likely equivalent. This measurement will help us answer research question 2.
    
    \item Since serverless systems keep their containers warm for some time, the memory they claim is of interest. We will measure the memory footprint of keeping containers warm. \todo{Memory footprint of container during execution? Is that interesting?} We measure only the footprint of the container technology, not OpenWhisk. For Docker we use \inl{docker stats} and for our executor we use \inl{heaptrack} \footnote{\url{https://github.com/KDE/heaptrack}}. This measurement allows answering research question 3.
\end{itemize}

Finally, all measurements will lead us to a conclusion on research question 4.

Our test procedure follows this pattern:

\begin{enumerate}
    \item Compile OpenWhisk standalone version in our modified wasm version (openwhisk-wasm) as well as the latest available version, which is \inl{1.0.0} (openwhisk-vanilla).
    \item Start the OpenWhisk version under test on the test machine
    \item Use the local \inl{wsk} command line tool to create the test action(s)
    \item From a separate machine, start the test binary, which sends requests to the test machine
\end{enumerate}

\subsection{Deployment}

We run our evaluation on an edge device as well as a \inl{x86\_64} desktop machine. The edge device is a Raspberry Pi Model 3B, which has 1 GB of RAM and is running the latest version of the 64-bit Raspberry Pi OS (version \inl{2020-08-20}). We are using the 64-bit version, because the \inl{wasmtime} runtime cannot be compiled for the 32-bit ARM architecture. The desktop has an Intel® Xeon® E3-1231 v3 (3.40 GHz) server-grade CPU with 4 physical cores and 8 logical threads. Mass storage is a Samsung SSD 850 EVO -- which is where Docker images are stored and loaded from -- it has 8 GB of RAM and is running Ubuntu 20.04.2 LTS.

Due to difficulties in deploying OpenWhisk on the ARM64-based Raspberry Pi with Kubernetes and ansible, we use the standalone Java version for evaluation. Since we are not benchmarking OpenWhisk itself, but rather the underlying container technology, we do not believe this to be a threat to validity. The standalone version uses OpenWhisk's \emph{lean} components for internal messaging and load balancing. These are components written specifically for deployment on resource-constrained devices.

\section{Cold Start}

The first version of the test created an action called \inl{hash} (our CPU-bound workload), and then runs concurrent requests against it. In the result we noticed that for 5 and more concurrent requests, there are only 4 cold starts. OpenWhisk will only create 4 containers and lets other requests wait until one of the currently executing ones finishes, so it can reuse the container. This is a fair optimization, but it does not let us test how 5 or more concurrent cold starts actually affect each other. So, instead of creating just one action, we create $N$ actions with different names but the same code. That forces OpenWhisk to execute them in different containers. However, it will still only allocate 4 containers at a time, waiting for one of them to complete before removing it and starting the next. Thus, the performance of the container removal operation also becomes important, as it blocks the provisioning of the next. It is implicitly part of the cold start time as we define it, since a container that needs to wait due to a slow remove operation from another container will have a higher \inl{waitTime}.
% This isn't reflected in the cold start latency, but we can look at the \inl{waitTime} annotation of OpenWhisk's activation record instead. It measures how long the activation spent waiting in OpenWhisk's queues. If the removal operation of Wasm runtimes is shorter than dockers, then Wasm action activations should have to spend less time waiting than their docker counterparts. Although, this value would also be affected by longer cold starts, so it can only provide a partial answer to this question.

\begin{figure}
    \begin{center}
        \input{figures/pi-cold-start-separate-hash.pgf}
    \end{center}
    \caption{The cold start time with a CPU-bound workload on a Raspberry Pi with both Docker and Wasm-based container runtimes.}
    \label{fig:pi-cold-start-hash}
\end{figure}

The result of our cold start test can be seen in figure \ref{fig:pi-cold-start-hash}. While testing the deployment with Wasm runtimes showed no performance issues, the vanilla OpenWhisk deployment was harder to test. It executed a Docker pull operation on every container start, resulting in a higher \inl{waitTime} than absolutely necessary. This can be turned off, which improved cold start times. Moreover, only after increasing the Raspberry Pi's swap memory from the default 100 MB to 1024 MB were we able to get results for up to 6 concurrent requests, whereas before, the Pi froze and had to be rebooted. This rather anecdotal evidence is, however, reflected in the figure, with the docker runtime having orders of magnitude higher cold start latencies than any of the Wasm runtimes. At 5 and 6 concurrent requests, the OpenWhisk log files showed Docker commands timing out and the testing time became unreasonably long -- as reflected in the figure -- which is why we stopped testing at that point. Note that the scale is logarithmic rather than linear, which may be suprising.

Even at 2 concurrent requests, the Wasm executors have a cold start time of around one second, increasing with more requests. \inl{wasmtime} has the most consistent behaviour, followed by \inl{wamr} and \inl{wasmer}. Overall, the runtimes are very similar in their startup performance, especially \inl{wasmtime} and \inl{wamr}, even though they are written in different languages and use different execution modes. The Wasm executors have, on average, less than 1\% the cold start time than docker.

% Look at cold start time only for wasm runtimes, then reference the docker cold start time on an x86 host and draw a comparison. I.e. if the runtimes achieve a startup time of ~100ms on an edge device and docker achieves the same on an x86 host, then we can perhaps conclude that the Wasm runtimes are suitable/acceptable as container runtimes for the serverless edge.

\begin{figure}
    \begin{center}
        \input{figures/pc-cold-start-separate-hash.pgf}
    \end{center}
    \caption{The cold start time with a CPU-bound workload on a Desktop machine with both Docker and Wasm-based container runtimes. The docker values range between 869 ms and 2873 ms, while the Wasm values generally range from 47 ms to 180 ms.}
    \label{fig:pc-cold-start-hash}
\end{figure}

For comparison, the cold start times on our \inl{x86\_64} desktop machine are in figure \ref{fig:pc-cold-start-hash}. The relative differences are similar to the Raspberry Pi. In particular, the Wasm executors behave more predictably on both machines. The average improvement in cold start times are 94\% for \inl{wasmtime} and \inl{wamr} and 93\% for \inl{wasmer}. The improvements are very consistent across the number of concurrent requests, with a standard deviation of less than 0.01 percentage points for all runtimes.

These comparisons give a good holistic picture of the performance of both runtimes in the OpenWhisk context. However, because the measurements include other OpenWhisk specifics, they are less useful for a general container runtime comparison. Thus...
% Run experiment: cold start, then warm start -- see relationship between the two, for docker & wasm

% Does OpenWhisk allocate containers corresponding to the number of logical CPUs?
% median vs mean
% how many runs is data based on?
% use a 30MB or sth module to test WAMR impl performance then

\section{Throughput}

\begin{figure}
    \begin{center}
        \input{figures/pi-pc-load-mixed.pgf}
    \end{center}
    \caption{The throughput in requests per second with an equal amount of I/O- and CPU-bound workload on a Raspberry Pi and a Desktop machine with both Docker and Wasm-based container runtimes.}
    \label{fig:pi-pc-load-mixed}
\end{figure}

In the previous cold start test it became apparent that running OpenWhisk vanilla with Docker on the Raspberry Pi is challenging. Because OpenWhisk runs four containers under heavy load, it puts too much strain on our test edge device. Thus, we make use of the previously introduced concurrency limit that we can set for each action. We set the limit to 10, such that 10 actions can be executed concurrently in the same container. With this setup, we can run the load test for the Pi, because OpenWhisk only creates two containers -- at least up to a certain threshold. Log collection is another process that tends to fail often, so we use \inl{--logsize 0} when creating the action, to prevent OpenWhisk from attempting to collect them. We use the same parameters for creating the Wasm actions. With that we get the result in figure \ref{fig:pi-pc-load-mixed}. OpenWhisk vanilla manages to handle up to 10 concurrent requests on the Pi, at which point it creates a third container, which forces the Pi to use its swap memory and become orders of magnitude slower. Hence, at this point we stopped testing. The workload we run is mixed and made up of an equal amount of I/O- and CPU-bound actions. Whether one or the other is executed is determined at random, such that, on average, half will be I/O-bound and half will be CPU-bound. We execute two runs and take the average for each number of concurrent requests. Note that the CPU-bound workload is chosen such that the execution time is similar on the Raspberry and the Desktop machine, since the blocking I/O-bound workload also takes the same amount of time on both devices. That makes the results more comparable. This test is executed with Apache JMeter\footnote{\url{https://jmeter.apache.org/}}, a load testing tool. 

\begin{table}[h!]
    \centering
    \begin{tabular}{c | c | c}
        Runtime        & Raspberry Pi & Desktop\\
        \hline
        \inl{wasmtime} & 3.8x & 2.2x\\
        \inl{wasmer}   & 4.2x & 3.0x\\
        \inl{wamr}     & 2.4x & 1.6x\\
    \end{tabular}
    \caption{Average factor by which the respective WebAssembly executors are faster than Docker in the mixed workload scenario.}
    \label{table:pi-pc-load-mixed-improvements}
\end{table}

We can immediately see a a stronger difference in all container runtimes than in the cold start test.
The factors of improvements are listed in table \ref{table:pi-pc-load-mixed-improvements}, where we can see, that on both devices, the WebAssembly executors are better able to handle a mixed workload. The executors perform similarly on both hardware classes when examining the order of fastest to slowest. However, on the Desktop, \inl{wasmer} has a greater lead on \inl{wasmtime} than on the Pi. Both of these runtimes perform with higher fluctuations than \inl{wamr}, which has a very consistent behaviour.

\inl{wasmer} likely takes the lead here, because we configured it to use the \inl{LLVM} compiler toolchain which produces high-quality native code and can be opened by the runtime with a fast \inl{dlopen} call. On the other hand, \inl{wamrc} also uses \inl{LLVM} for ahead-of-time compilation, but performs worse. One contributing factor may be that the \inl{wamr} API does not allow us to thread-safely initialize the module once and run it often. The module needs to be instantiated from the raw bytes on every \inl{run} call. Because of that, the implementation is less efficient compared to the other executors. However, ... we will run an evaluation for that later -- comparing instantiation times of the Wasm executors.
\inl{wasmtime} is configured to use the \inl{cranelift} JIT compiler. Because of the inherent trade-off such a compiler has to make between compilation speed and code quality, it is perhaps unsuprising that it is less performant than the \inl{LLVM}-based wasmer compiler. Interestingly, it does perform better than \inl{wamr}.

To get a more isolated picture of the container runtimes performance, we test the workloads separately on our Desktop machine. The I/O-bound test is supposed to test the container runtimes' ability to handle concurrent network requests. As we've explored in chapter \ref{chapter:background}, serverless functions are frequently used with external services, such as databases. This test emulates this behaviour by blocking for 300ms, implemented using a simple thread sleep. In the Wasm executors, this is implemented with the capabilities, which allows the Wasm runtimes to supply native host functions as imports to the module. In this instance, we create our action using \inl{--annotation net\_access true} with the \inl{wsk} cli. The Wasm executor then provides the \inl{http.get} function as an import to the module. Each executor implements this with the aforementionend \inl{sleep}. The module can then use these functions as if they were part of the module. This way, we could easily implement an actual HTTP GET request. However, the lack of interface types means the module can only call native functions with integer parameters and return values, such that the practical use is still inhibited.

\begin{figure}
    \begin{center}
        \input{figures/pc-load-block.pgf}
    \end{center}
    \caption{The throughput with an I/O-bound workload on a Desktop machine with both Docker and Wasm-based container runtimes. The y-axis shows the throughput in requests per second, while the x-axis shows the number of concurrent requests, each of which is repeated 50 times to generate a consistent load over a short period of time.}
    \label{fig:pc-load-block}
\end{figure}

Cold starts should not affect this test, so a warmup phase is executed before gathering data, which lets OpenWhisk create the containers and keep them in-memory; »warm«. The throughput on a non-edge device is plotted in figure \ref{fig:pc-load-block}. OpenWhisk imposes the same 4 container limit for all container runtimes, which puts a limit on the throughput for I/O-bound workloads. The relative differences between both container runtimes can be seen, regardless. Overall, they perform very similarly, with the Wasm executors completing a slightly greater number of requests per second. On average, the Wasm executors are 15\% (\inl{wamr}) and 16\% (\inl{wasmer}, \inl{wasmtime}) faster than Docker.

\begin{figure}
    \begin{center}
        \input{figures/pc-load-hash.pgf}
    \end{center}
    \caption{The throughput with a CPU-bound workload on a Desktop machine with both Docker and Wasm-based container runtimes. The y-axis shows the throughput in requests per second, while the x-axis shows the number of concurrent requests, each of which is repeated 50 times to generate a consistent load over a short period of time.}
    \label{fig:pc-load-hash}
\end{figure}

In figure \ref{fig:pc-load-hash} is the result of the fully CPU-bound load test. This test shows a large performance gap among the Wasm executors, but also to Docker. On average, the Wasm executors process only 39\% (\inl{wamr}), 57\% (\inl{wasmtime}) and 88\% (\inl{wasmer}) of the throughput that Docker achieves.

Docker was particularly hard to evaluate for this workload. OpenWhisk -- at least in the standalone version -- is hardly able to saturate the CPU, because the system will not create enough containers to fully utilize the CPU.
This likely occurrs because each container is run with the \inl{--cpu-shares} Docker option, limiting the share of the CPU the container is allowed use. Hence, if not enough containers are created, some invocations have to wait for others to complete, even though the CPU isn't fully utilized.
By increasing the concurrency limit, we can work around that and let OpenWhisk execute actions in the same container to fully leverage the assigned CPU shares. Increasing this limit too much, forces more actions to share the same CPU shares, while not creating new containers.
However, increasing the memory on the container, which in turn makes OpenWhisk increase the CPU shares, does not have a positive effect either. Rather, it delays OpenWhisk's decision to create new containers even longer, resulting in even poorer performance. After some trial and error we arrived at values which seemed to leverage the CPU rather well, the default 256 MB of memory and the concurrency limit set to 3.
The Wasm executors showed no such signs, simply because no memory or CPU limiting is implemented in the first place. This will have to be a feature of a fully production-ready Wasm executor, however.

Cold start latencies and resource costs for keeping containers warm are ignored in this test. With that in mind, the test shows that for a steady stream of highly concurrent CPU-bound requests, using Docker is the best option. In particular, if the container is likely to be reused many times. This comes perhaps not unsuprisingly, given that the code executing is Rust compiled to a native \inl{x86\_64} binary. There is hardly a way to generate faster code. In fact, if we compare the pure execution time of our wasm action outside the serverless context, we see the native binary being even faster. Here we precompile our action for wasmer and then measure only the pure function execution time, without the setup of necessary contexts. In that case, wasmer achieves 60\% of the throughput compared to the native binary. Compared to the 88\% from above, it indicates that OpenWhisk is unable to fully utilize the CPU, either because of the discussed configuration or because of internal overhead, like queuing and container management. 

Note that using native code in serverless functions is not a particularly popular approach. According to \citeauthor{Eismann2021}, the overwhelming share of serverless functions is implemented in interpreted languages such as JavaScript (42\%) and Python (42\%) \cite{Eismann2021}. Data from dashbird.io\footnote{\url{https://dashbird.io/}}, a serverless monitoring tool, coroborrates the assumption that interpreted languages are dominant. Of their 3000+ users, 76\% use JavaScript, 13\% use Python and 8\% use Java \cite{Rehemaegi2019}.
This has two implications. One is, that high performance functions can be considered a niche application in serverless. If this were not the case, we would likely see a higher share of compiled languages. Although a language like Python can be extended with modules written in C, the dashbird data in particular indicates that JavaScript-level performance is mostly good enough.
The other is, if we can be as fast as these interpreted or JIT-ed languages, we are not suffering from a loss in performance and, if we can be even faster than those, we already have net performance gains to report. Thus, from that perspective, a fully CPU-bound WebAssembly program reaching 88\% of the throughput of a native binary is already an achievement.
% To put this into more perspective, we run a experiment that runs a workload in \inl{node.js}, arguably the go-to serverless runtime, and in WebAssembly. To that end, we write a function in AssemblyScript\footnote{\url{https://www.assemblyscript.org}}, a strict variant of TypeScript. We can then compile the same function with a TypeScript compiler for the \inl{node.js} runtime and one for our Wasm executors. This also shows a very practical way, how developers could utilize these WebAssembly executors: Simply use their existing TypeScript functions and compile them to WebAssembly. The assumption is, that a precompiled WebAssembly binary should offer much higher performance than the JIT-based \inl{node.js}.

\section{Resource Usage}

This test aims to evaluate, what the resource costs are for keeping actions in memory and ready for execution.
To that end, we run our cold start test again, and measure the amount of memory for each container once the actions have finished execution, but before OpenWhisk removes them. At this point, Docker containers have been paused by OpenWhisk. We use the memory usage output of \inl{docker stats} for each container as the basis for evaluation. For the Wasm executors we read the resident set size (rss) of the entire process. Hence, we also measure the overhead of the web framework we run on, but also the Wasm runtimes' objects which are instantiated globally, such as \inl{Engine}s. We divide the measured amount by the number of containers that are kept alive at that point, to get an estimated memory usage for each container. The amount of memory consumed when idling, i.e. after startup, are 7.07 MiB for \inl{wasmtime}, 4.23 MiB for \inl{wamr} and 5.28 MiB for \inl{wasmer}.

\begin{figure}
    \begin{center}
        \input{figures/pc-memory-hash.pgf}
    \end{center}
    \caption{The memory usage of Docker and Wasm container runtimes.}
    \label{fig:pc-memory-hash}
\end{figure}

The measurements are in figure \ref{fig:pc-memory-hash}. We use our previously introduced \inl{hash} module, which comes at a size of 1.51 MiB in its original wasm form, i.e. before precompilation. For the Wasm executors, we expect the amount of memory consumed to depend primarily on the size of the Wasm module. For comparison we create another module with a larger size to determine whether the relationship is in fact proportional. (todo)