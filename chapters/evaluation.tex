\chapter{Evaluation}
\label{chapter:evaluation}

In this chapter, we evaluate our WebAssembly executor by comparing it to OpenWhisk vanilla. First we discuss and define the types of workloads we use in our experiments. Then we go into details of our methodology, the experimental setup, which metrics we measure and what hardware we deploy on. All that will lead to answers for our research questions.

\section{Methodology}

\subsection{Goals}

Recall our research questions.

\begin{enumerate}
    \item By how much can WebAssembly runtimes alleviate the cold start latency?
  
    \item How do the performance characteristics of the WebAssembly and Docker container runtimes differ?
  
    \item What are the resource costs for keeping containers warm in both container runtimes?
  
    \item Which WebAssembly runtime is the most suitable for use on edge devices?
\end{enumerate}

% In order to answer these questions, we need to measure 
% These questions guide our evaluation approach.

\subsection{Workload Types}

In Chapter \ref{chapter:background} we described the serverless workload in detail, which guided decisions in our implementation. Specifically, we already used that research to get an idea about what concurrency model may be a better fit for our executor.

This research workload represents the current \emph{cloud} workload and may or may not be transferable to workloads at the edge. Because of that, we also take some examples of visionary use cases into account and infer scenarios, i.e. the order of cold starts and warm starts, as well as workload types, i.e. whether functions are CPU- or I/O-bound. That includes both serverless edge workloads and latency-sensitive ones.

% To measure the performance of the serverless platform in a meaningful way, we reviewed the literature for use cases, where latency is critical; in turn, where the cold start time matters most. From these examples as well as characterizations of the workload types in section \ref{section:serverless_workload}, we infer an evaluation workload.


\begin{description}[style=multiline, leftmargin=2.5cm, font=\bfseries]

    \item[Vital Signs Analysis] \citeauthor{Nastic2017} introduce measuring a patient's vital signs with wearable sensors during a situation of crisis \cite{Nastic2017}. Since lives are at stake, decisions must be made quickly. The scenario is both a latency-critical and an edge computing application. It serves as an example, where a serverless platform on edge computers can provide lower latency than sending the data to the cloud. Thus, part of the latency is reduced by moving the computation physically closer to the sensor. However, the second important factor for latency is the platform's execution time. When a sensor comes online and sends its first request, the execution suffers from a cold start. Once sensors are online, they need to analyze data in real time and continuously. Therefore, when the cold start happens, the platform may already be under load from other requests, and there may be concurrent cold starts. This is a scenario where high performance and efficient usage of the limited resources is required, and one where the same functions are called repeatedly -- warm starts.
    The workload itself is data analytics, but not described in more detail. We classify this as primarily CPU-bound, because we can assume it involves data preprocessing, statistical analysis and perhaps even machine learning inference.

    \item[Edge AI] \citeauthor{Rausch2019} describe a motivational use cases for AI at the serverless edge. A personal bio sensor measures blood sugar levels and sends it to an edge device, which refines a pre-trained model with that data and serves the model for inferencing \cite{Rausch2019}. The workload for this type of serverless function needs a significant amount of CPU-time for the machine learning tasks but also writes and reads parts of the model to and from disk. We classify this as both CPU- and file-I/O-bound. Considering that measuring blood sugar levels is sufficient in greater intervals, e.g. 10 minutes, or whenever the user does a manual measurement, each of those would spawn a function that does the analysis and training, but could be discarded once it is finished. That is, keeping the function warm would be unnecessary and wasting resources. If the user actively took the measurement and waits for the analysis results, a low latency is required for a good quality of experience. Thus, the cold start of the analysis function should be as short as possible.
    
    \item[Augmented Reality] \citeauthor{Huang2012} implemented a mobile augmented reality (AR) system that analyzes images of book spines locally, e.g. on a smartphone, and then sends the features to the cloud to find a match in a database \cite{Huang2012}. \citeauthor{Baresi2019} propose to offload the image recognition task to a more powerful edge device, to save on battery on the hosting device \cite{Baresi2019}. AR applications require a low latency to provide a good user experience. Offloading such recognition tasks to nearby and more powerful edge devices presupposes their capability to execute them at high speed. This is a scenario where a serverless platform would receive a high number of concurrent CPU-bound requests.

    \item[APIs] According to \citeauthor{Eismann2021a} -- who conducted a manual analysis of serverless applications -- 28\% of their analyzed applications were APIs \cite{Eismann2021a}. Even with today's cold start latency, developers are already using serverless for user-facing applications, where latency is rather important. However, as \citeauthor{Leitner2019} noted in their interviews with practitioners, many of them voiced concerns about the high latency associated with cold starts \cite{Leitner2019}. If the latency could be reduced significantly, even the APIs of latency-sensitive applications can be built on serverless. A useful API is typically accessing or modifying state. Since serverless functions by themselves are stateless, they need to communicate with other services such as databases, event buses and logging services. This composition requires many network I/O-bound calls and is usually light on the CPU, since the function spends most of its time waiting for I/O to complete.

\end{description}

We infer two workloads from these, while also taking into account our previous research. Both CPU- and network I/O-bound workloads are a primary type that ought to be handled well by a serverless platform. Some limitations apply due to WASI's immaturity or serverless idiosyncrasies. In particular, to model a full machine learning example, we should access an already trained model and run predictions on it. However, due to the lack of proper networking in WASI, we cannot retrieve it from an external service. Due to vanilla OpenWhisk using Docker containers, storing the model on disk would require granting each container access to the model's path on the host, which is not officially supported by the platform. Since file and network I/O are perceived similarly by the waiting function, we can focus on simulating one of these, which is going to be network I/O due to its prevalence.

% It is also one less parameter to take into account when reasoning about the results.
We write the test actions in Rust and compile them to \inl{wasm32-wasi} and \inl{aarch64} or \inl{x86\_64} respectively, depending on whether we test a WebAssembly or Docker executor and which hardware we test on. This ensures that our results are not distorted by side effects from two different implementations in separate languages. In order to execute the native binary in OpenWhisk we use its blackbox feature. It is implemented by the \inl{dockerskeleton} image, which lets us execute any action that adheres to a JSON-in, JSON-out protocol. We could also use the official Rust support by OpenWhisk, however, that takes a Rust source file and compiles it during the initialization phase -- massively increasing the cold start latency. We think using the blackbox version makes for a fair comparison between both container runtime types, since it measures the startup time of the respective container implementations, making it much more comparable to other serverless platforms. Although interpreted languages, such as JavaScript or Python, make up the overwhelming share of serverless languages, we use natively compiled actions to compare the precompiled WebAssembly to this optimum in performance. To also give an idea how performance compares to these prevalent languages, we also write an experimental action in a language, which compiles to both JavaScript and WebAssembly.
OpenWhisk makes its docker runtime images available for the \inl{x86\_64} architecture only, so we need to build this image for \inl{aarch64}\footnote{\url{https://hub.docker.com/r/pgackst/dockerskeleton}} first, in order to execute on a Raspberry Pi. The concrete implementations of our evaluation actions are the following programs.

\begin{description}[style=multiline, leftmargin=2.5cm, font=\bfseries]
    \item[CPU-bound] For this workload, we repeatedly hash a bytestring in a loop using the \inl{blake3}\footnote{\url{https://crates.io/crates/blake3}} algorithm. This is a convenient choice since its reference implementation is in Rust. Hashing is CPU-bound and free from system calls, so it is a good candidate for this workload type. We choose the number of iterations such that the completion takes roughly 100ms in a native binary on the respective hardware, in order to have a non-trivial amount of work to be done for each invocation -- significantly more than OpenWhisk needs for its internal scheduling.
    \item[I/O-bound] For this workload we want to measure the effect of a blocking operation, such as an HTTP request. However, due to the lack of networking in WASI, we instead use a \inl{sleep} system call. In WebAssembly this is implemented through capabilities. The module declares an import called \inl{http.get}, which the WebAssembly executor provides. It is a function natively compiled into the executor which blocks for exactly 300ms, so the perceived effect of this function is the same as waiting for I/O completion.
\end{description}


\subsection{Setup}

We want to measure the cold start latency and action execution time across all our evaluations. To that end, we utilize OpenWhisk's activation record, which is a collection of data resulting from each action invocation.  Recall that OpenWhisk either initializes and runs the action, in the case of a cold start, or skips the initialization and runs it in a warm container. Each activation record contains a \inl{waitTime} annotation, which is the time between the arrival of the request and the provisioning of a container. For Docker this means creating and starting the container. In WebAssembly executors, no equivalent operation exists, such that the \inl{waitTime} will only correspond to the OpenWhisk internal overhead. Rather, the WebAssembly container is created during function initialization, which is not included in \inl{waitTime}, but recorded as a separate annotation: \inl{initTime}. Thus, for a fair comparison between both container runtimes, we define the cold start time as \inl{waitTime + initTime}. Measured this way, the cold start time is simply the time between OpenWhisk receiving the request and the container being ready for execution, independent of the underlying container runtime. Since both times are part of the latency the user perceives, we believe this to be a meaningful measurement. It is important to note that this allows for a fair comparison in relative terms, that is, the recorded cold start times are not made up entirely of the container runtimes' startup cost, but also of OpenWhisk internal operations, which may account for a significant portion. This may limit the comparability to cold start measurements from other research. To facilitate this comparison, we provide the pure startup time, i.e. only \inl{initTime} of WebAssembly executors as well.

%The record also contains a \inl{duration} field which is the duration of the entire execution, including the potential \inl{initTime}. To measure the pure performance of the container runtimes, we subtract \inl{initTime} from \inl{duration}, if necessary.

In Figure \ref{fig:evaluation-time-measurement} we show a visual representation of what constitutes cold start latency and the request latency perceived by the caller, which is the amount of time from which throughput will be calculated.

\begin{figure}
    \centering
    \includegraphics{figures/EvaluationTimeMeasurement.pdf}
    \caption{The two time measurements we use to evaluate the system's performance.}
    \label{fig:evaluation-time-measurement}
\end{figure}

There are a two papers which describe their approach to measuring serverless performance in more detail, which we use as a basis for our evaluation.

\begin{itemize}
    \item \citeauthor{McGrath2017} use concurrency tests to evaluate a platform's ability to scale \cite{McGrath2017}. The concurrency test starts by issuing a request to the platform and reissues it as soon as it completes. At a fixed interval, it adds another request up to an upper bound. In this manner, the load increases over time until a plateau is reached, where an increase in requests no longer increases the number of completed requests per time unit.
    \item \citeauthor{Hall2019} characterize access patterns to serverless functions based on a real-world scenario. They describe three different patterns: Non-concurrent requests from a single client; multiple clients making a single request, possibly concurrent and multiple clients making multiple requests, also possibly concurrent \cite{Hall2019}.
\end{itemize}

Based on these, we use two similar tests to measure cold start times and throughput.

\begin{itemize}
    \item To measure cold start times we first configure OpenWhisk's deallocation time as 10 seconds. We can then send requests at intervals exceeding 10 seconds to always trigger cold starts. In order to measure the effect of concurrency on cold starts, we send $i$ number of concurrent requests, where $i$ iterates through the set $\{1,...,N\}$ and $N$ is an upper bound we determine during the tests. After each iteration, we wait for OpenWhisk to destroy all containers before continuing. That results in exactly $i$ concurrent cold starts on each iteration. This measurement will aid in answering research question 1.

    \item To evaluate the performance of the systems, we will run concurrent requests under various workloads consisting of either CPU- or I/O-bound actions, or both. In the mixed scenario, we use the approach by \citeauthor{McGrath2017} in their concurrency test, and pick workload types at random. This is to ensure that we evaluate the system for both types and close to a real-world scenario. It is also similar to the approach of \citeauthor{Hall2019}, in that we have multiple concurrent requests but multiple clients are replaced by diverse workload types. The isolated experiments help us evaluate the pure performance of the respective workload types. Finally, we will run a workload based on research data introduced in Chapter \ref{chapter:background} which simulates that of a cloud provider. We will go into more detail for each measurement in the respective sections. These experiments will help us answer research question 2.

    \item This test aims to evaluate, what the resource costs of actions are, specifically their code size and the amount of memory they consume while being held in-memory; ready for execution. We use the Docker API to measure each container's memory footprint and measure the allocated memory of the WebAssembly executor process to infer the costs for WebAssembly containers. This measurement supports the answer to research question 3.
\end{itemize}

Finally, all measurements and the experience of the design phase will lead us to a conclusion on research question 4.

Our test procedure follows this pattern:

\begin{enumerate}
    \item Based on the latest available stable release of OpenWhisk, which is \inl{1.0.0}, compile the standalone version in our modified Wasm variant, as well as the unmodified vanilla version.
    \item Start the OpenWhisk variant under test on the test machine.
    \item Use the local \inl{wsk} command line tool to create the test action(s).
    \item From a separate machine, start the test procedure, which will send requests to the test machine.
\end{enumerate}

\subsection{Deployment}

We run our evaluation on an edge device as well as a \inl{x86\_64} desktop machine. The edge device is a Raspberry Pi Model 3B, which has 1 GB of RAM and is running the latest version of the 64-bit Raspberry Pi OS (version \inl{2020-08-20}). We are using the 64-bit version, because the \inl{wasmtime} runtime cannot be compiled for the 32-bit ARM architecture. The desktop has an Intel® Xeon® E3-1231 v3 (3.40 GHz) server-grade CPU with 4 physical cores and 8 logical threads. Mass storage is a Samsung SSD 850 EVO -- which is where Docker images are stored and loaded from -- and it has 8 GB of RAM and is running Ubuntu 20.04.2 LTS.

Due to difficulties in deploying OpenWhisk on the Raspberry Pi with Kubernetes and ansible, we use the standalone Java version for evaluation. The standalone version uses OpenWhisk's \emph{lean} components for internal messaging and load balancing. These are components written specifically for deployment on resource-constrained devices. Since we are not benchmarking OpenWhisk itself, but rather the underlying container technology, we do not believe this to be a threat to validity.

\section{Cold Start}

\begin{figure}
    \begin{center}
        \input{figures/pi-cold-start-separate-hash.pgf}
    \end{center}
    \caption{The cold start time with a CPU-bound workload on a Raspberry Pi with both Docker and Wasm-based container runtimes. Note that the scale is logarithmic rather than linear, which may be surprising.}
    \label{fig:pi-cold-start-hash}
\end{figure}

During testing, various issues arose that we want to discuss, in order to give context about the deployment process. 
The first version of the test created our CPU-bound test action, called \inl{hash}, and then ran concurrent requests against OpenWhisk. In the result we noticed that for 5 and more concurrent requests, there were only 4 cold starts, independent of the underlying container runtime. It turned out that OpenWhisk would only create 4 containers and let other requests wait until one of the currently executing ones finished, so it could reuse the container. This is a fair optimization, but it does not let us test how 5 or more cold starts actually affect each other. So, instead of creating just one action, we create $N$ actions with different names but the same code. OpenWhisk will treat these as different actions and execute them in separate containers accordingly. However, it will still only allocate 4 containers at a time, something that does not appear to be configurable, waiting for one of them to complete before removing it and starting the next. Thus, the performance of the container removal operation also becomes important, as it blocks the provisioning of the next. It is implicitly part of the cold start time as we define it, since a container that needs to wait due to a slow remove operation from another container will have a higher \inl{waitTime}.
% This isn't reflected in the cold start latency, but we can look at the \inl{waitTime} annotation of OpenWhisk's activation record instead. It measures how long the activation spent waiting in OpenWhisk's queues. If the removal operation of Wasm runtimes is shorter than dockers, then Wasm action activations should have to spend less time waiting than their docker counterparts. Although, this value would also be affected by longer cold starts, so it can only provide a partial answer to this question.
% While testing the deployment with Wasm runtimes showed no performance issues, the vanilla OpenWhisk deployment was harder to test. 

OpenWhisk vanilla showed further issues in its default configuration. On every container start it executes a Docker pull operation, even if the image was locally present, which meant checking if the local image was up to date with the remote version, resulting in a higher \inl{waitTime} than necessary. Therefore, we turned this behavior off, since it is not an inherent part of the container startup, which improved cold start times.
% The Raspberry Pi we use has 913 MiB of total memory. The OS needs 210 MiB and OpenWhisk uses 185 MiB of memory after startup.

With two running Docker containers, OpenWhisk starts to approach the limits of the Raspberry Pi's memory. If OpenWhisk decides to start a third container, the system runs out of memory and freezes. Only after increasing the swap memory from the default 100 MiB to 1024 MiB were we able to test up to 6 concurrent cold starts.
This is reflected in Figure \ref{fig:pi-cold-start-hash} -- where we plotted the result of the cold start test -- with the Docker runtime having orders of magnitude higher cold start latencies than any of the Wasm executors. At 5 and 6 concurrent requests, the OpenWhisk logs showed Docker commands timing out and the testing time became unreasonably long -- as reflected in the figure -- which is why we stopped testing at that point.

Even at 2 concurrent requests, the Wasm executors have a cold start time of around one second, increasing with more requests. \inl{wasmtime} has the most consistent behaviour, followed by \inl{wamr} and \inl{wasmer}. Overall, the runtimes are similar in their startup performance, especially \inl{wasmtime} and \inl{wamr}, even though they are written in different languages and use different compilation strategies. The Wasm executors have, on average, less than 0.5\% the cold start time of Docker.

% Look at cold start time only for wasm runtimes, then reference the docker cold start time on an x86 host and draw a comparison. I.e. if the runtimes achieve a startup time of ~100ms on an edge device and docker achieves the same on an x86 host, then we can perhaps conclude that the Wasm runtimes are suitable/acceptable as container runtimes for the serverless edge.

\begin{figure}
    \begin{center}
        \input{figures/pc-cold-start-separate-hash.pgf}
    \end{center}
    \caption{The cold start time with a CPU-bound workload on a Desktop machine with both Docker and Wasm-based container runtimes. The docker values range between 869 ms and 2873 ms, while the Wasm values generally range from 47 ms to 180 ms.}
    \label{fig:pc-cold-start-hash}
\end{figure}

For comparison, the cold start times on our \inl{x86\_64} desktop machine are in Figure \ref{fig:pc-cold-start-hash}. The relative differences are similar to the Raspberry Pi. In particular, the Wasm executors behave more predictably on both machines. Docker cold start latencies suffer under rising concurrency and vary much more.
The average reduction in cold start times are 94\% for \inl{wasmtime} and \inl{wamr} and 93\% for \inl{wasmer}.
% The improvements are very consistent across the number of concurrent requests, with a standard deviation of less than 0.01 percentage points for all runtimes.

These comparisons give a good holistic picture of the performance of both runtimes in the OpenWhisk context. However, because the measurements include other OpenWhisk specifics, they are less useful for a general container runtime comparison. Thus, we plotted \inl{initTime} for the Wasm executors in Figure \ref{fig:pc-pi-cold-start-wasm-only}. This time represents the duration of \inl{/init} endpoint on the executor. It provides comparability to serverless platforms other than OpenWhisk since it isolates the overhead introduced \emph{only} by the executor. It also shows the Wasm executors in more detail -- providing a clearer picture for the evaluation among them, since the Docker runtime distorted the original graph to a large degree.

Independent of the hardware, \inl{wasmer} is about 47\% slower than \inl{wasmtime} and 61\% slower than \inl{wamr}. At initialization time, the runtime receives the bytes and deserializes them into a \inl{Module}, similar to \inl{wasmtime}. However, since we use the \inl{NativeEngine} in \inl{wasmer} and it needs to open the shared object via \inl{dlopen}, the bytes need to be written to a temporary file first. We do not expect this accounts for all of the differences seen, however.
The runtime seems to have an inherently higher cost for deserialization and is slightly less consistent in its performance, based on the confidence intervals.

\begin{figure}
    \begin{center}
        \input{figures/pc-pi-cold-start-wasm-only.pgf}
    \end{center}
    \caption{The cold start time with a CPU-bound workload on the Raspberry Pi only for the Wasm executors and with a confidence interval of 95\%.}
    \label{fig:pc-pi-cold-start-wasm-only}
\end{figure}

% Run experiment: cold start, then warm start -- see relationship between the two, for docker & wasm

% Does OpenWhisk allocate containers corresponding to the number of logical CPUs?
% median vs mean
% how many runs is data based on?
% use a 30MB or sth module to test WAMR impl performance then

\section{Throughput}

\subsection{Mixed Workload}

In the previous cold start test it became apparent that running OpenWhisk vanilla with Docker on the Raspberry Pi is challenging. Because OpenWhisk runs four containers under heavy load, it puts too much strain on our test edge device. Thus, we make use of the previously introduced concurrency limit that we can set for each action. We set the limit to 10, such that 10 actions can be executed concurrently in the same container. With this setup, we can run the load test for the Pi, because OpenWhisk only creates two containers -- at least up to a certain threshold. Log collection is another process that tends to fail often, so we use \inl{--logsize 0} when creating the action, to prevent OpenWhisk from attempting to collect them. We use the same parameters for creating the Wasm actions. We run one iteration of this test before starting the measurement, in order to pre-warm containers. This is to test the system in a warm state isolated from the effects of the first cold starts and get an unaffected measurement of the performance of WebAssembly containers compared to native binaries in Docker containers. At some points during the test run the system might decide to scale up, if the load crosses a certain threshold, and perform a cold start of another container, which would be included in the test. This would be the most realistic performance for the Vital Signs Analysis example we gave earlier, i.e. continuous load on a function with the occasional cold start. With this setup, we get the results in Figure \ref{fig:pi-pc-load-mixed}.

\begin{figure}
    \begin{center}
        \input{figures/pi-pc-load-mixed.pgf}
    \end{center}
    \caption{The throughput in requests per second with an equal amount of I/O- and CPU-bound workload on a Raspberry Pi and a Desktop machine with both Docker and Wasm-based container runtimes.}
    \label{fig:pi-pc-load-mixed}
\end{figure}

OpenWhisk vanilla manages to handle up to 10 concurrent requests on the Pi, at which point it creates a third container, which forces the Pi to use its swap memory and become orders of magnitude slower. Again, at this point we stopped testing. The workload we run is mixed and made up of an equal amount of I/O- and CPU-bound actions. Whether one or the other is executed is determined at random, such that, on average, half will be I/O-bound and half will be CPU-bound. We execute two runs and take the average for each number of concurrent requests. Note that the CPU-bound workload is chosen such that the execution time is similar on the Raspberry and the Desktop machine, since the blocking I/O-bound workload also takes the same amount of time on both devices. That makes the results more comparable. This test is executed with Apache JMeter\footnote{\url{https://jmeter.apache.org/}}, a load testing tool. 

\begin{table}[h!]
    \centering
    \begin{tabular}{c | c | c}
        Runtime        & Raspberry Pi & Desktop\\
        \hline
        \inl{wasmtime} & 3.8 & 2.2\\
        \inl{wasmer}   & 4.2 & 3.0\\
        \inl{wamr}     & 2.4 & 1.6\\
    \end{tabular}
    \caption{Average factor by which the respective WebAssembly executors are faster than Docker in the mixed workload scenario.}
    \label{table:pi-pc-load-mixed-improvements}
\end{table}

We can immediately see a stronger difference in all container runtimes than in the cold start test.
The performance improvements are listed in Table \ref{table:pi-pc-load-mixed-improvements}, where we can see, that on both devices, the WebAssembly executors are better able to handle a mixed workload. The executors perform similarly on both hardware classes when examining the order of fastest to slowest. However, on the Desktop, \inl{wasmer} has a greater lead on \inl{wasmtime} than on the Pi. Both of these runtimes perform with higher fluctuations than \inl{wamr}, which has a very consistent behaviour. In general, the effect is more pronounced on the Raspberry Pi, indicating that Docker is less suitable for edge devices than for cloud hardware.

The lead is taken by \inl{wasmer}, likely because we configured it to use the \inl{LLVM} compiler toolchain, which produces high-quality native code. On the other hand, \inl{wamrc} also uses \inl{LLVM} for ahead-of-time compilation, but performs worse. In our later experiments we will see that that \inl{wamr} has a stronger tendency to optimize for size, perhaps at the expense of speed, which may be a contributing factor. Additionally, its API does not allow us to thread-safely initialize the module once and run it often. Instead, the module needs to be instantiated from the raw bytes on every \inl{run} call. Because of that, the implementation is less efficient compared to the other executors.
\inl{wasmtime} is configured to use the \inl{cranelift} JIT compiler. Because of the inherent trade-off such a compiler has to make between compilation speed and code quality, it is perhaps unsurprising that it is less performant than \inl{wasmer}. Interestingly, it still performs better than \inl{wamr}.

\subsection{I/O-bound Workload}

To get a more isolated picture of the container runtimes performance, we test the workloads separately on our Desktop machine. The I/O-bound test is supposed to test the container runtimes' ability to handle concurrent network requests. As we've explored in Chapter \ref{chapter:background}, serverless functions are frequently used with external services, such as databases. This test simulates this behaviour by blocking with a thread \inl{sleep} just like an I/O request would block. In the Wasm executors this is implemented with the capabilities, which allows the Wasm runtimes to supply native host functions as imports to the module. In this instance, we create our action using \inl{--annotation net\_access true} with the \inl{wsk} cli. The Wasm executor then provides the \inl{http.get} function as an import to the module. Each executor implements this with the aforementionend \inl{sleep}. The module can then use these functions as if they were part of the module. This way, we could easily implement an actual HTTP GET request. However, the lack of interface types means the module can only call native functions with integer parameters and return values, such that the practical use is still inhibited.

\begin{figure}
    \begin{center}
        \input{figures/pc-load-block.pgf}
    \end{center}
    \caption{The throughput with an I/O-bound workload on a Desktop machine with both Docker and Wasm-based container runtimes. The y-axis shows the throughput in requests per second, while the x-axis shows the number of concurrent requests, each of which is repeated 50 times to generate a consistent load over a short period of time.}
    \label{fig:pc-load-block}
\end{figure}

To isolate the performance from cold starts, a warmup phase is executed before gathering data, which lets OpenWhisk create the containers and keep them warm. The throughput is plotted in Figure \ref{fig:pc-load-block}. OpenWhisk imposes the same 4 container limit for all container runtimes, which puts a limit on the throughput for I/O-bound workloads. However, again we can use the concurrency limit to increase performance -- here we set it to 3. The \inl{dockerskeleton} we use in OpenWhisk vanilla performs very poorly for I/O-bound workloads. Two concurrent requests take twice the time that one request takes. This may be a limitation of the action proxy used in the \inl{dockerskeleton}.
It can only achieve more throughput by starting new containers. Thus, the concurrency limit essentially delays the peak I/O-bound throughput for the Docker version until OpenWhisk creates 4 containers.
% This explains the plateauing at regular intervals in the figure.

The Wasm executors on the other hand, have no such issue and can essentially scale up to the 4 container limit. Since we set the concurrency limit to 3, and $3 \cdot 4 = 12$, we reach the throughput peak at that point. A higher concurrency limit would consequently increase the possible throughput. Since blocking takes the same amount of time in every Wasm runtime, this performance is mainly determined by the executor itself and its threading model -- something we have explored at the end of Chapter \ref{chapter:design}. Other than the OpenWhisk limitations, the performance rests on the number of threads that can be spawned on the system.

\begin{figure}
    \begin{center}
        \input{figures/pc-load-hash.pgf}
    \end{center}
    \caption{The throughput with a CPU-bound workload on a Desktop machine with both Docker and Wasm-based container runtimes. The y-axis shows the throughput in requests per second, while the x-axis shows the number of concurrent requests, each of which is repeated 50 times to generate a consistent load over a short period of time.}
    \label{fig:pc-load-hash}
\end{figure}

Because the concurrency limit has such a pronounced effect on the performance, we also plotted the same workload with a concurrency limit of one. Given what we just described, it is no surprise that at 4 concurrent requests the system's throughput is saturated. This test also implicitly shows the direct comparison of the container runtime's overhead, even when the execution time is predetermined. Evidently, Docker has a larger overhead than the Wasm executors, all else being equal.

% Would be interesting to evaluate this in node.js -- which is primed for I/O

\subsection{CPU-bound Workload}

In Figure \ref{fig:pc-load-hash} is the result of the fully CPU-bound load test. This test shows a large performance gap among the Wasm executors, but also to Docker. On average, the Wasm executors process only 39\% (\inl{wamr}), 57\% (\inl{wasmtime}) and 88\% (\inl{wasmer}) of the throughput that Docker achieves.

Docker was particularly hard to evaluate for this workload. OpenWhisk -- at least in the standalone version -- is hardly able to saturate the CPU, because the system will not create enough containers to fully utilize it.
This likely occurrs because each container is run with the \inl{--cpu-shares} Docker option, limiting the share of the CPU the container is allowed to use. Hence, if not enough containers are created, some invocations have to wait for others to complete, even though the CPU isn't fully utilized.
By increasing the concurrency limit, we can work around that and let OpenWhisk execute actions in the same container to fully leverage the assigned CPU shares. Increasing this limit too much, forces more actions to share the same CPU shares, while not creating new containers.
However, increasing the memory on the container, which in turn makes OpenWhisk increase the CPU shares, does not have a positive effect either. Rather, it delays OpenWhisk's decision to create new containers even longer, resulting in even poorer performance. After some trial and error we arrived at values which seemed to leverage the CPU rather well, the default 256 MB of memory and the concurrency limit set to 3.
The Wasm executors showed no such signs, simply because no memory or CPU limiting is implemented in the first place. This will have to be a feature of a fully production-ready Wasm executor, however.

Cold start latencies and resource costs for keeping containers warm are ignored in this test. With that in mind, the test shows that for a steady stream of highly concurrent CPU-bound requests, using Docker is the best option. In particular if the container is likely to be reused many times. This comes perhaps not unsurprisingly, given that the code executing is Rust compiled to a native \inl{x86\_64} binary. There is hardly a way to generate faster code.

In fact, if we compare the pure execution time of our WebAssembly action outside the serverless context, we see the native binary being even faster. For that we precompile our action for \inl{wasmer} and then measure only the pure function execution time, without the setup of necessary contexts like \inl{Engine} or even \inl{Module}. In that case, \inl{wasmer} achieves just 60\% of the throughput compared to the native binary. Compared to the 88\% from above, it indicates that OpenWhisk with Docker is unable to fully utilize the available hardware, either because of the discussed configuration or because of internal overhead, like queuing and container management.

\begin{figure}
    \begin{center}
        \input{figures/pc-load-prime.pgf}
    \end{center}
    \caption{The ECDF for the execution of a WebAssembly module executed in the \inl{wasmtime} executor and JavaScript run in the \inl{openwhisk/action-nodejs-v14} image. The y-axis shows the proportion of total requests that were finished with the latency given on the x-axis, or less.}
    \label{fig:pc-load-prime}
\end{figure}

Note that using native code in serverless functions is not a particularly popular approach. Recall the serverless workload types we described in Chapter \ref{chapter:background}. We noted that functions which exhaust the CPU are more of a niche and, that if the executor can offer performance on the level of \inl{node.js}, it would roughly be on the same level as most of today's functions.
To put this into perspective, we run an experiment to demonstrate the potential improvement WebAssembly can give in comparison to today's prevalent serverless runtime: \inl{node.js}.
To that end, we write a function in AssemblyScript\footnote{\url{https://www.assemblyscript.org}}, a strict variant of TypeScript. It can compile directly to WebAssembly with the \inl{binaryen} toolchain, that we already use for optimizing our Wasm modules. Because AssemblyScript uses the types from WebAssembly itself as its basis, not all TypeScript code can simply be compiled to WebAssembly. Some minimal changes may be necessary to compile to both targets. We setup a sieve of eratosthenes to calculate prime numbers; a CPU-bound workload. The produced WebAssembly module does not run in all our Wasm executors out of the box, so only \inl{wasmtime} is shown. Calculating prime numbers is a use case were raw performance matters and as such, \inl{wasmtime} completes requests 4.68 times faster. The empirical cumulative distribution function (ECDF) of the results can be seen in Figure \ref{fig:pc-load-prime}. It shows that \inl{wasmtime} performs more consistently than the \inl{node.js} execution, evident by the lesser spread on the former's curve. Given that the WebAssembly code is precompiled, while \inl{node.js} must compile hot code paths just-in-time, this is not surprising. On the other hand, since the sieve is a loop and thus a very hot code path, the loss in performance is surprising.
Thus, for a computationally intensive serverless function written in TypeScript, it may be worth porting it to AssemblyScript, so it compiles to WebAssembly. However, AssemblyScript's goal is \emph{not} to be a TypeScript to WebAssembly compiler but rather an extension to the web ecosystem; complementing TypeScript and JavaScript. As programming languages go, AssemblyScript is still fairly young. While it shows that the accessibility to write source code which compiles to WebAssembly is lower than using systems programming languages like C or Rust, whether it could eventually act as a general purpose language to write serverless WebAssembly, is uncertain.

In light of this experiment and the preceding one, a fully CPU-bound WebAssembly module reaching 88\% of the performance of a native binary in Docker, but with much reduced startup costs, would seem to be an acceptable compromise.

\subsection{Realistic Workload}

\begin{figure}
    \begin{center}
        \input{figures/pc-load-zipf.pgf}
    \end{center}
    \caption{The ECDF of our experiment, designed based on serverless usage data from Microsoft Azure. The y-axis shows the proportion of total requests that were finished with the latency given on the x-axis, or less.}
    \label{fig:pc-load-zipf}
\end{figure}

Given what the research about serverless workloads showed in Chapter \ref{chapter:background}, we run another experiment. Recall the analysis from Microsoft Azure that found 81\% of functions are invoked less than once per minute, but those accessed more frequently make up 99.6\% of all invocations \cite{Shahrad2020}.
It shows that there is a small number of functions accessed frequently, and a larger amount of functions accessed sporadically.
Since we cannot simulate a test on the scale of Azure, we map the workload to a smaller scope. The test sends a sufficient number of concurrent requests in 4 threads. The latter comes from the 4 container limit imposed by OpenWhisk. Since blocking actions are not handled well by OpenWhisk vanilla, we accommodate. Consequently, the concurrency limit of the individual actions is set to its default of one, to have a one-to-one request-container relationship. For each request sent, we use randomization such that 90\% of requests are sent to the same two actions. These are equally likely to be either CPU- or I/O-bound. For the other 10\% of requests, one of 60 actions is selected at random, where the likelihood is again 50\% for them to be either CPU- or I/O-bound.

Thus, there is a high probability that a function is called, for which a warm container should be available. Occasionally, one of the rare functions is called which will inevitably cause a cold start. In summary, this test will show the performance of the system under load, where most functions are warm but the occasional cold start will need to be handled. We have plotted the ECDF of this test in Figure \ref{fig:pc-load-zipf}.

For the Docker runtime, we can see that around 40\% of requests were finished in less than 500 ms. However, around 60\% of requests were only finished in less than 1500 ms. We can reasonably explain this with warm and cold starts. The pure function execution times are always less than 300 ms, because we configured them so. From Figure \ref{fig:pc-cold-start-hash} we know that the average cold start time of Docker on the Desktop at 4 concurrent requests is 1269 ms. Thus, once a cold start is incurred it adds that amount to the latency seen by the caller. Therefore, either functions run in a warm container for around 500 ms, or they cause a cold start and need the same amount of time in addition to the cold start, which is why we see the sudden increase at around 500 ms. Once cold starts are occurring, they have a detrimental effect on the overall system performance as indicated by \todo{Is it clear what I mean by that?} the decrease in slope beyond 1000 ms.

Up to 20\% of invocations handled by the Docker runtime finish in the same amount of time or faster than WebAssembly executors. Given the bend at 40\% and our experimental setup, this is because half of the actions executed warm are CPU-bound, while the other half is I/O-bound. This is consistent with the previous data. As we have seen in Figure \ref{fig:pc-load-hash}, the Docker runtime is able to handle CPU-bound actions faster, thanks to directly executing native code. But it became clear in Figure \ref{fig:pc-load-block} that the runtime is less well equipped to handle I/O-bound ones. Hence, why we see the bend at around 20\%, where the docker curve migrates from being faster than the Wasm executors to being slower.

The WebAssembly executors show latencies beyond 500 ms only for a very small proportion of the requests.
Similar to Docker, there is a visible bend in some of the executor's curves, but not from cold starts. Since it is located at around the 50\% mark, this has to be due to the different workload types. Data from the earlier experiment shows that the cold starts for the Wasm executors are 72 ms (\inl{wasmtime}), 76 ms (\inl{wamr}) and 83 ms (\inl{wasmer}) for 4 concurrent requests. Therefore, the cold starts are not significant enough to saliently show up in the ECDF.
About half of \inl{wasmer}'s requests are faster before it drops to \inl{wasmtime}'s levels. Taking Figure \ref{fig:pc-load-hash} into account once more, where \inl{wasmer} was the fastest Wasm executor, half of these requests correspond to CPU-bound actions. The curve for \inl{wamr} is analogous. Notably, \inl{wasmtime} shows no salient points, but rather has the smoothest graph. Most likely, it so happens to execute both workload types at around the same speed.

%We previously saw that \inl{wamr} has a cheap cold start, if viewed in isolation, i.e. only the cost of the \inl{init} procedure. This figure shows the true cost of the runtime, which technically does have a cheap \inl{init} procedure as a consequence of its API limitations, but has to do more work in the \inl{run} method instead.

One question that might arise while looking at this figure is why 60\% of the requests running on Docker seem to be suffering a cold start. If we send 90\% of requests to the same two functions, then the same amount should be in the hot path. Due to being limited to 4 containers, OpenWhisk will remove one to make room for another one under sufficient load and if it deems it necessary. If we look at the number of cold starts, indeed we see around 58\% of requests requiring one. This is a fundamental limitation of OpenWhisk, perhaps due to our usage of the standalone version and would likely not apply to a, for example, Kubernetes deployment. However, it affects Wasm executors as much as Docker according to our numbers. But evidently, the Wasm executors are able to cope much better with repeated cold starts.

% Could mention throughput, data exists.


\section{Resource Usage}

This test aims to evaluate what the resource costs are, for keeping actions in memory and ready for execution.
To that end, we run our cold start test again and measure the amount of memory for each container once the actions have finished execution, but before OpenWhisk removes them. At this point, Docker containers have been paused by OpenWhisk. We use the memory usage output of \inl{docker stats} as the basis for evaluation. For the Wasm executors we read the resident set size (rss) of the entire process. Hence, we also measure the overhead of the web framework we run on, but also the Wasm runtimes' objects which are instantiated globally, such as \inl{Engine}s. Thus, this method is slightly imprecise in that it reads more than the pure memory occupied by the container. It would be difficult to define what constitutes the container, too, since the data structures, global or not, are all linked together to produce the running system. Since the Docker container also contains other data only more or less relevant to the execution, this is acceptable. It gives us an estimate for the average total cost of keeping a container in memory. Ultimately, the entire memory consumption is what matters. This method works slightly in favor of Docker, since we do not measure the memory allocated to the Docker daemon process.
For the Wasm executors, we divide the measured amount of memory by the number of containers that are kept alive at that point, to get an estimated memory usage for each container. The amount of memory consumed when idling, i.e. after startup, are 7.07 MiB for \inl{wasmtime}, 4.23 MiB for \inl{wamr} and 5.28 MiB for \inl{wasmer}. At first glance, it may seem that \inl{wasmtime} fares the worst. However, this may be due to the fact that \inl{wasmtime}'s API is the most suitable to our needs, because it lets us instantiate more of the necessary data structures ahead of time than the other runtimes. We are very happy to trade this memory for less setup costs later. \todo{I will rework this part once I have finishe the wasmer section in the Design.}

% wasm-opt -O4 and lto = true applied

For the Wasm executors, we expect the amount of consumed memory to depend primarily on the size of the Wasm module.
So we run this test thrice. For that we compile the \inl{hash} module with different options to get different module sizes.

\begin{itemize}
    \item Release mode with link-time optimization (LTO)
    \item Release mode without LTO
    \item Debug mode
\end{itemize}

LTO enables compilers to perform expensive, but very effective optimizations at compile time, when the different libraries of the program are linked together. Compiling our hash module with LTO brings the size down to 16.5\% (312 KiB) of the non-LTO version.
The resulting sizes of the Wasm modules and the native binaries are given in Table \ref{table:hash-binary-size}, to provide context for the following results.

% TODO: In the preceding section, check whether the claim about wasmtime <-> wasmer is correct

\begin{table}[h!]
    \centering
    \begin{tabular}{c | c | c}
        Compilation mode & WebAssembly & Native Binary\\
        \hline
        Release + LTO & 312 KiB                 & 2.01 MiB\\
        Release       & 1.51 MiB (\times 4.83)  & 3.80 MiB (\times 1.89)\\
        Debug         & 4.76 MiB (\times 15.25) & 6.46 MiB (\times 3.21)\\
    \end{tabular}
    \caption{Size of the hash module when compiled to \inl{wasm32-wasi} and \inl{x86\_64-unknown-linux-musl} targets respectively. To provide three different container sizes we use release mode with LTO and without as well as debug mode. The factors of increase over the release with LTO are also given.}
    \label{table:hash-binary-size}
\end{table}


\begin{figure}
    \begin{center}
        \input{figures/pc-memory.pgf}
    \end{center}
    \caption{The average memory usage of Docker and WebAssembly containers given different module sizes.}
    \label{fig:pc-memory}
\end{figure}

The measurements are in Figure \ref{fig:pc-memory}. These confirm that for the Wasm executors, the in-memory container size is largely dependent on the module size, simply because the container \emph{is} the module in memory plus the necessary supporting data structures. In particular, the modules are held in memory for the fastest possible access. For Docker on the other hand, we have to explore in some more detail how an action is executed to understand the results. In order to run native binaries, we use the \inl{dockerskeleton} image. When a container is created from that image, the so-called action proxy starts. This is a proxy service that implements the familiar \inl{/init} and \inl{/run} endpoints. It is generic over the underlying executable, so it could run bash or perl scripts as well as any native binary. When \inl{/run} is invoked and the initialized action is executed, the script or binary runs, its result is read by the proxy and returned. Consequently, after the binary finished execution, the memory of its process is freed as usual. Hence, when we measure the memory in between action executions, we only measure the amount needed for the action proxy and the rest of the container. It does not matter whether the action itself is 1 MiB or 10 MiB, the memory cost of keeping the container warm is the same. This is confirmed by our measurement. We can expect Docker containers to be consistent in their memory usage independent of the concrete action. This particular execution model applies to the \inl{dockerskeleton} but other images, such as the one based on \inl{node.js}, may differ.

Between different module sizes, the results show \inl{wasmer}'s containers to double their memory usage from the LTO to the non-LTO version, with another increase by roughly 3 times to the debug module. For \inl{wasmtime}, the difference between LTO and non-LTO are negligible, and then the memory usage increases by 2.4 times. For \inl{wamr} the difference between the release modes are barely noticeable and then go up by 36\%. Docker's container sizes between release modes hardly differ and only increases by 2.5\% to the debug version.

LTO does not seem to make a large difference to \inl{wasmtime} or \inl{wamr}, but \inl{wasmer} seems to not apply optimizations when precompiling.
The results show that for \inl{wasmer} and \inl{wasmtime} the container size primarily depends on the module size, except for LTO, where we suspect \inl{wasmtime} to apply optimizations when precompiling and thus coming out at the same size. Due to its JIT nature, it is probably more likely to trade memory for compilation speed or code performance.
While \inl{wamr} seems to be very memory efficient, which it might be, we have to point out that due to its architecture, we do not hold instantiated modules in memory in-between \inl{run} invocations, but only the module bytes themselves.

\begin{table}[h!]
    \centering
    \begin{tabular}{c | c | c | c}
        Runtime          Release (LTO) & Release    & Debug\\
        \hline
        \inl{wasmer}     5.5 \times    & 2.7 \times & 1.2 \times\\
        \inl{wasmtime}   3.5 \times    & 3.5 \times & 1.4 \times\\
        \inl{wamr}       7.4 \times    & 7.1 \times & 5.3 \times\\
    \end{tabular}
    \caption{The ratio of memory usage for the different module sizes in WebAssembly executors compared to the Docker container runtime.}
    \label{table:docker-wasm-memory-ratio}
\end{table}

Given these results and our minimal but realistic WebAssembly module size, we could hold more WebAssembly containers in the same amount of memory than Docker. Especially more of the less popular functions could be kept warm resulting in less cold starts. Although, with cheap cold starts, there is an interesting trade off between using resources to keep functions alive, and saving memory but paying the small, but nonetheless non-zero latency cost for the cold start.
Table \ref{table:docker-wasm-memory-ratio} shows how many more containers could be kept warm in the respective Wasm executors compared to Docker.
When optimizing the module size with LTO or other techniques, we can fit more than five times as many containers in the same amount of memory compared to Docker, when using \inl{wasmer}. Even for big modules we stay below the consistent memory usage of Docker.
% For realistic module sizes, for which we would assume the average to be between the release and debug versions of these modules, we would be able to fit roughly twice the number of containers in the same memory that Docker would be able to for \inl{wasmer} and \inl{wasmtime}.
In more constrained environments with lower memory availability, \inl{wamr} -- primarily due to how we use it in the executor -- would be a good choice given its much lower memory usage, but at the cost of performance as we have seen in the previous measurements.
Ultimately, the overhead introduced by the executors themselves is very minimal and mostly dependent on the module size. This puts the control in the action developer's hands. If they manage to strip their binary down to the necessary contents, they can expect a small memory footprint at execution time. With Docker, there is a consistent memory usage but higher than the Wasm executors and less opportunity for developers to influence its footprint.

% One thing to note is that the module sizes could even be much smaller. Rust programs can take advantage of link-time optimization (LTO), which enables compilers to perform expensive, but effective optimizations when the different libraries of the program are linked together. Compiling our hash module with LTO brings the size down to 16.5\% (312 KiB) of the non-LTO version.
If memory is a priority, other optimizations such as using a smaller memory allocator are also possible. Written for the use case in browsers, \inl{wee\_alloc}\footnote{\url{https://github.com/rustwasm/wee_alloc}} is an allocator that trades speed for size. Rust also allows writing \inl{no\_std} programs, i.e. programs without its standard library. Since this is not how we anticipate most functions would be written, we did not employ this technique. However, without the standard library only the code and libraries the developer brings in end up in the binary, granting a maximum of control over the program's size.
Using other languages may also come with size benefits. The prime number example we wrote in AssemblyScript is very small in comparison to the Rust-produced modules and comes out optimized at about 22 KiB. It shows that modules can be very minimal and the in-memory container sizes would benefit accordingly for most of the runtimes.

% Note that we deliberately do not use no_std or other mechanisms to make the modules small, so as to represent a real-world applicable use-case where Rust's std library would most likely be used
% A mature serverless platform could offer an API the serverless function can program against, such that no_std would be facilitated. Still, since std provides types that many of Rust's libraries work with, it is more realistic to use it. Then again, we can assume this to be on the higher end, given that AssemblyScript produces modules less than X KB. So with more optimization on this end, we can get the container size down by a lot.

It would also be possible to accommodate systems with very constrained memory settings, by fully exploiting the WebAssembly runtime's design. Wasm runtimes do not fundamentally have to keep modules in memory. The command line versions of \inl{wasmtime} or \inl{wasmer} cache compiled modules on the filesystem by default, so the support already exists. \inl{wasmer} would be a prime-candidate for this type of optimization. The executor instantiates a module from in-memory bytes it receives from OpenWhisk. However, \inl{wasmer} -- in its AoT mode with LLVM and the \inl{NativeEngine} -- needs to write the bytes of the shared object it gets to a temporary file, because \inl{dlopen} only works on files. Thus it already needs to write to a file internally because of this restriction. Doing this procedure in the executor itself would be possible, since the API allows us to create a module directly from a file to skip this process. It means we would have to create the \inl{Module} from that file on every \inl{run} invocation, in order to save on memory. Given that the \inl{init} time of \inl{wasmer} is currently around 20 ms, according to Figure \ref{fig:pc-pi-cold-start-wasm-only}, it is not infeasible to incurr this cost on every \inl{run} if memory is a priority. Alternatively, \inl{wasmtime}'s deserialization cost is just half, and thus could be similarly cached to the file system. Again, there is a trade off between memory and performance, but most importantly, it \emph{exists} and can be made by the implementor for whichever property is more essential.

\section{Summary}

In this section, we conclude the evaluation chapter by summarizing the results and answering the research questions based on them.

\subsubsection*{By how much can WebAssembly runtimes alleviate the cold start latency?}

Our WebAssembly executors can reduce the cold start latency, on average, by more than 99.5\% on a Raspberry Pi compared to OpenWhisk using Docker. This is in part due to Docker reaching the memory limits of the Pi and swapping. However, even on our \inl{x86\_64} host, where enough memory is available, the reduction of cold start time is between 93\% and 94\%, on average. The executors behave more consistently in the cold start than the Docker container runtime. Within the Wasm runtimes, \inl{wasmer} takes more time for the cold start than the others, being between 47\% and 61\% slower. The \inl{wamr} runtime and \inl{wasmtime} behave very similar with \inl{wamr} taking the lead, due to its simple initialization procedure.

When only accounting for the overhead introduced by the executors themselves, on the \inl{x86\_64} host, \inl{wasmtime} introduces less than 10 ms, and \inl{wasmer} less than 20 ms. In comparison with Docker startup times, which are always far north of 100 ms \cite{Wang2018, Manner2018}, this is an order of magnitude lower. Furthermore, we have seen in our tests what is also confirmed by other research \cite{Mohan2019}, that Docker startup times rise under increasing concurrency, while the latencies are consistent for the Wasm executors. Thus, the Wasm executors should also perform well outside the OpenWhisk context.

\subsubsection*{How do the performance characteristics of the WebAssembly and Docker container runtimes differ?}

In the mixed and realistic workload we can best see the holistic picture. Here, the WebAssembly executors are always faster than Docker. In the mixed workload, the WebAssembly executors are between 1.6 and 3 times faster on the Desktop, and 2.4 to 4.2 times faster on the Pi. In the realistic workload, we find Docker's overall performance to be strongly linked to the cold start time. The WebAssembly executors, however, are not noticeably susceptible to cold starts, showing much more consistent behavior.

For I/O-bound workloads, the used Docker image showed a higher overhead even when the execution time itself was fixed. It proves that Docker has a higher inherent management cost. On the other hand, it handles pure CPU-bound workloads very well. It proves once more that Docker containers can be very performant, once they are running. The issue is getting to that point.
The Wasm executors scale linearly for I/O-bound workloads, up to the limits imposed by OpenWhisk and later the underlying operating system, due to their threading model. Once Wasm runtimes allow handling I/O asynchronously -- which is starting to appear -- we expect this performance to be even better while consuming much fewer resources.

In pure CPU utilization, \inl{wasmer} managed 88\% of the throughput compared to the native binary in the Docker container, with the other runtimes falling behind more quickly. Our AssemblyScript experiment showed, however, that the prevalent \inl{node.js} runtime would perform much worse under such workloads indicating that they are either not a primary use case or that this level of performance is good enough. Since WebAssembly shows to be several factors faster, this is an overall improvement.

\subsubsection*{What are the resource costs for keeping containers warm in both container runtimes?}

The cost for keeping containers warm is relatively consistent for Docker independent of module size, for the image we tested. WebAssembly container's in-memory size is primarily determined by the module they hold. The \inl{wasmer} runtime shows the strongest such effect closely followed by \inl{wasmtime}. Due to not keeping modules in memory between requests, \inl{wamr} shows very little memory usage and exemplifies the use case when memory would be very constrained and a priority over speed. Overall, for reasonably sized WebAssembly modules, the Wasm executors can be expected to consume less memory than the Docker image we tested.

\subsubsection*{Which WebAssembly runtime is the most suitable for use on edge devices?}

% To answer this question, we take into account everything we have learned during both the design and evaluation phase. 
Both \inl{wasmtime} and \inl{wasmer} have an API that allows us to efficiently implement our main requirements for modules: An initialize once, run often lifecycle. Unfortunately, \inl{wamr}'s API is less suitable to that task and it may be part of why it lacks in performance. It has the lowest performance of Wasm executors in I/O- and CPU-bound workloads, as well as the mixed and realistic workload. The memory usage, on the other hand, is lowest in terms of idle footprint and container size, from a pure numbers perspective. A certain upside is its support for more instruction set architectures than the others, which is relevant for edge devices. From a performance perspective however, unless memory is very constrained, it is the least suitable of our selected runtimes.

It then comes down to the other two runtimes. The main advantage of \inl{wasmer} is its Ahead-of-Time compilation that produces very high quality native code, evident by its performance in CPU-bound workloads. This seems to be coupled with being 47\% slower in cold starts than \inl{wasmtime}, which handles this task particularly well. On edge devices and in a mixed workload, the margin to \inl{wasmer} is slimmer than on the Desktop. Both runtimes support primarily \inl{x86\_64} and \inl{aarch64}, which covers a wide range of devices, but not all. How relevant that is depends on the planned use of the serverless platforms and whether its worker nodes would run on devices with other ISAs.
In conclusion, both of these runtimes are good general-purpose choices and perform well under various circumstances.
