\chapter{Background}
\label{chapter:background}

In this chapter we establish a common background for serverless computing and WebAssembly.

\section{Serverless computing}

\begin{quote}
    \quot{Serverless computing is an emerging cloud-based execution model in which user-defined functions are seamlessly and transparently hosted and managed by a distributed platform. \cite{Nastic2017}}
\end{quote}

\citeauthor{McGrath2017} describe serverless as the manifestation of the idea, that applications are defined by events and the actions they trigger \cite{McGrath2017}. Events can be a user pressing a button on a frontend, a file being uploaded to a storage system or an edge device measuring sensor data and posting it. If many events occur at once, the serverless platform is able to scale-up to meet the demand, while the customer does not have to manage this process. In that case, the customer is paying per-use of functions. Serverless is also ideal if no events occur, because no cost for the user is incurred. Serverless platforms are able to offer that, because they follow the scale-to-zero principle: functions that aren't used don't take up resources. Or in theory, anyway. In practice, one caveat applies.


\subsection{Cold Start}

In the context of serverless computing, a cold start refers to the invocation of a function, when all of the necessary resources for its execution must be provisioned from scratch. A cold start thus delays the execution of the function itself, by the amount of time it takes for the resources to become ready. This is what is called the cold start latency or cold start time.

\citeauthor{Wang2018} have measured the cold start latency at major cloud providers such as Amazon Web Services (AWS) and Google Cloud Platform. The median cold start latency for AWS was between 250 ms (1536 MB) and 265 ms (128 MB) and between 110 ms (2048 MB) and 493 ms (128 MB) for Google, both depending on the amount of memory assigned to the instance \cite{Wang2018}.

Cold starts prevent serverless platforms from scaling to zero in practice. If every function would be executed in a newly provisioned container, subsequent invocations of the same function would not be performant enough. Thus, typical optimizations are to keep these resources provisioned, such that repeated requests are faster to execute. Apache OpenWhisk for instance, keeps a functions container paused and ready for reuse for 10 minutes, before removing it entirely. AWS Lambda's cold start policy has been reverse-engineered, and the findings indicate that an instance stays alive for 5 to 7 minutes, during which time no cold start will take place \cite{ShilCold2021}.

\subsection{Serverless Edge Computing}

With the growing number of IoT devices, we're \quot{entering the post-cloud era}, as \citeauthor{Shi2016} write. The edge computing paradigm promises to be a solution to the massive amounts of data that will be generated at the edge. A wearable body sensor measuring health data for example, can use the wearer's smartphone (edge device) for data processing rather than sending the workload all the way to the cloud. In general, edge devices are resources that sit between the data source and the cloud. This processing close to the data source has a number of advantages, such as enabling low latency response times as well as working with privacy-sensitive data. This is necessary, since sending all data to the cloud has comparably higher latency and puts more strain on the network \cite{Shi2016}.

One piece of the puzzle towards realizing this paradigm, is serverless edge computing. \citeauthor{Aslanpour2021} lay out the vision for serverless edge computing, of which we want to highlight a few.

\begin{itemize}
    \item Due to IoT devices' unpredictable workload, an under- or overprovisioning of static resources such as VMs or containers is likely. Serverless with its ability to scale to zero (even with above's caveat) provides more precise provisioning of resources and only incurs cost when used, bringing advantages for operator and user, respectively. In an energy-constrained setting such as on single-board computers (SBC) executing serverless workloads, this becomes doubly-important.
    \item IoT architectures are usually event-driven, making them a perfect fit for serverless, as already described above.
    \item Serverless computing tends to be used for unpredictable, bursty workloads that require immediate up-scaling. The event-driven nature of IoT applications fits this pattern well \cite{Aslanpour2021}.
\end{itemize}

Serverless computing can be the enabler for edge computing. But some of its original design decisions need to be reexamined, as \citeauthor{Nastic2018} point out, due to the

\begin{quote}
    \quot{[...] inherently different nature of Edge infrastructure, for example, in terms of available resources, network, geographical hyper-distribution, very large scale, etc. \cite{Nastic2018}}
\end{quote}

This motivates our work to reexamine one of the fundamental building blocks of serverless computing, in order to close a gap towards the vision of serverless edge computing.

\section{Serverless workload}
\label{section:serverless_workload}

Since the edge computing paradigm is already well-aligned with cloud-based serverless computing, we think that a solution that works well for one, should work well for the other with minimal adjustments. Serverless functions should already be as lightweight as possible in either scenario, so the goals for both are well-aligned.
To understand what our solution must satisfy we want to examine the type of workload that is typical for serverless more closely.

\citeauthor{Shahrad2020} have characterized the serverless workload at a large cloud-provider, Microsoft Azure. Because of their large number of users, this study can be considered very representative of the average workload.
They find that on average 81\% of the functions are invoked less than once per minute. However, those accessed more frequently make up 99.6\% of all invocations \cite{Shahrad2020}.

Those frequently accessed functions should thus be kept in memory, to avoid the cold start entirely. The less frequently accessed functions should not be kept in memory, but created, executed and destroyed immediately, in order to save resources. For this to be viable, the cold-start needs to be a cheap operation. In general, the cheaper the cold start, the smaller the amount of time that functions need to be kept in memory.
Customers are only billed for the execution time of the function in FaaS. Operators bear the cost of keeping functions in memory -- or resources warm, more generally. Thus, shorter cold starts reduce costs.

\citeauthor{Shahrad2020} also find that on average, 50\% of functions execute for less than one second.

\begin{quote}
    \quot{The main implication is that the function execution times are at the same order of magnitude as the cold start times reported for major providers. \emph{This makes avoiding and/or optimizing cold starts extremely important for the overall performance of a FaaS offering} \cite{Shahrad2020}.}
\end{quote}

This is corroborated by the findings of \citeauthor{Wang2018} above.
Because of those, the primary goal in this work is that of reducing the cold start latency, to make the first function invocation less costly. However, we also need to make sure that what we have a resource-efficient way to keep the function in memory, since the most frequently accessed ones make up the overwhelming share of invocations and cold starting them would be even more costly.
As \citeauthor{Shahrad2020} point out, there is a fundamental trade-off between using memory to keep functions warm, and not using it, but taking the hit in additional latency, caused by cold starting the function. Thus, unless we can eliminate the cold start completely, we will need some kind of keep-alive policy. Hence, we also investigate the resource usage of WebAssembly modules in memory.

\citeauthor{Eismann2021} conducted a study on why companies adopt serverless, for which applications it is best-suited and how they are implemented. They analyzed 89 open-source serverless applications, of which 84\% have bursty workloads. Thus, serverless platforms need to be able to handle concurrent requests well. 69\% of the applications have a data volume of less than 10 MB. Thus, memory-bound applications are likely not a primary use case. Further, they find 39\% of the applications have a high traffic intensity, 47\% have a low traffic intensity and 17\% utilize scheduled functions. Thus, the latter combined 64\% represent on-demand scenarios where the platform will experience cold starts, since it is less likely that a warm function exists in those cases. According to the authors, 88\% of the applications make use of Backend-as-a-Service (BaaS) offerings. Storage, databases and messaging services are the most popular ones \cite{Eismann2021}. Hence, the majority of functions will make requests to external services, oftentimes implemented with HTTP which means those functions will frequently wait for network I/O to complete while the CPU is not actively utilized. More generally, I/O-bound workloads are a primary type that serverless platforms should be able to handle well.

\section{WebAssembly}

% - origins (asm.js)
% - first implemented in browsers, targeted at web
% - but standalone runtimes exist, similar to V8 which was the node.js runtime for Chrome and later became node.js
% - now also used at cloud service providers for serverless functions (fastly, cloudflare) and as the VM for smart contracts implementation, for instance in IOTA. In all three of these cases security is quite important, as untrusted code is executed.
% - security properties of Wasm
% - Finally, speed is also important (whole reason why asm.js was made in the first place)
% - to use outside of the browser, need a interface to the OS, the task that is done by the browser on the web platform

For the longest time, JavaScript was the sole client-side language in web browsers. With the rising popularity of the web platform and its growing number of APIs, more and more complex web apps were written. Sometimes in other programming languages. Necessarily, these languages had to use JavaScript as a target format, like Java bytecode or machine-level assembly in order to run on the web. Of course, JavaScript wasn't designed for this and so performance was lacking.

In 2013 a solution to this problem was introduced by engineers at Mozilla, aptly named \inl{asm.js}. It restricts itself to the parts of JavaScript that can be optimized ahead-of-time \cite{Herman2014}. Thus it might be used to compile a C/C++ program to the \inl{asm.js} target format to execute it faster with a JavaScript runtime than the equivalent JavaScript program would be. Benchmarks even showed it to run no more than 1.5x slower than native code \cite{Zakai2013}.

Finally, WebAssembly (Wasm) was born out of \inl{asm.js} in 2015, with more layers of optimization. It is a portable and universal binary instruction format for memory-safe, sandboxed execution in a virtual machine.
It is possible to write programs in a variety of languages like C, C++, Rust, AssemblyScript, C\#, Go, Swift and many more, and compile them to Wasm, which finally relieves JavaScript of its role as the universal target format of the web \cite{W3C2020}.

% Since it is a bytecode format independent from JavaScript, it requires a separate runtime and sandboxing mechanism.

\subsection{Performance}

Compared to \inl{asm.js}, the Wasm binary format is smaller in size (10-20\%) and faster to parse (by an order of magnitude)
\cite{Clark2019}. It is 33\% faster than \inl{asm.js} on average \cite{Haas2017}.
\citeauthor{NotSoFast} ran the PolyBenchC benchmark suite to compare Wasm to native code and found that 13 of the 24 benchmarks performed within 10\% of native code. However, they argue that the roughly 100 lines of code long benchmarks are not fully representative of typical use cases. They run the SPEC CPU benchmark suite, which are significantly larger POSIX applications. On average, they find Wasm to be slower than native by 1.55\times in Firefox and 1.45\times in Chrome \cite{NotSoFast}.
% Not representative, if we precompile to native code, but may be relevant if we choose to also evaluate the Wasm runtimes JIT modes
Wasm modules tend to be smaller than native x86-64 binaries. On average, Wasm modules have 85\% of the size.
\cite{Haas2017}.

% How relevant is native speed actually for the serverless context -- examine in later chapter?

% The binary format is designed to allow streaming the contents, so that parts of it can be compiled even before the entire module has been transmitted. (but this is actually irrelevant for our OW implementation, since we have the init call first, in which the module is stored in a hashmap, and later the run call, where it is directly read from memory)


\subsection{Security}

% sandbox, memory-safety, linear memory, ...

Wasm defines two important goals for security.

\begin{quote}
    \quot{(1) protect users from buggy or malicious modules, and (2) provide developers with useful primitives and mitigations for developing safe applications, within the constraints of (1) \cite{W3C2020}.}
\end{quote}

To that end, Wasm uses fault isolation techniques to sandbox the executing module. Interaction with the host environment is only possible through imported functions \cite{W3C2020}. This mechanism represents a \quot{safe foreign function interface} as it can communicate with the outside environment, but not escape the sandbox \cite{Haas2017}.

In Wasm, heap allocations allocate in its linear memory. This memory is separate from the code space, which prevents programs from overwriting instructions. Programs can only operate in their own execution environment, but cannot escape. This means Wasm runtimes can safely execute multiple untrusted modules, with their own linear memories, in the same process memory space and without requiring additional isolation \cite{Haas2017}. This is a key aspect that enables Wasm as a lightweight container technology. Only a single instance of a runtime is needed to execute many (serverless) functions.

Wasm also features control-flow integrity because it enforces structured control flow. Jump targets and other properties are validated in a single-pass before a module is instantiated. In particular, validation prevents jumps from targeting arbitrary locations. This validation must happen before a module is executed by a runtime \cite{Haas2017}.

With both performance and security under its belt as well as being a portable and universal target format, Wasm turns out to be useful not just in the browser, but also outside of it.

% http://wingolog.org/archives/2020/10/15/on-binary-security-of-webassembly
% Some of the criticism of Wasm security by "everything old is new again" is due to the fact, that C and C++ are simply not memory-safe. Shipping them as x86\_64 binaries compiled with a modern compiler, will add mitigations for these unsafeties. Wasm does not have those, so the vulnerabilities are once again exploitable. However, whether this is Wasm's fault is debatable. Wasm aims to be a universal target format. It does aim to enable C or C++ to run on the Web, similar to asm.js. However it is not the only language that was designed for.

% Rust has the concept of zero-cost abstractions. The concept can be boiled down to, if you don't use it, you don't pay for it (cite stroustroup or so). Early version's of the Rust language had green threads built-in, which was removed precisely for that reason. Similarly, Wasm should not introduce mitigations that every language would pay for, even if they are already memory-safe by design, like Rust or even Java. In this sense, it is not Wasm's responsibilty to fix the insecurities of C or C++.

\subsection{System Interface}

Wasm targets an abstract machine, so it needs an interface to the system. In the browser that works by calling into the browser through JavaScript glue code. Outside of it, a new solution was needed: The WebAssembly System Interface (Wasi). This interface was designed with Wasm's goals -- portability and security -- in mind.
Wasi is binary-compatible, so that Wasm binaries are portable between different concrete systems like Linux and Windows, but also browsers. The Wasm runtimes effectively implement that interface and translate it to the underlying concrete system.
Wasi also innovates in security, over the classic coarse-grained access control. It specifies a fine-grained capability-based security model, that consists of two parts. First, Wasi makes use of the aforementioned import mechanism of Wasm, to adhere to its security model. The Wasm module cannot directly call an OS system call due to sandboxing, but imports Wasi functions \cite{Clark2019}. For instance, a module that wants to open a file needs import statements like these:

% Change to `wat` after this PR has landed: https://github.com/pygments/pygments/pull/1564
\begin{minted}{python}
(import "wasi_snapshot_preview1" "fd_read" (func (;5;) (type 8)))
(import "wasi_snapshot_preview1" "path_open" (func (;7;) (type 22)))
\end{minted}

The runtime needs to supply those imports somehow, or the module cannot run. In effect, the runtime and module need to be in consensus about which functions can be accessed. In a serverless context for example, the cloud provider could grant or refuse access to certain functions, enabling security policies on a per-function basis. However, even that is not fine-grained enough. The cloud provider might only want to allow serverless functions to cache some data temporarily in the \inl{/tmp/} directory, but not allow opening \inl{\~/.ssh/id\_rsa}. This is where Wasi takes ideas from capability-based security.

In order to open a file, the module needs to call functions accessing the file, like \inl{fd\_read}, with a preopened file descriptor \cite{Clark2019}. The Wasm module then has the \emph{capability} to access \emph{that} file, but not others. The file can be opened by the runtime itself, or the program that embeds the runtime. They store that file descriptor in a special Wasi module which the runtime -- similar to a dynamic linker -- links to the Wasm module from where it imports the functions. Again, this gives the embedder (i.e. a cloud provider) the chance to customize the exact capabilities on a fine-grained level.

% Compare with docker in terms of sandboxing, security, system interface, performance, resource cost
% we can control access to resources as well -- at least memory (more research)

\subsection{Execution}

Just like Java Bytecode needs a JVM, Wasm needs a runtime as well. Since WebAssembly itself is just a specification, we need a concrete implementation to execute a module. There are many implementations available, the most used ones likely being those in the major browser JavaScript engines. However, since we don't need JavaScript support, we will use one of the standalone runtimes. Those come with different execution engines and goals. Wasm can be interpreted, just-in-time compiled or compiled to native code, ahead-of-time. All of these have different properties in terms of their cold-start latency as well as their runtime performance. We will discuss concrete runtimes in chapter \ref{chapter:design}.
